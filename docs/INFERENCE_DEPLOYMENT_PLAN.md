# ğŸš€ ĞŸĞ»Ğ°Ğ½ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Inference ÑĞµÑ€Ğ²Ğ¸ÑĞ°

## ğŸ“‹ ĞĞ±Ğ·Ğ¾Ñ€

ĞŸĞ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ñ‹ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½Ñ‘Ğ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ğ¹ inference ÑĞµÑ€Ğ²Ğ¸Ñ Ñ Web UI Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ€Ñ‹Ğ±.

## ğŸ—ï¸ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      User Browser                           â”‚
â”‚                    (Web Interface)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ HTTP (Upload Image)
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Kubernetes LoadBalancer                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FastAPI Service                           â”‚
â”‚              (Fish Classification API)                      â”‚
â”‚                                                             â”‚
â”‚  - POST /predict (upload image)                            â”‚
â”‚  - GET /health                                             â”‚
â”‚  - GET /model_info                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                        â”‚
             â”‚ Load model             â”‚ Log predictions
             â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MLflow Server    â”‚    â”‚   PostgreSQL         â”‚
â”‚  (Model Registry)  â”‚    â”‚  (Predictions Log)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹

### 1. FastAPI Inference Service

**Ğ¤Ğ°Ğ¹Ğ»**: `api/inference_service.py`

**Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸**:
- Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· MLflow Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğµ
- Endpoint Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹
- Preprocessing Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹
- Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹

**Endpoints**:
```python
POST /predict
  - Input: multipart/form-data (image file)
  - Output: JSON {
      "class": "salmon",
      "confidence": 0.95,
      "all_predictions": {
        "salmon": 0.95,
        "trout": 0.03,
        ...
      },
      "prediction_time": 0.123
    }

GET /health
  - Output: {"status": "healthy", "model_loaded": true}

GET /model_info
  - Output: {"model_name": "...", "version": "1", "classes": [...]}
```

### 2. Web UI

**Ğ¤Ğ°Ğ¹Ğ»**: `web/index.html`

**Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸**:
- Drag & drop Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹
- ĞŸÑ€ĞµĞ²ÑŒÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
- ĞÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
- Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ (progress bars)
- Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹

**Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸**:
- HTML5 + CSS3 + JavaScript (vanilla)
- Bootstrap 5 Ğ´Ğ»Ñ UI
- Chart.js Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸

### 3. Kubernetes Deployment

**Ğ¤Ğ°Ğ¹Ğ»Ñ‹**:
- `k8s/inference-api-deployment.yaml`
- `k8s/inference-api-service.yaml`
- `k8s/inference-api-hpa.yaml`

**ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ**:
- 2-3 Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ HA
- HPA: Ğ°Ğ²Ñ‚Ğ¾ÑĞºĞµĞ¹Ğ»Ğ¸Ğ½Ğ³ 2-10 Ñ€ĞµĞ¿Ğ»Ğ¸Ğº
- Resource limits: 2 CPU, 4GB RAM
- Liveness/Readiness probes
- LoadBalancer Ğ´Ğ»Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°

## ğŸ”„ Workflow Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ

### Ğ¨Ğ°Ğ³ 1: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ âœ… (ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ)

```bash
# 1. Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ MLflow
Airflow DAG: test_mlflow_integration (~2-3 Ğ¼Ğ¸Ğ½)

# 2. ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
Airflow DAG: fish_classification_training_full (~20-25 Ğ¼Ğ¸Ğ½)

# 3. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² MLflow UI
http://130.193.38.189:5000
  - Experiment: fish-classification
  - Model: fish-classification-mobilenetv2
  - Version: 1
```

### Ğ¨Ğ°Ğ³ 2: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Inference API ğŸ”„ (ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ÑˆĞ°Ğ³)

```bash
# 1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ FastAPI ÑĞµÑ€Ğ²Ğ¸Ñ
api/inference_service.py
api/requirements.txt
api/Dockerfile

# 2. Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Docker Ğ¾Ğ±Ñ€Ğ°Ğ·
docker build -t fish-inference-api:latest api/
docker tag fish-inference-api:latest cr.yandex/...
docker push cr.yandex/...

# 3. Ğ”ĞµĞ¿Ğ»Ğ¾Ğ¸Ğ¼ Ğ² Kubernetes
kubectl apply -f k8s/inference-api-deployment.yaml
kubectl apply -f k8s/inference-api-service.yaml
kubectl apply -f k8s/inference-api-hpa.yaml

# 4. ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ IP
kubectl get svc inference-api-service
```

### Ğ¨Ğ°Ğ³ 3: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Web UI ğŸ”„ (Ñ„Ğ¸Ğ½Ğ°Ğ»)

```bash
# 1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ°Ğ¹Ñ‚
web/index.html
web/style.css
web/app.js

# 2. Ğ”ĞµĞ¿Ğ»Ğ¾Ğ¸Ğ¼ Ğ² Kubernetes (Nginx)
kubectl apply -f k8s/web-ui-deployment.yaml
kubectl apply -f k8s/web-ui-service.yaml

# 3. ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ URL
kubectl get svc web-ui-service
```

## ğŸ“ ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

### Ğ§ĞµÑ€ĞµĞ· Web UI:

1. ĞÑ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚Ğµ `http://<web-ui-ip>`
2. ĞŸĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°ĞµÑ‚Ğµ Ñ„Ğ¾Ñ‚Ğ¾ Ñ€Ñ‹Ğ±Ñ‹
3. ĞĞ°Ğ¶Ğ¸Ğ¼Ğ°ĞµÑ‚Ğµ "Classify"
4. Ğ’Ğ¸Ğ´Ğ¸Ñ‚Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚:
   ```
   ğŸŸ Salmon (95.3% confidence)
   
   All predictions:
   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Salmon: 95.3%
   â–ˆâ–ˆ Trout: 2.1%
   â–ˆ Bass: 1.5%
   ...
   ```

### Ğ§ĞµÑ€ĞµĞ· API:

```bash
# Upload image
curl -X POST http://<api-ip>/predict \
  -F "file=@my_fish.jpg"

# Response
{
  "class": "salmon",
  "confidence": 0.953,
  "all_predictions": {
    "salmon": 0.953,
    "trout": 0.021,
    "bass": 0.015,
    ...
  },
  "prediction_time": 0.123
}
```

## ğŸ”§ Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸

### Model Loading Strategy

```python
# ĞŸÑ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğµ ÑĞµÑ€Ğ²Ğ¸ÑĞ°
def load_model():
    mlflow.set_tracking_uri(MLFLOW_URI)
    
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ latest version
    model = mlflow.keras.load_model(
        f"models:/fish-classification-mobilenetv2/latest"
    )
    
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ class indices
    client = mlflow.tracking.MlflowClient()
    run = client.get_latest_versions("fish-classification-mobilenetv2")[0]
    class_indices = mlflow.artifacts.load_dict(
        f"runs:/{run.run_id}/class_indices.json"
    )
    
    return model, class_indices
```

### Image Preprocessing

```python
def preprocess_image(image_bytes):
    # Load image
    img = Image.open(io.BytesIO(image_bytes))
    
    # Resize to model input size
    img = img.resize((224, 224))
    
    # Convert to array
    img_array = np.array(img) / 255.0
    
    # Add batch dimension
    img_array = np.expand_dims(img_array, axis=0)
    
    return img_array
```

### Prediction with Logging

```python
async def predict(image_bytes):
    # Preprocess
    img_array = preprocess_image(image_bytes)
    
    # Predict
    start_time = time.time()
    predictions = model.predict(img_array)
    prediction_time = time.time() - start_time
    
    # Get top class
    top_class_idx = np.argmax(predictions[0])
    top_class = class_names[top_class_idx]
    confidence = float(predictions[0][top_class_idx])
    
    # Log to database
    log_prediction(
        image_hash=hashlib.md5(image_bytes).hexdigest(),
        predicted_class=top_class,
        confidence=confidence,
        prediction_time=prediction_time
    )
    
    return {
        "class": top_class,
        "confidence": confidence,
        "all_predictions": dict(zip(class_names, predictions[0])),
        "prediction_time": prediction_time
    }
```

## ğŸ“Š ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³

### ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Prometheus

```python
from prometheus_client import Counter, Histogram

prediction_counter = Counter(
    'predictions_total',
    'Total number of predictions',
    ['class', 'confidence_bucket']
)

prediction_latency = Histogram(
    'prediction_latency_seconds',
    'Prediction latency in seconds'
)
```

### Grafana Dashboard

- Predictions per minute
- Average confidence by class
- Prediction latency (p50, p95, p99)
- Error rate
- Model version in use

## ğŸš€ Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¸

### Ğ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ:
1. âœ… Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ `test_mlflow_integration` DAG
2. ğŸ”„ Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ `fish_classification_training_full` DAG
3. âœ… ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² MLflow UI

### Ğ—Ğ°Ğ²Ñ‚Ñ€Ğ°:
1. ğŸ“ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ FastAPI Inference Service
2. ğŸ³ Ğ¡Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Docker Ğ¾Ğ±Ñ€Ğ°Ğ·
3. â˜¸ï¸ Ğ Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ Ğ² Kubernetes
4. ğŸŒ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Web UI
5. ğŸ‰ ĞŸÑ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ pipeline

## ğŸ’¡ Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)

- **A/B Testing**: ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²ĞµÑ€ÑĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
- **Batch Predictions**: Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹
- **Feedback Loop**: Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ
- **Model Retraining**: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- **Multi-model Serving**: ensemble Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
