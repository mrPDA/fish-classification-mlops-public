# üîß DataProc Troubleshooting

## –ü—Ä–æ–±–ª–µ–º–∞: "Failed initialization actions" –∏ "Unhealthy services: YARN"

### –°–∏–º–ø—Ç–æ–º—ã

```
Error: Cluster c9qglkokffhea36bth6d has failed initialization actions
Cluster c9qglkokffhea36bth6d has unhealthy services: YARN
```

### –ü—Ä–∏—á–∏–Ω—ã

1. **–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –¥–ª—è YARN**
   - `s2.micro` (2 CPU, 8GB RAM) —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã YARN
   - YARN —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 4 CPU –∏ 16GB RAM –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π —Ä–∞–±–æ—Ç—ã

2. **Initialization actions –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã**
   - –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ init actions –º–æ–∂–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã
   - Timeout –∏–ª–∏ –æ—à–∏–±–∫–∏ —Å–∫—Ä–∏–ø—Ç–∞ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –ø–∞–¥–µ–Ω–∏—é –∫–ª–∞—Å—Ç–µ—Ä–∞

3. **–ú–µ–¥–ª–µ–Ω–Ω—ã–π –¥–∏—Å–∫**
   - `network-hdd` —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π –¥–ª—è YARN –∏ Spark
   - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è `network-ssd`

### –†–µ—à–µ–Ω–∏–µ

#### 1. –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–µ—Å—É—Ä—Å—ã –∫–ª–∞—Å—Ç–µ—Ä–∞

```python
# –í dags/fish_classification_training_full.py

# Master node
masternode_resource_preset='s2.small',  # 4 CPU, 16GB RAM
masternode_disk_type='network-ssd',
masternode_disk_size=30,

# Compute nodes
computenode_resource_preset='s2.small',  # 4 CPU, 16GB RAM
computenode_disk_type='network-ssd',
computenode_disk_size=30,
computenode_count=1,
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ YARN —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ
- ‚úÖ –ë—ã—Å—Ç—Ä–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
- ‚úÖ –ù–∞–¥—ë–∂–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Spark jobs

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
- –ù—É–∂–Ω–æ 8 CPU (4 master + 4 compute)
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–≤–æ—Ç—É: `yc quota list --folder-id=<folder-id>`

#### 2. –û—Ç–∫–ª—é—á–∏—Ç—å initialization actions

```python
# –û—Ç–∫–ª—é—á–∏—Ç—å initialization action
# initialization_actions=[
#     InitializationAction(
#         uri=f's3a://{S3_BUCKET}/scripts/dataproc-init.sh',
#         timeout=600
#     )
# ],
```

**–ü–æ—á–µ–º—É:**
- –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –≤ Spark job (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ)
- –ú–µ–Ω—å—à–µ —Ç–æ—á–µ–∫ –æ—Ç–∫–∞–∑–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
- –ë—ã—Å—Ç—Ä–µ–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è

#### 3. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é

```python
# –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ services
services=['YARN', 'SPARK', 'LIVY'],

# –ë–µ–∑ data nodes (–∏—Å–ø–æ–ª—å–∑—É–µ–º compute nodes)
datanode_count=0,

# –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ Spark properties
properties={
    'spark:spark.sql.warehouse.dir': f's3a://{S3_BUCKET}/spark-warehouse/',
    'spark:spark.yarn.submit.waitAppCompletion': 'true',
},
```

## –ü—Ä–æ–±–ª–µ–º–∞: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–≤–æ—Ç—ã CPU

### –°–∏–º–ø—Ç–æ–º

```
Error: insufficient Compute quota for: compute.instanceCores.count
required 8.00 but available 6.00
```

### –†–µ—à–µ–Ω–∏–µ 1: –£–≤–µ–ª–∏—á–∏—Ç—å –∫–≤–æ—Ç—É (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

**–ß–µ—Ä–µ–∑ –∫–æ–Ω—Å–æ–ª—å:**
1. –û—Ç–∫—Ä–æ–π—Ç–µ [Yandex Cloud Console](https://console.cloud.yandex.ru/)
2. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –≤–∞—à folder
3. –°–ª–µ–≤–∞ –≤—ã–±–µ—Ä–∏—Ç–µ "–ö–≤–æ—Ç—ã"
4. –ù–∞–π–¥–∏—Ç–µ "Compute Cloud" ‚Üí "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ vCPU"
5. –ù–∞–∂–º–∏—Ç–µ "–£–≤–µ–ª–∏—á–∏—Ç—å –∫–≤–æ—Ç—ã"
6. –£–∫–∞–∂–∏—Ç–µ:
   - –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: 6
   - –¢—Ä–µ–±—É–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: 12-16 (—Å –∑–∞–ø–∞—Å–æ–º)
   - –ü—Ä–∏—á–∏–Ω–∞: "MLOps –ø—Ä–æ–µ–∫—Ç —Å DataProc –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π"
7. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –∑–∞–ø—Ä–æ—Å

**–ß–µ—Ä–µ–∑ CLI:**
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –∫–≤–æ—Ç—ã
yc quota list --folder-id=<folder-id>

# –ß–µ—Ä–µ–∑ –ø–æ–¥–¥–µ—Ä–∂–∫—É
# 1. –°–æ–∑–¥–∞–π—Ç–µ —Ç–∏–∫–µ—Ç –≤ –∫–æ–Ω—Å–æ–ª–∏
# 2. –ò–ª–∏ –Ω–∞–ø–∏—à–∏—Ç–µ –Ω–∞ support@cloud.yandex.ru
```

**–°—Ä–æ–∫–∏:**
- –û–±—ã—á–Ω–æ 1-4 —á–∞—Å–∞ –≤ —Ä–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è
- –î–æ 24 —á–∞—Å–æ–≤ –≤ –≤—ã—Ö–æ–¥–Ω—ã–µ

### –†–µ—à–µ–Ω–∏–µ 2: –û—Å–≤–æ–±–æ–¥–∏—Ç—å —Ä–µ—Å—É—Ä—Å—ã

**–í—Ä–µ–º–µ–Ω–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ VM:**

```bash
# –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ç–µ–∫—É—â–∏–µ VM
yc compute instance list

# –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Grafana (2 CPU)
yc compute instance stop <grafana-instance-id>

# –ò–ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å MLflow (2 CPU)
yc compute instance stop <mlflow-instance-id>

# –û—Å–≤–æ–±–æ–¥–∏—Ç—Å—è 2-4 CPU ‚Üí –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å DataProc
```

**–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:**
```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞—Ç–Ω–æ
yc compute instance start <instance-id>
```

### –†–µ—à–µ–Ω–∏–µ 3: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é (–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```python
# –í–ù–ò–ú–ê–ù–ò–ï: YARN –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–º!

# –í–∞—Ä–∏–∞–Ω—Ç 1: –¢–æ–ª—å–∫–æ master node (–±–µ–∑ compute)
masternode_resource_preset='s2.small',  # 4 CPU
computenode_count=0,  # –ë–µ–∑ compute nodes

# –í–∞—Ä–∏–∞–Ω—Ç 2: s2.micro (—Ä–∏—Å–∫ –ø–∞–¥–µ–Ω–∏—è YARN)
masternode_resource_preset='s2.micro',  # 2 CPU
computenode_resource_preset='s2.micro',  # 2 CPU
# –ò—Ç–æ–≥–æ: 4 CPU (–Ω–æ YARN –Ω–µ—Å—Ç–∞–±–∏–ª–µ–Ω)
```

## –ü—Ä–æ–±–ª–µ–º–∞: DataProc –∫–ª–∞—Å—Ç–µ—Ä —Å–æ–∑–¥–∞—ë—Ç—Å—è –æ—á–µ–Ω—å –¥–æ–ª–≥–æ

### –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è

```
‚úÖ –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞:    5-7 –º–∏–Ω—É—Ç
‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è YARN:   2-3 –º–∏–Ω—É—Ç—ã
‚úÖ –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ —Ä–∞–±–æ—Ç–µ:  7-10 –º–∏–Ω—É—Ç
```

### –ï—Å–ª–∏ –¥–æ–ª—å—à–µ 15 –º–∏–Ω—É—Ç

**1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏ –≤ –∫–æ–Ω—Å–æ–ª–∏:**
```bash
# –í Airflow UI ‚Üí Logs ‚Üí create_dataproc_cluster
# –ò–ª–∏ –≤ Yandex Cloud Console ‚Üí DataProc ‚Üí Clusters ‚Üí <cluster-id> ‚Üí Logs
```

**2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞:**
```bash
yc dataproc cluster list
yc dataproc cluster get <cluster-id>
```

**3. –¢–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:**
- –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ S3 ‚Üí –ø—Ä–æ–≤–µ—Ä—å—Ç–µ service account IAM roles
- –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç ‚Üí –ø—Ä–æ–≤–µ—Ä—å—Ç–µ NAT Gateway
- Security group –±–ª–æ–∫–∏—Ä—É–µ—Ç ‚Üí –¥–æ–±–∞–≤—å—Ç–µ –ø—Ä–∞–≤–∏–ª–∞

## –ü—Ä–æ–±–ª–µ–º–∞: Spark job –ø–∞–¥–∞–µ—Ç —Å OOM (Out of Memory)

### –°–∏–º–ø—Ç–æ–º

```
Error: Container killed by YARN for exceeding memory limits
```

### –†–µ—à–µ–Ω–∏–µ

```python
# –£–≤–µ–ª–∏—á–∏—Ç—å –ø–∞–º—è—Ç—å –¥–ª—è executor
properties={
    'spark:spark.executor.memory': '4g',
    'spark:spark.executor.cores': '2',
    'spark:spark.driver.memory': '4g',
    'spark:spark.memory.fraction': '0.8',
},

# –ò–ª–∏ —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
computenode_resource_preset='s2.medium',  # 8 CPU, 32GB RAM
```

## –ü—Ä–æ–±–ª–µ–º–∞: –ù–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ñ–∞–π–ª –≤ S3

### –°–∏–º–ø—Ç–æ–º

```
Error: Path does not exist: s3a://bucket/path/to/file
```

### –†–µ—à–µ–Ω–∏–µ

**1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Ñ–∞–π–ª –≤ S3:**
```bash
yc storage s3api list-objects --bucket <bucket-name> --prefix spark_jobs/
```

**2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å IAM —Ä–æ–ª–∏ service account:**
```bash
yc iam service-account list-access-bindings <service-account-id>
```

–ù—É–∂–Ω—ã —Ä–æ–ª–∏:
- `storage.viewer` (–º–∏–Ω–∏–º—É–º)
- `storage.editor` (–¥–ª—è –∑–∞–ø–∏—Å–∏)

**3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å S3 credentials –≤ DAG:**
```python
# –í create_pyspark_job
s3_main_script=f's3a://{S3_BUCKET}/spark_jobs/train_fish_model_minimal.py',
```

## Best Practices

### 1. –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–∞

| –ó–∞–¥–∞—á–∞ | Master | Compute | –ò—Ç–æ–≥–æ |
|--------|--------|---------|-------|
| **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ** | s2.micro (2 CPU) | 0 | 2 CPU |
| **Development** | s2.small (4 CPU) | s2.small (4 CPU) √ó 1 | 8 CPU |
| **Production** | s2.medium (8 CPU) | s2.medium (8 CPU) √ó 2-3 | 24-32 CPU |

### 2. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏

```python
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ preemptible compute nodes (–¥–µ—à–µ–≤–ª–µ –≤ 3 —Ä–∞–∑–∞)
computenode_preemptible=True,

# –ê–≤—Ç–æ—É–¥–∞–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ—Å–ª–µ job
# (—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ delete_dataproc_cluster task)

# –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π timeout –¥–ª—è init actions
initialization_actions=[
    InitializationAction(
        uri=f's3a://{S3_BUCKET}/scripts/init.sh',
        timeout=300  # 5 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º
    )
],
```

### 3. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

**–õ–æ–≥–∏ –≤ S3:**
```python
log_group_id='<log-group-id>',  # Yandex Cloud Logging
```

**YARN UI:**
```
http://<master-node-ip>:8088/cluster
```

**Spark History Server:**
```
http://<master-node-ip>:18080
```

### 4. –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

```python
# –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ security groups
security_group_ids=[SECURITY_GROUP_ID],

# –ù–µ —Ö—Ä–∞–Ω–∏—Ç–µ credentials –≤ –∫–æ–¥–µ
S3_ACCESS_KEY = Variable.get('S3_ACCESS_KEY')  # –ò–∑ Lockbox

# SSH –∫–ª—é—á–∏ –∏–∑ Airflow Variables
ssh_public_keys=[SSH_PUBLIC_KEY],
```

## –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã

```bash
# –°–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
yc dataproc cluster list

# –î–µ—Ç–∞–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
yc dataproc cluster get <cluster-id>

# –£–¥–∞–ª–∏—Ç—å –∑–∞–≤–∏—Å—à–∏–π –∫–ª–∞—Å—Ç–µ—Ä
yc dataproc cluster delete <cluster-id>

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–≤–æ—Ç—ã
yc quota list --folder-id=<folder-id>

# –õ–æ–≥–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
yc logging read --group-id=<log-group-id> --filter="resource_id='<cluster-id>'"

# –°–ø–∏—Å–æ–∫ jobs
yc dataproc job list --cluster-name=<cluster-name>

# –û—Ç–º–µ–Ω–∏—Ç—å job
yc dataproc job cancel --cluster-name=<cluster-name> --job-id=<job-id>
```

## –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º

- [ ] –ö–≤–æ—Ç–∞ CPU –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞ (–º–∏–Ω–∏–º—É–º 8 CPU)
- [ ] Service account –∏–º–µ–µ—Ç –≤—Å–µ –Ω—É–∂–Ω—ã–µ —Ä–æ–ª–∏
- [ ] –§–∞–π–ª—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ S3 (spark script, dataset)
- [ ] NAT Gateway –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç
- [ ] Security group —Ä–∞–∑—Ä–µ—à–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Ç—Ä–∞—Ñ–∏–∫
- [ ] SSH –∫–ª—é—á–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
- [ ] Airflow variables –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã
- [ ] MLflow –¥–æ—Å—Ç—É–ø–µ–Ω –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã:**

```python
masternode_resource_preset='s2.small',      # 4 CPU, 16GB RAM
masternode_disk_type='network-ssd',
masternode_disk_size=30,

computenode_resource_preset='s2.small',     # 4 CPU, 16GB RAM
computenode_disk_type='network-ssd',
computenode_disk_size=30,
computenode_count=1,

# –ë–µ–∑ initialization actions
# –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –≤ Spark job
```

**–ò—Ç–æ–≥–æ: 8 CPU, 32GB RAM, ~$3-5 –∑–∞ –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è (20-25 –º–∏–Ω—É—Ç)**

