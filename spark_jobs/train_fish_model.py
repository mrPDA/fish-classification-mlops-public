"""
PySpark —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–± —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Transfer Learning –∏ MLflow
"""

import os
import sys
import subprocess
import argparse
import shutil
from datetime import datetime

# === –ü–†–û–í–ï–†–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô ===
# DataProc 2.1 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç conda –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å Python 3.8 (/opt/conda/bin/python)
# –°–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ DataProc 2.1:
# - Python: 3.8.13
# - –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: scikit-learn 0.24.1, pandas 1.2.4, numpy 1.20.1, boto3 1.16.7, botocore 1.19.7
# - –ö–†–ò–¢–ò–ß–ù–û: botocore 1.19.7 —Ç—Ä–µ–±—É–µ—Ç urllib3<1.26,>=1.25.4
# - –ù—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å: tensorflow, mlflow, pillow
print("üîß Checking ML dependencies in DataProc 2.1 conda environment (Python 3.8)...")

# –°–ø–∏—Å–æ–∫ –ø–∞–∫–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å/—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
required_packages = {
    'tensorflow': 'tensorflow==2.13.0',  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
    'mlflow': 'mlflow==2.9.2',  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º (–Ω–µ –≤ –±–∞–∑–æ–≤–æ–º –æ–±—Ä–∞–∑–µ 2.1)
    'boto3': None,   # –£–∂–µ –µ—Å—Ç—å: 1.16.7
    'pandas': None,  # –£–∂–µ –µ—Å—Ç—å: 1.2.4
    'numpy': None,   # –£–∂–µ –µ—Å—Ç—å: 1.20.1, –Ω–æ –±—É–¥–µ–º –æ–±–Ω–æ–≤–ª—è—Ç—å –¥–æ 1.24.3
    'scipy': 'scipy==1.10.1',  # –û–±–Ω–æ–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å numpy 1.24.3
    'pillow': 'pillow==10.1.0',  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
    'scikit-learn': None,  # –£–∂–µ –µ—Å—Ç—å: 0.24.1
}

packages_to_install = []

for pkg_name, pkg_spec in required_packages.items():
    try:
        import_name = pkg_name.replace('-', '_').lower()
        if pkg_name == 'pillow':
            import_name = 'PIL'
        elif pkg_name == 'scikit-learn':
            import_name = 'sklearn'
        
        __import__(import_name)
        print(f"‚úÖ {pkg_name} already available")
    except ImportError:
        if pkg_spec:
            print(f"‚ö†Ô∏è  {pkg_name} not found, will install: {pkg_spec}")
            packages_to_install.append(pkg_spec)
        else:
            print(f"‚ö†Ô∏è  {pkg_name} not found but should be in conda!")

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–∞–∫–µ—Ç—ã
if packages_to_install:
    print(f"üì¶ Installing {len(packages_to_install)} packages...")
    
    # –ö–†–ò–¢–ò–ß–ù–û: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞–∫–µ—Ç—ã –ë–ï–ó –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º --no-deps –¥–ª—è TensorFlow –∏ MLflow, –∑–∞—Ç–µ–º —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é
    
    # –®–∞–≥ 1: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–∞–∑–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏
    print("üîß Step 1: Installing compatible base dependencies...")
    base_deps = [
        'urllib3<1.26,>=1.25.4',  # –°–æ–≤–º–µ—Å—Ç–∏–º–∞—è —Å botocore 1.19.7
        'typing-extensions<4.6.0,>=3.6.6',  # –î–ª—è TensorFlow
        'protobuf<5,>=3.12.0',  # –î–ª—è TensorFlow –∏ MLflow
        'click<9,>=7.0',  # –î–ª—è MLflow
        'packaging<24',  # –î–ª—è MLflow
    ]
    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--user'] + base_deps)
    print("‚úÖ Base dependencies installed!")
    
    # –®–∞–≥ 2: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º TensorFlow –∏ MLflow –ë–ï–ó –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    print("üîß Step 2: Installing TensorFlow and MLflow with --no-deps...")
    for pkg in packages_to_install:
        print(f"   Installing {pkg} (no-deps)...")
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--user', '--no-deps', pkg])
    print("‚úÖ TensorFlow and MLflow installed!")
    
    # –®–∞–≥ 3: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º numpy 1.24.3 –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º sys.path –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
    print("üîß Step 3a: Installing numpy 1.24.3...")
    subprocess.check_call([
        sys.executable, '-m', 'pip', 'install', '--user',
        '--force-reinstall', '--no-deps', 'numpy==1.24.3'
    ])
    print("‚úÖ NumPy 1.24.3 installed!")
    
    # –ö–†–ò–¢–ò–ß–ù–û: –í—Å—Ç–∞–≤–ª—è–µ–º user-local –ü–ï–†–ï–î –≤—Å–µ–º–∏ –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏ –ø—É—Ç—è–º–∏, –≤–∫–ª—é—á–∞—è conda
    print("üîß Step 3b: Reconfiguring sys.path to prioritize user-local packages...")
    user_site = '/home/dataproc-agent/.local/lib/python3.8/site-packages'
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è user_site –∏–∑ sys.path
    while user_site in sys.path:
        sys.path.remove(user_site)
    # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ —Å–∞–º–æ–µ –Ω–∞—á–∞–ª–æ
    sys.path.insert(0, user_site)
    print(f"   ‚úÖ sys.path[0] = {sys.path[0]}")
    
    # –ö–†–ò–¢–ò–ß–ù–û: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º scipy –û–¢–î–ï–õ–¨–ù–û –≤ user-local –î–û –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏ numpy
    print("üîß Step 3c: Installing scipy 1.10.1 in user-local...")
    subprocess.check_call([
        sys.executable, '-m', 'pip', 'install', '--user',
        '--force-reinstall', '--no-deps', 'scipy==1.10.1'
    ])
    print("‚úÖ SciPy 1.10.1 installed in user-local!")
    
    # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û: –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ–º numpy 1.24.3 –ò scipy 1.10.1 –≤ sys.modules
    print("üîß Step 3d: Pre-loading numpy 1.24.3 and scipy 1.10.1 into sys.modules...")
    # –£–¥–∞–ª—è–µ–º –ª—é–±—ã–µ —Ä–∞–Ω–µ–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ numpy –∏ scipy –º–æ–¥—É–ª–∏
    to_reload = [key for key in sys.modules.keys() if key.startswith(('numpy', 'scipy'))]
    for mod in to_reload:
        del sys.modules[mod]
    # –¢–µ–ø–µ—Ä—å –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º numpy –∏ scipy - –î–û–õ–ñ–ù–´ –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è –∏–∑ user-local
    import numpy as np_test
    import scipy as sp_test
    print(f"   ‚úÖ Pre-loaded numpy: {np_test.__version__} from {np_test.__file__}")
    print(f"   ‚úÖ Pre-loaded scipy: {sp_test.__version__} from {sp_test.__file__}")
    if not np_test.__version__.startswith('1.24'):
        raise RuntimeError(f"‚ùå WRONG NUMPY! Expected 1.24.x, got {np_test.__version__}")
    if not sp_test.__version__.startswith('1.10'):
        raise RuntimeError(f"‚ùå WRONG SCIPY! Expected 1.10.x, got {sp_test.__version__}")
    print("   ‚úÖ NumPy 1.24.3 and SciPy 1.10.1 successfully pre-loaded!")
    
    # –ó–∞—Ç–µ–º —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Å --upgrade-strategy only-if-needed
    print("üîß Step 3e: Installing remaining dependencies with constraints...")
    ml_deps = [
        'absl-py>=1.0.0',
        'astunparse>=1.6.0',
        'flatbuffers>=23.1.21',
        'gast<=0.4.0,>=0.2.1',
        'google-pasta>=0.1.1',
        'h5py>=2.9.0',
        'keras<2.14,>=2.13.1',
        'libclang>=13.0.0',
        'opt-einsum>=2.3.2',
        'tensorboard<2.14,>=2.13',
        'tensorflow-estimator<2.14,>=2.13.0',
        'tensorflow-io-gcs-filesystem>=0.23.1',
        'termcolor>=1.1.0',
        'wrapt>=1.11.0',
        # MLflow dependencies
        'alembic!=1.10.0,<2',
        'cloudpickle<4',
        'databricks-cli<1,>=0.8.7',
        'docker<7,>=4.0.0',
        'entrypoints<1',
        'Flask<4',
        'gitpython<4,>=2.1.0',
        'gunicorn<22',
        'importlib-metadata!=4.7.0,<8,>=3.7.0',
        'Jinja2<4,>=2.11',
        'markdown<4,>=3.3',
        'pytz<2024',
        'pyyaml<7,>=5.1',
        'querystring-parser<2',
        'sqlparse<1,>=0.4.0',
    ]
    subprocess.check_call([
        sys.executable, '-m', 'pip', 'install', '--user',
        '--upgrade-strategy', 'only-if-needed'  # –ù–ï –æ–±–Ω–æ–≤–ª—è—Ç—å —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ!
    ] + ml_deps)
    print("‚úÖ All dependencies installed!")
    
    print("‚úÖ All missing packages installed successfully!")
else:
    print("‚úÖ All dependencies already available!")

print("‚úÖ Dependency check complete!")

# –£–î–ê–õ–Ø–ï–ú –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –ø–∞–∫–µ—Ç—ã –∏–∑ user-local (urllib3, typing-extensions)
user_site = '/home/dataproc-agent/.local/lib/python3.8/site-packages'

# 1. urllib3 (–∫–æ–Ω—Ñ–ª–∏–∫—Ç—É–µ—Ç —Å boto3)
urllib3_path = os.path.join(user_site, 'urllib3')
if os.path.exists(urllib3_path):
    print(f"üóëÔ∏è  Removing incompatible urllib3 from {urllib3_path}")
    shutil.rmtree(urllib3_path)
    # –¢–∞–∫–∂–µ —É–¥–∞–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    for item in os.listdir(user_site):
        if item.startswith('urllib3-'):
            item_path = os.path.join(user_site, item)
            if os.path.isdir(item_path):
                shutil.rmtree(item_path)
            else:
                os.remove(item_path)
    print(f"‚úÖ Removed incompatible urllib3")

# 2. typing-extensions (–∫–æ–Ω—Ñ–ª–∏–∫—Ç—É–µ—Ç —Å TensorFlow –∏ sqlalchemy)
typing_ext_path = os.path.join(user_site, 'typing_extensions.py')
if os.path.exists(typing_ext_path):
    print(f"üóëÔ∏è  Removing typing-extensions 4.13.2 from {typing_ext_path}")
    os.remove(typing_ext_path)
    # –£–¥–∞–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    for item in os.listdir(user_site):
        if item.startswith('typing_extensions-'):
            item_path = os.path.join(user_site, item)
            if os.path.isdir(item_path):
                shutil.rmtree(item_path)
            else:
                os.remove(item_path)
    print(f"‚úÖ Removed typing-extensions 4.13.2 (will use 4.5.0 for TensorFlow)")

# Spark imports
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# TensorFlow/Keras imports
print(f"üîç Attempting to import tensorflow from sys.path:")
for i, path in enumerate(sys.path[:5]):
    print(f"   [{i}] {path}")
import tensorflow as tf
print(f"‚úÖ TensorFlow imported successfully! Version: {tf.__version__}")
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# MLflow imports
import mlflow
import mlflow.keras
from mlflow.tracking import MlflowClient

# S3/Boto3 imports
import boto3
from botocore.client import Config


def parse_args():
    """–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(description='Train fish classification model')
    
    # S3 –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--s3-endpoint', required=True, help='S3 endpoint URL')
    parser.add_argument('--s3-bucket', required=True, help='S3 bucket name')
    parser.add_argument('--s3-access-key', required=True, help='S3 access key')
    parser.add_argument('--s3-secret-key', required=True, help='S3 secret key')
    
    # MLflow –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--mlflow-tracking-uri', required=True, help='MLflow tracking URI')
    parser.add_argument('--mlflow-experiment', default='fish-classification', help='MLflow experiment name')
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    parser.add_argument('--num-classes', type=int, default=9, help='Number of fish classes')
    parser.add_argument('--image-size', type=int, default=224, help='Image size')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size')
    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs')
    parser.add_argument('--learning-rate', type=float, default=0.001, help='Learning rate')
    
    return parser.parse_args()


def setup_s3_client(endpoint, access_key, secret_key):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ S3 –∫–ª–∏–µ–Ω—Ç–∞"""
    return boto3.client(
        's3',
        endpoint_url=endpoint,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        config=Config(signature_version='s3v4')
    )


def download_dataset_from_s3(s3_client, bucket, local_path='/tmp/fish_dataset'):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ S3"""
    print(f"üì• Downloading dataset from S3 bucket: {bucket}")
    
    os.makedirs(local_path, exist_ok=True)
    
    # –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    prefix = 'datasets/processed_images/'
    
    paginator = s3_client.get_paginator('list_objects_v2')
    pages = paginator.paginate(Bucket=bucket, Prefix=prefix)
    
    file_count = 0
    for page in pages:
        if 'Contents' not in page:
            continue
            
        for obj in page['Contents']:
            key = obj['Key']
            if key.endswith('/'):  # Skip directories
                continue
                
            # –°–æ–∑–¥–∞—ë–º –ª–æ–∫–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
            local_file = os.path.join(local_path, key.replace(prefix, ''))
            os.makedirs(os.path.dirname(local_file), exist_ok=True)
            
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
            s3_client.download_file(bucket, key, local_file)
            file_count += 1
            
            if file_count % 100 == 0:
                print(f"   Downloaded {file_count} files...")
    
    print(f"‚úÖ Downloaded {file_count} files to {local_path}")
    return local_path


def create_transfer_learning_model(num_classes, image_size, learning_rate):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å Transfer Learning –Ω–∞ –±–∞–∑–µ MobileNetV2
    """
    print(f"üèóÔ∏è Creating transfer learning model (MobileNetV2)")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å MobileNetV2 (ImageNet weights)
    base_model = MobileNetV2(
        input_shape=(image_size, image_size, 3),
        include_top=False,
        weights='imagenet'
    )
    
    # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –±–∞–∑–æ–≤—ã–µ —Å–ª–æ–∏
    base_model.trainable = False
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–±
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.3)(x)
    predictions = Dense(num_classes, activation='softmax')(x)
    
    # –°–æ–∑–¥–∞—ë–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    model = Model(inputs=base_model.input, outputs=predictions)
    
    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy', 'top_k_categorical_accuracy']
    )
    
    print(f"‚úÖ Model created with {model.count_params():,} parameters")
    print(f"   Trainable parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}")
    
    return model


def train_model(model, dataset_path, batch_size, epochs, image_size):
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ImageDataGenerator
    """
    print(f"üéì Starting model training...")
    
    # Data augmentation –¥–ª—è train set
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        horizontal_flip=True,
        zoom_range=0.2,
        shear_range=0.2,
        fill_mode='nearest',
        validation_split=0.2  # 80% train, 20% validation
    )
    
    # –¢–æ–ª—å–∫–æ rescale –¥–ª—è validation set
    val_datagen = ImageDataGenerator(
        rescale=1./255,
        validation_split=0.2
    )
    
    # –¢–æ–ª—å–∫–æ rescale –¥–ª—è test set
    test_datagen = ImageDataGenerator(rescale=1./255)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º train –¥–∞–Ω–Ω—ã–µ
    train_generator = train_datagen.flow_from_directory(
        os.path.join(dataset_path, 'train'),
        target_size=(image_size, image_size),
        batch_size=batch_size,
        class_mode='categorical',
        subset='training',
        shuffle=True
    )
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º validation –¥–∞–Ω–Ω—ã–µ
    validation_generator = val_datagen.flow_from_directory(
        os.path.join(dataset_path, 'train'),
        target_size=(image_size, image_size),
        batch_size=batch_size,
        class_mode='categorical',
        subset='validation',
        shuffle=False
    )
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ test set
    test_dir = os.path.join(dataset_path, 'test')
    test_generator = None
    if os.path.exists(test_dir) and os.listdir(test_dir):
        test_generator = test_datagen.flow_from_directory(
            test_dir,
            target_size=(image_size, image_size),
            batch_size=batch_size,
            class_mode='categorical',
            shuffle=False
        )
        print(f"   Test samples: {test_generator.samples}")
    
    print(f"   Train samples: {train_generator.samples}")
    print(f"   Validation samples: {validation_generator.samples}")
    print(f"   Classes: {train_generator.class_indices}")
    
    # Callbacks
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True,
        verbose=1
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=2,
        min_lr=1e-7,
        verbose=1
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    history = model.fit(
        train_generator,
        epochs=epochs,
        validation_data=validation_generator,
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )
    
    return history, train_generator.class_indices, test_generator


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    args = parse_args()
    
    print("=" * 80)
    print("üêü Fish Classification Model Training with Transfer Learning")
    print("=" * 80)
    print(f"Start time: {datetime.now()}")
    print(f"MLflow Tracking URI: {args.mlflow_tracking_uri}")
    print(f"S3 Bucket: {args.s3_bucket}")
    print(f"Image size: {args.image_size}x{args.image_size}")
    print(f"Batch size: {args.batch_size}")
    print(f"Epochs: {args.epochs}")
    print(f"Learning rate: {args.learning_rate}")
    print("=" * 80)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLflow
    mlflow.set_tracking_uri(args.mlflow_tracking_uri)
    mlflow.set_experiment(args.mlflow_experiment)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ S3
    s3_client = setup_s3_client(
        args.s3_endpoint,
        args.s3_access_key,
        args.s3_secret_key
    )
    
    # –ù–∞—á–∏–Ω–∞–µ–º MLflow run
    with mlflow.start_run(run_name=f"fish-classification-{datetime.now().strftime('%Y%m%d-%H%M%S')}"):
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        mlflow.log_param("base_model", "MobileNetV2")
        mlflow.log_param("pretrained_weights", "imagenet")
        mlflow.log_param("num_classes", args.num_classes)
        mlflow.log_param("image_size", args.image_size)
        mlflow.log_param("batch_size", args.batch_size)
        mlflow.log_param("epochs", args.epochs)
        mlflow.log_param("learning_rate", args.learning_rate)
        mlflow.log_param("optimizer", "Adam")
        mlflow.log_param("loss", "categorical_crossentropy")
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        dataset_path = download_dataset_from_s3(
            s3_client,
            args.s3_bucket
        )
        
        # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å
        model = create_transfer_learning_model(
            args.num_classes,
            args.image_size,
            args.learning_rate
        )
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        history, class_indices, test_generator = train_model(
            model,
            dataset_path,
            args.batch_size,
            args.epochs,
            args.image_size
        )
        
        # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        final_train_acc = history.history['accuracy'][-1]
        final_val_acc = history.history['val_accuracy'][-1]
        final_train_loss = history.history['loss'][-1]
        final_val_loss = history.history['val_loss'][-1]
        
        mlflow.log_metric("final_train_accuracy", final_train_acc)
        mlflow.log_metric("final_val_accuracy", final_val_acc)
        mlflow.log_metric("final_train_loss", final_train_loss)
        mlflow.log_metric("final_val_loss", final_val_loss)
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ test set (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        test_acc = None
        test_loss = None
        if test_generator is not None:
            print("üß™ Evaluating model on test set...")
            test_results = model.evaluate(test_generator, verbose=1)
            test_loss = test_results[0]
            test_acc = test_results[1]
            test_top_k = test_results[2] if len(test_results) > 2 else None
            
            mlflow.log_metric("test_accuracy", test_acc)
            mlflow.log_metric("test_loss", test_loss)
            if test_top_k is not None:
                mlflow.log_metric("test_top_k_accuracy", test_top_k)
            
            print(f"‚úÖ Test accuracy: {test_acc:.4f}")
            print(f"‚úÖ Test loss: {test_loss:.4f}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
        for epoch in range(len(history.history['accuracy'])):
            mlflow.log_metric("train_accuracy", history.history['accuracy'][epoch], step=epoch)
            mlflow.log_metric("val_accuracy", history.history['val_accuracy'][epoch], step=epoch)
            mlflow.log_metric("train_loss", history.history['loss'][epoch], step=epoch)
            mlflow.log_metric("val_loss", history.history['val_loss'][epoch], step=epoch)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ MLflow —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        print("üíæ Saving model to MLflow...")
        mlflow.keras.log_model(
            model,
            "model",
            registered_model_name="fish-classification-mobilenetv2"
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º class indices
        mlflow.log_dict(class_indices, "class_indices.json")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        client = MlflowClient()
        run_id = mlflow.active_run().info.run_id
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏
        latest_versions = client.get_latest_versions("fish-classification-mobilenetv2")
        if latest_versions:
            model_version = latest_versions[0].version
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            client.set_model_version_tag(
                name="fish-classification-mobilenetv2",
                version=model_version,
                key="val_accuracy",
                value=f"{final_val_acc:.4f}"
            )
            
            if test_acc is not None:
                client.set_model_version_tag(
                    name="fish-classification-mobilenetv2",
                    version=model_version,
                    key="test_accuracy",
                    value=f"{test_acc:.4f}"
                )
            
            client.set_model_version_tag(
                name="fish-classification-mobilenetv2",
                version=model_version,
                key="training_date",
                value=datetime.now().isoformat()
            )
            
            client.set_model_version_tag(
                name="fish-classification-mobilenetv2",
                version=model_version,
                key="base_model",
                value="MobileNetV2"
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            client.update_model_version(
                name="fish-classification-mobilenetv2",
                version=model_version,
                description=f"Fish classification model trained with Transfer Learning (MobileNetV2). "
                           f"Val Accuracy: {final_val_acc:.4f}, "
                           f"Test Accuracy: {test_acc:.4f if test_acc else 'N/A'}, "
                           f"Trained on {datetime.now().strftime('%Y-%m-%d')}"
            )
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π stage management
            # –ï—Å–ª–∏ accuracy —Ö–æ—Ä–æ—à–∞—è (>75%), –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ Staging
            if final_val_acc > 0.75:
                print(f"üéØ Model performance is good (val_acc: {final_val_acc:.4f} > 0.75)")
                print("üì¶ Promoting model to Staging stage...")
                
                client.transition_model_version_stage(
                    name="fish-classification-mobilenetv2",
                    version=model_version,
                    stage="Staging",
                    archive_existing_versions=False
                )
                print(f"‚úÖ Model version {model_version} promoted to Staging")
            else:
                print(f"‚ö†Ô∏è  Model performance is below threshold (val_acc: {final_val_acc:.4f} <= 0.75)")
                print("   Model will remain in 'None' stage")
            
            print(f"üì¶ Model registered: fish-classification-mobilenetv2 v{model_version}")
        
        print("=" * 80)
        print("‚úÖ Training completed successfully!")
        print(f"   Final train accuracy: {final_train_acc:.4f}")
        print(f"   Final validation accuracy: {final_val_acc:.4f}")
        if test_acc is not None:
            print(f"   Final test accuracy: {test_acc:.4f}")
        print(f"   Final train loss: {final_train_loss:.4f}")
        print(f"   Final validation loss: {final_val_loss:.4f}")
        if test_loss is not None:
            print(f"   Final test loss: {test_loss:.4f}")
        print(f"   MLflow run ID: {run_id}")
        print("=" * 80)
        print(f"End time: {datetime.now()}")


if __name__ == "__main__":
    main()
