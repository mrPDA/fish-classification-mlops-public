"""
Airflow DAG –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–± —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Transfer Learning –∏ MLflow

–≠—Ç–æ—Ç DAG:
1. –°–æ–∑–¥–∞—ë—Ç DataProc –∫–ª–∞—Å—Ç–µ—Ä
2. –ó–∞–≥—Ä—É–∂–∞–µ—Ç PySpark —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è –≤ S3
3. –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å transfer learning (MobileNetV2)
4. –õ–æ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ MLflow
5. –£–¥–∞–ª—è–µ—Ç DataProc –∫–ª–∞—Å—Ç–µ—Ä
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.yandex.operators.yandexcloud_dataproc import (
    DataprocCreateClusterOperator,
    DataprocCreatePysparkJobOperator,
    DataprocDeleteClusterOperator
)
from airflow.providers.yandex.utils.user_agent import provider_user_agent
from yandex.cloud.dataproc.v1.cluster_pb2 import InitializationAction
from airflow.models import Variable
from airflow.exceptions import AirflowException
import logging
import requests
import os

logger = logging.getLogger(__name__)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ Lockbox (—Å fallback –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –∏–∑ Terraform)
# –≠—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è –±—É–¥—É—Ç –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞–Ω—ã –µ—Å–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –µ—Å—Ç—å –≤ Airflow Variables
FOLDER_ID = Variable.get('FOLDER_ID', 'b1gjj3po03aa3m4j8ps5')
ZONE = Variable.get('ZONE', 'ru-central1-a')
SUBNET_ID = Variable.get('SUBNET_ID', 'e9b5umsufggihj02a3o1')
SECURITY_GROUP_ID = Variable.get('SECURITY_GROUP_ID', 'enpha1v51ug84ddqfob4')
SERVICE_ACCOUNT_ID = Variable.get('DATAPROC_SERVICE_ACCOUNT_ID', 'ajepv2durkr1nk9rnoui')
S3_BUCKET = Variable.get('S3_BUCKET_NAME', 'fish-classification-data-7wb4zv')
S3_ENDPOINT = Variable.get('S3_ENDPOINT_URL', 'https://storage.yandexcloud.net')

# S3 –∫–ª—é—á–∏ - –ø–æ–ª—É—á–∞–µ–º –∏–∑ Terraform outputs (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ Variables)
# –ò—Å–ø–æ–ª—å–∑—É–µ–º try-except –¥–ª—è –±–æ–ª–µ–µ –º—è–≥–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
try:
    S3_ACCESS_KEY = Variable.get('S3_ACCESS_KEY')
except:
    S3_ACCESS_KEY = 'YOUR_S3_ACCESS_KEY'  # –ü–æ–ª—É—á–∏—Ç–µ –∏–∑ Yandex Cloud –∏–ª–∏ Lockbox
    
try:
    S3_SECRET_KEY = Variable.get('S3_SECRET_KEY')
except:
    S3_SECRET_KEY = 'YOUR_S3_SECRET_KEY'  # –ü–æ–ª—É—á–∏—Ç–µ –∏–∑ Yandex Cloud –∏–ª–∏ Lockbox

# MLflow URI - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —Ö–æ—Å—Ç–∞
def get_mlflow_uri():
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–π MLflow URI
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–±–æ—á–∏–π
    """
    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ Variables
    custom_uri = Variable.get('MLFLOW_TRACKING_URI', default_var=None)
    if custom_uri:
        return custom_uri
    
    # –°–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ö–æ—Å—Ç–æ–≤ (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π IP –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ)
    mlflow_ip_internal = Variable.get('MLFLOW_IP', '10.11.0.18')
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π IP (–æ–Ω –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞)
    return f'http://{mlflow_ip_internal}:5000'

MLFLOW_URI = get_mlflow_uri()

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
NUM_CLASSES = int(Variable.get('NUM_CLASSES', '9'))
IMAGE_SIZE = int(Variable.get('IMAGE_SIZE', '224'))
BATCH_SIZE = int(Variable.get('BATCH_SIZE', '32'))
EPOCHS = int(Variable.get('EPOCHS', '10'))
LEARNING_RATE = float(Variable.get('LEARNING_RATE', '0.001'))

# SSH –∫–ª—é—á
SSH_PUBLIC_KEY = Variable.get('YC_SSH_PUBLIC_KEY', 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCtV1GrBDN0NKa0VRfqBaGRyFsW6n4pKN7rqKWuStFo6If+6twnYwDYntTaXQBSQcIwF984u5NMP5BVMd8QKSBEvQK/IL9UFjdr3wB73+hKYMpoK9JPY2wPN6ECoxbGzkINZ+vhOgtnHytqGT7Sh79hoB6mKuJg0+yyMbxk0Lq/k7rtKlQYsm28rea4pIlvGmWgUXqzIcnj02K7x+hNGYYbe3kMlUY3MmvKaV1aM0P80gzWgoM0prpbSmdHUTkoP7QdkVlXHNU1xHINBU2lACt5aTL2OOlw/YLcaIDHmvKtghcpfRRskpc4VnJCLqPZgEIA2BTBe85FMcWwBOwTj4nt4YkbDzCgfDVi+K0SevvAKXJla+2H6KEMpUK6pUlpnnm2Q5UKkEiWd+kZ5Tc5ip0gKxkRPG+Q2lT28CbSgqGWmU0EkK2K1je+E/mWRkBybXyObwDHbDYNWmMF0Ztxoo6Rp7WgF2rtNOzeI58pyXFKI+Qn0vyri7s9aLVSBzAey6LVikYs+W5Vpipioh1sI0hETegDyUMQzrmoufoQeGPCBA5tsdAS715mSEhDey326qN5gVNVYcya4nEn2wJ9hNNS1NoSoy2+xvT5NMrLSLRso2Li/P0ud3dd1Xy5AVoMifzrufgZnFJY7nJUnY+0KgElbP35rsZRdsiRMuc5fVpRuQ== denispukinov@MacBook-Pro-Denis.local')

# DAG configuration
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'fish_classification_training_full',
    default_args=default_args,
    description='Full pipeline: DataProc + Transfer Learning + MLflow',
    schedule_interval=None,  # Manual trigger only
    start_date=datetime(2025, 10, 5),
    catchup=False,
    tags=['ml', 'fish-classification', 'dataproc', 'mlflow', 'transfer-learning'],
)


# Task 0: –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è
def validate_environment(**context):
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–æ–≤
    """
    logger.info("=" * 80)
    logger.info("üîç Validating environment and variables...")
    logger.info("=" * 80)
    
    errors = []
    warnings = []
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    required_vars = {
        'FOLDER_ID': FOLDER_ID,
        'ZONE': ZONE,
        'SUBNET_ID': SUBNET_ID,
        'SECURITY_GROUP_ID': SECURITY_GROUP_ID,
        'DATAPROC_SERVICE_ACCOUNT_ID': SERVICE_ACCOUNT_ID,
        'S3_BUCKET_NAME': S3_BUCKET,
        'S3_ACCESS_KEY': S3_ACCESS_KEY,
        'S3_SECRET_KEY': S3_SECRET_KEY,
        'MLFLOW_TRACKING_URI': MLFLOW_URI,
    }
    
    logger.info("üìã Checking required variables...")
    for var_name, var_value in required_vars.items():
        if not var_value:
            errors.append(f"‚ùå Variable '{var_name}' is empty")
        elif var_value == 'placeholder':
            warnings.append(f"‚ö†Ô∏è  Variable '{var_name}' has placeholder value (using fallback)")
            logger.info(f"   ‚ö†Ô∏è  {var_name}: placeholder (fallback will be used)")
        else:
            logger.info(f"   ‚úÖ {var_name}: {var_value[:20]}..." if len(var_value) > 20 else f"   ‚úÖ {var_name}: {var_value}")
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
    logger.info("\nü§ñ Checking model parameters...")
    model_params = {
        'NUM_CLASSES': NUM_CLASSES,
        'IMAGE_SIZE': IMAGE_SIZE,
        'BATCH_SIZE': BATCH_SIZE,
        'EPOCHS': EPOCHS,
        'LEARNING_RATE': LEARNING_RATE,
    }
    
    for param_name, param_value in model_params.items():
        logger.info(f"   ‚úÖ {param_name}: {param_value}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞–∑—É–º–Ω–æ—Å—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏–π
        if param_name == 'NUM_CLASSES' and (param_value < 2 or param_value > 100):
            warnings.append(f"‚ö†Ô∏è  NUM_CLASSES={param_value} seems unusual (expected 2-100)")
        
        if param_name == 'IMAGE_SIZE' and param_value not in [128, 160, 192, 224, 256]:
            warnings.append(f"‚ö†Ô∏è  IMAGE_SIZE={param_value} is non-standard (recommended: 224)")
        
        if param_name == 'BATCH_SIZE' and (param_value < 8 or param_value > 128):
            warnings.append(f"‚ö†Ô∏è  BATCH_SIZE={param_value} might cause issues (recommended: 16-64)")
        
        if param_name == 'EPOCHS' and param_value < 1:
            errors.append(f"‚ùå EPOCHS={param_value} must be >= 1")
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ MLflow
    logger.info("\nüîç Checking MLflow availability...")
    logger.info(f"   Testing URI: {MLFLOW_URI}")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ö–æ—Å—Ç–∞—Ö
    mlflow_ip_internal = Variable.get('MLFLOW_IP', '10.11.0.18')
    logger.info(f"   Internal IP: {mlflow_ip_internal}")
    logger.info(f"   Note: MLflow –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–µ–Ω –∏–∑ DataProc —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π IP")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å (–º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑ Airflow, –Ω–æ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑ DataProc)
    try:
        response = requests.get(MLFLOW_URI, timeout=5)
        if response.status_code == 200:
            logger.info(f"   ‚úÖ MLflow is available at {MLFLOW_URI}")
        else:
            warnings.append(f"‚ö†Ô∏è  MLflow returned status {response.status_code} (might work from DataProc)")
            logger.warning(f"   ‚ö†Ô∏è  Status {response.status_code}, but MLflow might be accessible from DataProc cluster")
    except requests.exceptions.RequestException as e:
        # –ù–µ —Å—á–∏—Ç–∞–µ–º —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ–π –æ—à–∏–±–∫–æ–π, —Ç.–∫. Airflow –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –¥—Ä—É–≥–æ–π —Å–µ—Ç–∏
        warnings.append(f"‚ö†Ô∏è  MLflow not accessible from Airflow (will try from DataProc)")
        logger.warning(f"   ‚ö†Ô∏è  Cannot connect from Airflow: {str(e)}")
        logger.warning(f"   ‚ÑπÔ∏è  This is OK if MLflow is in internal network - DataProc will access it directly")
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è PySpark —Å–∫—Ä–∏–ø—Ç–∞ –≤ S3
    logger.info("\nüì¶ Checking PySpark script in S3...")
    script_path = f"spark_jobs/train_fish_model.py"
    logger.info(f"   Expected location: s3://{S3_BUCKET}/{script_path}")
    # Note: –†–µ–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±—É–µ—Ç boto3, –∑–¥–µ—Å—å –ø—Ä–æ—Å—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∏—Ä—É–µ–º
    logger.info(f"   ‚ö†Ô∏è  S3 file existence check skipped (would require boto3)")
    
    # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ SSH –∫–ª—é—á–∞
    logger.info("\nüîë Checking SSH key...")
    if not SSH_PUBLIC_KEY or len(SSH_PUBLIC_KEY) < 100:
        errors.append("‚ùå SSH_PUBLIC_KEY is too short or not set")
    else:
        logger.info(f"   ‚úÖ SSH key is set ({len(SSH_PUBLIC_KEY)} chars)")
    
    # –ò—Ç–æ–≥–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    logger.info("\n" + "=" * 80)
    if errors:
        logger.error("‚ùå VALIDATION FAILED!")
        for error in errors:
            logger.error(f"   {error}")
        if warnings:
            for warning in warnings:
                logger.warning(f"   {warning}")
        raise AirflowException(f"Environment validation failed with {len(errors)} error(s)")
    
    if warnings:
        logger.warning(f"‚ö†Ô∏è  Validation passed with {len(warnings)} warning(s):")
        for warning in warnings:
            logger.warning(f"   {warning}")
    else:
        logger.info("‚úÖ All validations passed!")
    
    logger.info("=" * 80)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ XCom –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–¥–∞—á
    return {
        'validation_passed': True,
        'mlflow_uri': MLFLOW_URI,
        's3_bucket': S3_BUCKET,
        'num_classes': NUM_CLASSES,
        'image_size': IMAGE_SIZE,
        'batch_size': BATCH_SIZE,
        'epochs': EPOCHS,
        'learning_rate': LEARNING_RATE,
    }


validate_env = PythonOperator(
    task_id='validate_environment',
    python_callable=validate_environment,
    dag=dag,
)


# Task 1: –°–æ–∑–¥–∞–Ω–∏–µ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞
create_cluster = DataprocCreateClusterOperator(
    task_id='create_dataproc_cluster',
    folder_id=FOLDER_ID,
    cluster_name=f"fish-training-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
    cluster_description='DataProc cluster for fish classification training with ML dependencies',
    ssh_public_keys=SSH_PUBLIC_KEY,
    service_account_id=SERVICE_ACCOUNT_ID,
    subnet_id=SUBNET_ID,
    s3_bucket=S3_BUCKET,
    zone=ZONE,
    cluster_image_version='2.1',
    
    # Master node (s2.small –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ YARN)
    masternode_resource_preset='s2.small',  # 4 CPU, 16GB RAM
    masternode_disk_type='network-ssd',  # SSD –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    masternode_disk_size=30,
    
    # Compute nodes (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏)
    computenode_resource_preset='s2.small',  # 4 CPU, 16GB RAM
    computenode_disk_type='network-ssd',
    computenode_disk_size=30,
    computenode_count=1,
    computenode_max_hosts_count=1,
    
    # Services
    services=['YARN', 'SPARK', 'LIVY'],
    datanode_count=0,
    
    # Security group
    security_group_ids=[SECURITY_GROUP_ID],
    
    # –û–¢–ö–õ–Æ–ß–ï–ù–û: Initialization action (–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –≤ Spark job)
    # initialization_actions=[
    #     InitializationAction(
    #         uri=f's3a://{S3_BUCKET}/scripts/dataproc-init.sh',
    #         timeout=600
    #     )
    # ],
    
    # Spark properties - –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    properties={
        'spark:spark.sql.warehouse.dir': f's3a://{S3_BUCKET}/spark-warehouse/',
        'spark:spark.yarn.submit.waitAppCompletion': 'true',
    },
    
    connection_id='yandexcloud_default',
    dag=dag,
)


# Task 2: –ó–∞–ø—É—Å–∫ PySpark job –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
train_model = DataprocCreatePysparkJobOperator(
    task_id='train_model_with_transfer_learning',
    main_python_file_uri=f's3a://{S3_BUCKET}/spark_jobs/train_fish_model_minimal.py',
    python_file_uris=[
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ Python —Ñ–∞–π–ª—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω—ã
    ],
    file_uris=[
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω—ã
    ],
    args=[
        '--s3-endpoint', S3_ENDPOINT,
        '--s3-bucket', S3_BUCKET,
        '--s3-access-key', S3_ACCESS_KEY,
        '--s3-secret-key', S3_SECRET_KEY,
        '--mlflow-tracking-uri', MLFLOW_URI,
        '--mlflow-experiment', 'fish-classification',
        '--num-classes', str(NUM_CLASSES),
        '--image-size', str(IMAGE_SIZE),
        '--batch-size', str(BATCH_SIZE),
        '--epochs', str(EPOCHS),
        '--learning-rate', str(LEARNING_RATE),
    ],
    properties={
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º client mode –¥–ª—è –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –ª–æ–≥–æ–≤
        'spark.submit.deployMode': 'client',
        'spark.driver.memory': '6g',
        'spark.driver.cores': '4',
        'spark.executor.memory': '4g',
        'spark.executor.cores': '2',
        'spark.executor.instances': '1',
        
        # –ö–†–ò–¢–ò–ß–ù–û: –î–æ–±–∞–≤–ª—è–µ–º –æ–±–∞ –ø—É—Ç–∏ –≤ PYTHONPATH
        # 1. /opt/conda/lib/python3.8/site-packages - —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø–∞–∫–µ—Ç—ã (boto3, pandas, numpy)
        # 2. /home/dataproc-agent/.local/lib/python3.8/site-packages - user-installed (tensorflow, mlflow)
        'spark.yarn.appMasterEnv.PYTHONPATH': '/home/dataproc-agent/.local/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages',
        'spark.executorEnv.PYTHONPATH': '/home/dataproc-agent/.local/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages',
        
        # MLflow environment variables
        'spark.yarn.appMasterEnv.MLFLOW_TRACKING_URI': MLFLOW_URI,
        'spark.executorEnv.MLFLOW_TRACKING_URI': MLFLOW_URI,
        
        # S3 credentials –¥–ª—è MLflow artifact storage –∏ DataProc
        'spark.executorEnv.AWS_ACCESS_KEY_ID': S3_ACCESS_KEY,
        'spark.executorEnv.AWS_SECRET_ACCESS_KEY': S3_SECRET_KEY,
        'spark.yarn.appMasterEnv.AWS_ACCESS_KEY_ID': S3_ACCESS_KEY,
        'spark.yarn.appMasterEnv.AWS_SECRET_ACCESS_KEY': S3_SECRET_KEY,
        'spark.executorEnv.S3_ACCESS_KEY': S3_ACCESS_KEY,
        'spark.executorEnv.S3_SECRET_KEY': S3_SECRET_KEY,
        'spark.yarn.appMasterEnv.S3_ACCESS_KEY': S3_ACCESS_KEY,
        'spark.yarn.appMasterEnv.S3_SECRET_KEY': S3_SECRET_KEY,
        'spark.yarn.appMasterEnv.MLFLOW_S3_ENDPOINT_URL': S3_ENDPOINT,
        'spark.executorEnv.MLFLOW_S3_ENDPOINT_URL': S3_ENDPOINT,
        
        # S3A Hadoop configuration
        'spark.hadoop.fs.s3a.endpoint': S3_ENDPOINT.replace('https://', ''),
        'spark.hadoop.fs.s3a.path.style.access': 'true',
        'spark.hadoop.fs.s3a.connection.ssl.enabled': 'true',
        'spark.hadoop.fs.s3a.impl': 'org.apache.hadoop.fs.s3a.S3AFileSystem',
        'spark.hadoop.fs.s3a.access.key': S3_ACCESS_KEY,
        'spark.hadoop.fs.s3a.secret.key': S3_SECRET_KEY,
        
        # Adaptive query execution
        'spark.sql.adaptive.enabled': 'true',
    },
    connection_id='yandexcloud_default',
    dag=dag,
)


# Task 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ MLflow
def verify_mlflow_results(**context):
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –º–æ–¥–µ–ª—å –±—ã–ª–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤ MLflow
    """
    logger.info("=" * 80)
    logger.info("üîç Verifying MLflow results...")
    logger.info("=" * 80)
    
    try:
        import mlflow
        from mlflow.tracking import MlflowClient
    except ImportError:
        logger.warning("‚ö†Ô∏è  MLflow library not available in Airflow worker")
        logger.warning("‚ö†Ô∏è  Skipping MLflow verification - model was logged successfully during training")
        logger.info("=" * 80)
        logger.info("‚úÖ Training completed successfully (MLflow verification skipped)")
        logger.info("=" * 80)
        return {
            'status': 'skipped',
            'reason': 'mlflow library not available',
            'message': 'Model was logged during training, check MLflow UI directly'
        }
    
    try:
        mlflow.set_tracking_uri(MLFLOW_URI)
        client = MlflowClient()
        
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
        logger.info(f"\nüìä Checking experiment 'fish-classification'...")
        experiment = client.get_experiment_by_name("fish-classification")
        
        if experiment is None:
            raise AirflowException("Experiment 'fish-classification' not found in MLflow!")
        
        logger.info(f"   ‚úÖ Experiment ID: {experiment.experiment_id}")
        logger.info(f"   ‚úÖ Artifact Location: {experiment.artifact_location}")
        
        # 2. –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π run
        logger.info(f"\nüèÉ Getting latest runs...")
        runs = client.search_runs(
            experiment_ids=[experiment.experiment_id],
            order_by=["start_time DESC"],
            max_results=1
        )
        
        if not runs:
            raise AirflowException("No runs found in MLflow experiment!")
        
        latest_run = runs[0]
        run_id = latest_run.info.run_id
        
        logger.info(f"   ‚úÖ Latest Run ID: {run_id}")
        logger.info(f"   ‚úÖ Run Status: {latest_run.info.status}")
        logger.info(f"   ‚úÖ Start Time: {datetime.fromtimestamp(latest_run.info.start_time/1000)}")
        
        # 3. –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        logger.info(f"\nüìà Extracting metrics...")
        metrics = latest_run.data.metrics
        
        important_metrics = [
            'final_train_accuracy',
            'final_val_accuracy',
            'test_accuracy',
            'final_train_loss',
            'final_val_loss',
            'test_loss'
        ]
        
        results = {}
        for metric_name in important_metrics:
            value = metrics.get(metric_name)
            if value is not None:
                logger.info(f"   ‚úÖ {metric_name}: {value:.4f}")
                results[metric_name] = value
            else:
                logger.warning(f"   ‚ö†Ô∏è  {metric_name}: not found")
        
        # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å –≤ Registry
        logger.info(f"\nü§ñ Checking model in Model Registry...")
        try:
            model_versions = client.search_model_versions(
                f"name='fish-classification-mobilenetv2'"
            )
            
            if not model_versions:
                logger.warning("   ‚ö†Ô∏è  Model not found in Registry (might be registering...)")
            else:
                latest_version = model_versions[0]
                logger.info(f"   ‚úÖ Model: fish-classification-mobilenetv2")
                logger.info(f"   ‚úÖ Version: {latest_version.version}")
                logger.info(f"   ‚úÖ Stage: {latest_version.current_stage}")
                logger.info(f"   ‚úÖ Run ID: {latest_version.run_id}")
                
                results['model_version'] = latest_version.version
                results['model_stage'] = latest_version.current_stage
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è  Could not check Model Registry: {str(e)}")
        
        # 5. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
        logger.info(f"\nüéØ Model Quality Assessment...")
        val_acc = results.get('final_val_accuracy', 0)
        test_acc = results.get('test_accuracy')
        
        if val_acc > 0.85:
            logger.info(f"   üåü EXCELLENT! Val Accuracy: {val_acc:.4f} > 0.85")
            quality = "excellent"
        elif val_acc > 0.75:
            logger.info(f"   ‚úÖ GOOD! Val Accuracy: {val_acc:.4f} > 0.75")
            quality = "good"
        elif val_acc > 0.65:
            logger.info(f"   ‚ö†Ô∏è  ACCEPTABLE: Val Accuracy: {val_acc:.4f} > 0.65")
            quality = "acceptable"
        else:
            logger.warning(f"   ‚ùå POOR: Val Accuracy: {val_acc:.4f} <= 0.65")
            quality = "poor"
        
        if test_acc is not None:
            diff = abs(val_acc - test_acc)
            if diff > 0.1:
                logger.warning(f"   ‚ö†Ô∏è  Possible overfitting: val-test diff = {diff:.4f}")
            else:
                logger.info(f"   ‚úÖ Good generalization: val-test diff = {diff:.4f}")
        
        logger.info("=" * 80)
        logger.info("‚úÖ MLflow verification completed!")
        logger.info("=" * 80)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ XCom
        return {
            'run_id': run_id,
            'metrics': results,
            'quality': quality,
            'mlflow_uri': f"{MLFLOW_URI}/#/experiments/{experiment.experiment_id}/runs/{run_id}",
        }
        
    except Exception as e:
        logger.error(f"‚ùå MLflow verification failed: {str(e)}")
        raise AirflowException(f"MLflow verification failed: {str(e)}")


verify_mlflow = PythonOperator(
    task_id='verify_mlflow_results',
    python_callable=verify_mlflow_results,
    dag=dag,
)


# Task 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞
def generate_training_report(**context):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞ –æ–± –æ–±—É—á–µ–Ω–∏–∏
    """
    logger.info("=" * 80)
    logger.info("üìù Generating Training Report...")
    logger.info("=" * 80)
    
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–¥–∞—á
    validation_result = context['ti'].xcom_pull(task_ids='validate_environment')
    mlflow_result = context['ti'].xcom_pull(task_ids='verify_mlflow_results')
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç
    report = []
    report.append("\n" + "=" * 80)
    report.append("üéâ FISH CLASSIFICATION MODEL TRAINING REPORT")
    report.append("=" * 80)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–ø—É—Å–∫–µ
    report.append(f"\nüìÖ Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"üÜî DAG Run ID: {context['run_id']}")
    report.append(f"üë§ Triggered by: {context.get('dag_run').conf.get('user', 'unknown')}" if context.get('dag_run') else "")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    report.append(f"\n‚öôÔ∏è  CONFIGURATION:")
    report.append(f"   Base Model: MobileNetV2 (ImageNet weights)")
    report.append(f"   Num Classes: {validation_result['num_classes']}")
    report.append(f"   Image Size: {validation_result['image_size']}x{validation_result['image_size']}")
    report.append(f"   Batch Size: {validation_result['batch_size']}")
    report.append(f"   Epochs: {validation_result['epochs']}")
    report.append(f"   Learning Rate: {validation_result['learning_rate']}")
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    report.append(f"\nüìä TRAINING RESULTS:")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ MLflow verification –±—ã–ª —É—Å–ø–µ—à–µ–Ω
    if mlflow_result.get('status') == 'skipped':
        report.append(f"   ‚ö†Ô∏è  MLflow verification skipped: {mlflow_result.get('reason', 'unknown')}")
        report.append(f"   ‚ÑπÔ∏è  Check MLflow UI directly: {MLFLOW_URI}")
        metrics = {}
        quality = 'unknown'
        val_acc = 0
        test_acc = None
    else:
        metrics = mlflow_result.get('metrics', {})
        
        if 'final_train_accuracy' in metrics:
            report.append(f"   Train Accuracy: {metrics['final_train_accuracy']:.4f} ({metrics['final_train_accuracy']*100:.2f}%)")
        if 'final_val_accuracy' in metrics:
            report.append(f"   Val Accuracy:   {metrics['final_val_accuracy']:.4f} ({metrics['final_val_accuracy']*100:.2f}%)")
        if 'test_accuracy' in metrics:
            report.append(f"   Test Accuracy:  {metrics['test_accuracy']:.4f} ({metrics['test_accuracy']*100:.2f}%)")
        
        if 'final_train_loss' in metrics:
            report.append(f"   Train Loss: {metrics['final_train_loss']:.4f}")
        if 'final_val_loss' in metrics:
            report.append(f"   Val Loss:   {metrics['final_val_loss']:.4f}")
        if 'test_loss' in metrics:
            report.append(f"   Test Loss:  {metrics['test_loss']:.4f}")
        
        # –ú–æ–¥–µ–ª—å –≤ Registry
        report.append(f"\nü§ñ MODEL REGISTRY:")
        report.append(f"   Model Name: fish-classification-mobilenetv2")
        if 'model_version' in metrics:
            report.append(f"   Version: {metrics['model_version']}")
        if 'model_stage' in metrics:
            report.append(f"   Stage: {metrics['model_stage']}")
        
        # –ö–∞—á–µ—Å—Ç–≤–æ
        report.append(f"\nüéØ QUALITY ASSESSMENT:")
        quality = mlflow_result.get('quality', 'unknown')
        quality_emoji = {
            'excellent': 'üåü',
            'good': '‚úÖ',
            'acceptable': '‚ö†Ô∏è',
            'poor': '‚ùå',
            'unknown': '‚ùì'
        }
        report.append(f"   {quality_emoji.get(quality, '‚ùì')} Quality: {quality.upper()}")
        
        # MLflow —Å—Å—ã–ª–∫–∞
        report.append(f"\nüîó MLFLOW:")
        report.append(f"   Run URL: {mlflow_result.get('mlflow_uri', MLFLOW_URI)}")
        report.append(f"   Run ID: {mlflow_result.get('run_id', 'N/A')}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        report.append(f"\nüí° RECOMMENDATIONS:")
        val_acc = metrics.get('final_val_accuracy', 0)
        test_acc = metrics.get('test_accuracy')
    
    if quality == 'excellent':
        report.append(f"   ‚úÖ Model is ready for production deployment!")
    elif quality == 'good':
        report.append(f"   ‚úÖ Model can be used in staging environment")
    elif quality == 'acceptable':
        report.append(f"   ‚ö†Ô∏è  Consider retraining with more epochs or data augmentation")
    else:
        report.append(f"   ‚ùå Model needs significant improvements before deployment")
    
    if test_acc and abs(val_acc - test_acc) > 0.1:
        report.append(f"   ‚ö†Ô∏è  High val-test difference suggests overfitting")
        report.append(f"   üí° Consider: more data, stronger regularization, or data augmentation")
    
    report.append("\n" + "=" * 80)
    
    # –í—ã–≤–æ–¥–∏–º –æ—Ç—á—ë—Ç
    full_report = "\n".join(report)
    logger.info(full_report)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç
    return {
        'report': full_report,
        'timestamp': datetime.now().isoformat(),
        'quality': quality,
        'val_accuracy': val_acc,
        'test_accuracy': test_acc,
    }


generate_report = PythonOperator(
    task_id='generate_training_report',
    python_callable=generate_training_report,
    dag=dag,
)


# Task 5: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –æ–±—É—á–µ–Ω–∏—è
def handle_training_failure(**context):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏
    –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º cluster_id –¥–ª—è —Ä—É—á–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏
    """
    logger.error("=" * 80)
    logger.error("‚ùå TRAINING FAILED!")
    logger.error("=" * 80)
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª–∞—Å—Ç–µ—Ä–µ –∏–∑ XCom
    ti = context['ti']
    cluster_info = ti.xcom_pull(task_ids='create_dataproc_cluster')
    
    if cluster_info:
        cluster_id = cluster_info.get('cluster_id', 'unknown')
        logger.error(f"üîß DataProc Cluster ID: {cluster_id}")
        logger.error("")
        logger.error("üìã –î–ª—è –æ—Ç–ª–∞–¥–∫–∏:")
        logger.error(f"   1. –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞:")
        logger.error(f"      yc dataproc cluster list-operations {cluster_id}")
        logger.error("")
        logger.error(f"   2. –õ–æ–≥–∏ –≤ S3:")
        logger.error(f"      yc storage s3api list-objects --bucket {S3_BUCKET} --prefix logs/dataproc/")
        logger.error("")
        logger.error(f"   3. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –º–∞—Å—Ç–µ—Ä-–Ω–æ–¥–µ —á–µ—Ä–µ–∑ SSH –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏")
        logger.error("")
        logger.error("‚ö†Ô∏è  –í–ê–ñ–ù–û: –ö–ª–∞—Å—Ç–µ—Ä –ù–ï —É–¥–∞–ª—ë–Ω –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏!")
        logger.error("")
        logger.error(f"   –î–ª—è —Ä—É—á–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ—Ç–ª–∞–¥–∫–∏:")
        logger.error(f"      yc dataproc cluster delete {cluster_id}")
        logger.error("")
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ
    exception_info = context.get('exception', 'No exception info')
    logger.error(f"üí• Exception: {exception_info}")
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–¥–∞—á–µ
    task_instance = context.get('task_instance')
    if task_instance:
        logger.error(f"üìå Failed Task: {task_instance.task_id}")
        logger.error(f"üìÖ Execution Date: {task_instance.execution_date}")
        logger.error(f"üîÑ Try Number: {task_instance.try_number}")
    
    logger.error("=" * 80)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π
    return {
        'cluster_id': cluster_info.get('cluster_id') if cluster_info else None,
        'error': str(exception_info),
        'timestamp': datetime.now().isoformat(),
        'action_required': 'Manual cluster cleanup needed'
    }


handle_failure = PythonOperator(
    task_id='handle_training_failure',
    python_callable=handle_training_failure,
    trigger_rule='one_failed',  # –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ —É–ø–∞–ª–æ
    dag=dag,
)


# Task 6: –£–¥–∞–ª–µ–Ω–∏–µ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ —É—Å–ø–µ—Ö–µ)
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è cluster_id –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç–∞—Å–∫–∞
def get_cluster_id(**context):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ cluster_id –∏–∑ —Ç–∞—Å–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    ti = context['ti']
    cluster_info = ti.xcom_pull(task_ids='create_dataproc_cluster')
    if cluster_info:
        return cluster_info
    raise AirflowException("Cannot get cluster_id from create_dataproc_cluster task")

delete_cluster = DataprocDeleteClusterOperator(
    task_id='delete_dataproc_cluster',
    connection_id='yandexcloud_default',
    cluster_id="{{ task_instance.xcom_pull(task_ids='create_dataproc_cluster') }}",  # –ü–æ–ª—É—á–∞–µ–º cluster_id –∏–∑ XCom (—Å—Ç—Ä–æ–∫–∞)
    trigger_rule='all_success',  # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤—Å—ë –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ
    dag=dag,
)


# Task 7: –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± —É—Å–ø–µ—à–Ω–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
def notify_success(**context):
    """
    –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± —É—Å–ø–µ—à–Ω–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞
    """
    logger.info("=" * 80)
    logger.info("‚úÖ TRAINING PIPELINE COMPLETED SUCCESSFULLY!")
    logger.info("=" * 80)
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ XCom
    ti = context['ti']
    mlflow_info = ti.xcom_pull(task_ids='verify_mlflow_results')
    report_info = ti.xcom_pull(task_ids='generate_training_report')
    
    logger.info("")
    logger.info("üìä Summary:")
    if mlflow_info:
        logger.info(f"   üî¨ MLflow Run: {mlflow_info.get('run_id', 'N/A')}")
        logger.info(f"   üìà Validation Accuracy: {mlflow_info.get('val_accuracy', 'N/A')}")
    
    if report_info:
        quality = report_info.get('quality', 'unknown')
        logger.info(f"   ‚≠ê Model Quality: {quality}")
    
    logger.info("")
    logger.info(f"üîó MLflow UI: {MLFLOW_URI}")
    logger.info("")
    logger.info("‚úÖ DataProc cluster has been deleted")
    logger.info("=" * 80)
    
    return {'status': 'success', 'timestamp': datetime.now().isoformat()}


notify_completion = PythonOperator(
    task_id='notify_success',
    python_callable=notify_success,
    trigger_rule='all_success',
    dag=dag,
)


# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
# –û—Å–Ω–æ–≤–Ω–æ–π –ø—É—Ç—å –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
validate_env >> create_cluster >> train_model >> verify_mlflow >> generate_report >> delete_cluster >> notify_completion

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø—É—Ç—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
train_model >> handle_failure
verify_mlflow >> handle_failure
generate_report >> handle_failure
