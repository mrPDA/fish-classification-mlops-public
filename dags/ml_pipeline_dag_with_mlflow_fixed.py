"""
ğŸš€ ML Pipeline DAG with MLflow Integration for Yandex Cloud - Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞĞ¯ Ğ’Ğ•Ğ Ğ¡Ğ˜Ğ¯
=================================================================================

Airflow DAG Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾ÑˆĞµĞ½Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ MLflow
Ğ½Ğ° Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Yandex Cloud.

Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸:
- Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ DataProc ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
- ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PySpark Ğ¸ MLflow
- ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
- Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² Yandex Object Storage
- ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· MLflow UI

ĞĞ²Ñ‚Ğ¾Ñ€: ML Pipeline Team
Ğ”Ğ°Ñ‚Ğ°: 2024
"""

import uuid
import logging
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.models import Variable
from airflow.utils.trigger_rule import TriggerRule
from airflow.utils.task_group import TaskGroup
from airflow.providers.yandex.operators.dataproc import (
    DataprocCreateClusterOperator,
    DataprocCreatePysparkJobOperator,
    DataprocDeleteClusterOperator
)
from urllib.parse import urlparse
import boto3
import random

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
logger = logging.getLogger(__name__)

# Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² (Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ¸Ğ· Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ DAG)
ALL_FILES = [
    "2019-08-22.txt", "2019-09-21.txt", "2019-10-21.txt",
    "2019-11-20.txt", "2019-12-20.txt", "2020-01-19.txt",
    "2020-02-18.txt", "2020-03-19.txt", "2020-04-18.txt",
    "2020-05-18.txt", "2020-06-17.txt", "2020-07-17.txt",
    "2020-08-16.txt", "2020-09-15.txt", "2020-10-15.txt",
    "2020-11-14.txt", "2020-12-14.txt", "2021-01-13.txt",
    "2021-02-12.txt", "2021-03-14.txt", "2021-04-13.txt",
    "2021-05-13.txt", "2021-06-12.txt", "2021-07-12.txt",
    "2021-08-11.txt", "2021-09-10.txt", "2021-10-10.txt",
    "2021-11-09.txt", "2021-12-09.txt", "2022-01-08.txt",
    "2022-02-07.txt", "2022-03-09.txt", "2022-04-08.txt",
    "2022-05-08.txt", "2022-06-07.txt", "2022-07-07.txt",
    "2022-08-06.txt", "2022-09-05.txt", "2022-10-05.txt",
    "2022-11-04.txt"
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯ DAG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

default_args = {
    'owner': 'ml-engineer',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=10),
}

dag = DAG(
    'ml_pipeline_with_mlflow_fixed',
    default_args=default_args,
    description='ğŸ¤– ML Pipeline Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ MLflow Ğ´Ğ»Ñ Yandex Cloud (Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞĞ¯ Ğ’Ğ•Ğ Ğ¡Ğ˜Ğ¯)',
    schedule_interval=None,  # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ
    catchup=False,
    tags=['ml', 'mlflow', 'dataproc', 'yandex-cloud', 'fixed'],
    max_active_runs=1,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ Ğ’Ğ¡ĞŸĞĞœĞĞ“ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ• Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def validate_configuration(**context):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ¼"""
    logger.info("ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ...")
    
    # ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ
    required_vars = [
        'FOLDER_ID', 'SUBNET_ID', 'SECURITY_GROUP_ID',
        'S3_BUCKET_SCRIPTS', 'S3_ACCESS_KEY', 'S3_SECRET_KEY',
        'DATAPROC_SERVICE_ACCOUNT_ID',
        'MLFLOW_TRACKING_URI',  # â† Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾
    ]
    
    missing_vars = []
    for var in required_vars:
        try:
            value = Variable.get(var)
            if not value:
                missing_vars.append(var)
        except:
            missing_vars.append(var)
    
    if missing_vars:
        raise ValueError(f"âŒ ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ: {missing_vars}")
    
    # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° MLflow URI
    mlflow_uri = Variable.get('MLFLOW_TRACKING_URI', '')
    if not mlflow_uri:
        raise ValueError("âŒ MLFLOW_TRACKING_URI Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½ â€” MLflow Ğ½Ğµ ÑĞ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑ…ĞµĞ¼Ñƒ URI
    parsed = urlparse(mlflow_uri)
    if parsed.scheme not in ('http', 'https', 'file'):
        raise ValueError("âŒ MLFLOW_TRACKING_URI Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ‚ÑŒÑÑ Ñ http://, https:// Ğ¸Ğ»Ğ¸ file:/")
    
    logger.info(f"âœ… MLflow Ğ²ĞºĞ»ÑÑ‡ĞµĞ½: {mlflow_uri}")
    
    logger.info("âœ… ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ°")
    return True

def select_random_files(**context):
    """Ğ’Ñ‹Ğ±Ğ¾Ñ€ 3 Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² - Ğ¿Ğ¾ 1 Ñ„Ğ°Ğ¹Ğ»Ñƒ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (Ğ‘Ğ«Ğ¡Ğ¢Ğ Ğ«Ğ™ Ğ¢Ğ•Ğ¡Ğ¢)"""
    logger.info("ğŸ² Ğ’Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ 3 Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ - Ğ¿Ğ¾ 1 Ğ½Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ")
    random.seed(42)
    selected = random.sample(ALL_FILES, 3) if len(ALL_FILES) >= 3 else ALL_FILES
    groups = {
        'iteration_1': [selected[0]],  # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ 1 Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 1
        'iteration_2': [selected[1]],  # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ 1 Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 2  
        'iteration_3': [selected[2]]   # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ 1 Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3
    }
    for key, files in groups.items():
        logger.info(f"  {key}: {', '.join(files)} (Ğ‘Ğ«Ğ¡Ğ¢Ğ Ğ«Ğ™ Ğ¢Ğ•Ğ¡Ğ¢: 1 Ñ„Ğ°Ğ¹Ğ»)")
        context['task_instance'].xcom_push(key=key, value=files)
    return groups

def _s3_endpoint_host() -> str:
    url = Variable.get('S3_ENDPOINT_URL', default_var='https://storage.yandexcloud.net')
    parsed = urlparse(url)
    return parsed.netloc if parsed.netloc else url

def validate_files_runtime(iteration: int, **context):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‚ Ğ² S3 Ğ±Ğ°ĞºĞµÑ‚Ğµ."""
    bucket = Variable.get('S3_DATA_BUCKET', default_var=Variable.get('S3_BUCKET_SCRIPTS', default_var=''))
    prefix = Variable.get('S3_DATA_PREFIX', default_var='').strip('/')
    files = context['ti'].xcom_pull(task_ids='select_random_files', key=f'iteration_{iteration}') or []
    if not files:
        raise RuntimeError(f"ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ {iteration} Ğ² XCom")

    endpoint_host = _s3_endpoint_host()
    s3 = boto3.client(
        's3',
        endpoint_url=f"https://{endpoint_host}",
        aws_access_key_id=Variable.get('S3_ACCESS_KEY', default_var=''),
        aws_secret_access_key=Variable.get('S3_SECRET_KEY', default_var='')
    )

    missing = []
    for fname in files:
        key = f"{prefix}/{fname}" if prefix else fname
        try:
            s3.head_object(Bucket=bucket, Key=key)
            logger.info(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½ Ğ¾Ğ±ÑŠĞµĞºÑ‚: s3://{bucket}/{key}")
        except Exception as e:
            logger.error(f"âŒ ĞĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°: s3://{bucket}/{key} ({e})")
            missing.append(key)

    if missing:
        raise RuntimeError(f"ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ² Ğ±Ğ°ĞºĞµÑ‚Ğµ {bucket}: {missing}")
    return True

def cleanup_and_report(**context):
    """ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°"""
    logger.info("ğŸ§¹ Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¾Ñ‚Ñ‡ĞµÑ‚...")
    
    # Ğ’ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ·Ğ´ĞµÑÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ:
    # - ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
    # - Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ…
    # - ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ğ¹
    # - ĞÑ€Ñ…Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¾Ğ²
    
    logger.info(f"âœ… ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°")
    
    return {
        'status': 'completed',
        'execution_date': context['execution_date'].isoformat()
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“‹ ĞĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ•ĞĞ˜Ğ• Ğ—ĞĞ”ĞĞ§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°
validate_config_task = PythonOperator(
    task_id='validate_configuration',
    python_callable=validate_configuration,
    dag=dag,
)

# Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
task_select_files = PythonOperator(
    task_id='select_random_files',
    python_callable=select_random_files,
    dag=dag,
)

# ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ (Ğ¾Ğ±ÑŠÑĞ²Ğ»ÑĞµĞ¼ Ğ”Ğ TaskGroup)
delete_tasks = []
first_validate = None
second_validate = None
third_validate = None

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ TaskGroup Ğ´Ğ»Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
with TaskGroup('ml_training_process', dag=dag) as training_group:
    
    for iteration in [1, 2, 3]:
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
        validate_files = PythonOperator(
            task_id=f'validate_input_files_{iteration}',
            python_callable=validate_files_runtime,
            op_kwargs={'iteration': iteration},
            dag=dag,
        )

        def create_cluster_runtime_iteration(**context):
            """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´"""
            exec_dt = context['execution_date']
            unique_id = uuid.uuid4().hex[:6]
            cluster_name = f"ml-train-{iteration}-{exec_dt.strftime('%Y%m%d-%H%M%S')}-{unique_id}"
            logger.info(f"ğŸ·ï¸ Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ {iteration}: {cluster_name}")

            folder_id = Variable.get('FOLDER_ID')
            subnet_id = Variable.get('SUBNET_ID')
            zone = Variable.get('ZONE', 'ru-central1-a')
            sa_id = Variable.get('DATAPROC_SERVICE_ACCOUNT_ID')
            security_group_id = Variable.get('SECURITY_GROUP_ID')
            s3_logs_bucket = f"{Variable.get('S3_BUCKET_SCRIPTS')}/dataproc-logs/"

            op = DataprocCreateClusterOperator(
                task_id=f'_inner_create_cluster_{iteration}',
                folder_id=folder_id,
                cluster_name=cluster_name,
                cluster_description=f'ML Training Cluster - Iteration {iteration}',
                subnet_id=subnet_id,
                s3_bucket=s3_logs_bucket,
                ssh_public_keys=[Variable.get('YC_SSH_PUBLIC_KEY', default_var='')],
                zone=zone,
                cluster_image_version=Variable.get('DATAPROC_CLUSTER_VERSION', '2.0'),
                security_group_ids=[security_group_id] if security_group_id else [],
                service_account_id=sa_id,
                masternode_resource_preset=Variable.get('DATAPROC_MASTER_PRESET', 's3-c2-m8'),
                masternode_disk_type='network-hdd',
                masternode_disk_size=int(Variable.get('DATAPROC_MASTER_DISK_SIZE', '40')),
                datanode_resource_preset=Variable.get('DATAPROC_WORKER_PRESET', 's3-c4-m16'),
                datanode_disk_type='network-hdd',
                datanode_disk_size=int(Variable.get('DATAPROC_WORKER_DISK_SIZE', '100')),
                datanode_count=int(Variable.get('DATAPROC_WORKER_COUNT', '2')),
                services=['YARN', 'SPARK', 'HDFS', 'MAPREDUCE'],
                dag=dag,
            )
            
            cluster_id = op.execute(context)
            logger.info(f"âœ… ĞšĞ»Ğ°ÑÑ‚ĞµÑ€ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ {iteration}: {cluster_id}")
            return cluster_id

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
        create_cluster = PythonOperator(
            task_id=f'create_cluster_{iteration}',
            python_callable=create_cluster_runtime_iteration,
            dag=dag,
        )

        # ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
        wait_ready = BashOperator(
            task_id=f'wait_cluster_ready_{iteration}',
            bash_command='sleep {{ var.value.get("DATAPROC_GRACE_SECONDS", 180) }}',
            dag=dag,
        )

        # Ğ—ĞĞ“Ğ›Ğ£Ğ¨ĞšĞ: Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒĞ¶Ğµ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ñ‹ Ğ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
        def skip_cleaning_data_ready(**context):
            """Ğ—Ğ°Ğ³Ğ»ÑƒÑˆĞºĞ° Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ - Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒĞ¶Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹"""
            iteration_num = context['iteration']
            logger.info(f"ğŸ”„ Ğ˜Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ {iteration_num}: ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒĞ¶Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğ² ml_batch_results/iteration_batch={iteration_num}/")
            logger.info(f"ğŸ“Š ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ ~1000 parquet Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ")
            logger.info(f"âœ… ĞŸĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ğ¼ ÑÑ€Ğ°Ğ·Ñƒ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸")
            return f"data_ready_iteration_{iteration_num}"
        
        clean_batch = PythonOperator(
            task_id=f'skip_cleaning_{iteration}',
            python_callable=skip_cleaning_data_ready,
            op_kwargs={'iteration': iteration},
            dag=dag,
        )

        # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
        # Ğ’ĞĞ–ĞĞ: DataprocCreatePysparkJobOperator ĞĞ• Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Jinja2 Ğ² args!
        # Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ run_name/Ğ¿ÑƒÑ‚Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ training-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°
        
        train_model_task = DataprocCreatePysparkJobOperator(
            task_id=f'run_ml_training_{iteration}',
            cluster_id="{{ ti.xcom_pull(task_ids='ml_training_process.create_cluster_" + str(iteration) + "') }}",
            main_python_file_uri=f"s3a://{Variable.get('S3_BUCKET_SCRIPTS')}/scripts/train_with_mlflow_yandex_fixed.py",
            args=[
                '--input', f"s3a://{Variable.get('S3_DATA_BUCKET', Variable.get('S3_BUCKET_SCRIPTS'))}/{Variable.get('BATCH_OUTPUT_FOLDER', 'ml_batch_results')}/iteration_batch={iteration}/",
                '--output', f"s3a://{Variable.get('S3_BUCKET_SCRIPTS')}/models/fraud_model_iter{iteration}",
                '--model-type', Variable.get('ML_MODEL_TYPE', 'rf'),
                '--label-col', Variable.get('ML_LABEL_COL', 'tx_fraud'),
                '--s3-endpoint-url', f"https://{_s3_endpoint_host()}",
                '--s3-access-key', Variable.get('S3_ACCESS_KEY'),
                '--s3-secret-key', Variable.get('S3_SECRET_KEY'),
                '--tracking-uri', Variable.get('MLFLOW_TRACKING_URI', ''),
                '--experiment-name', Variable.get('MLFLOW_EXPERIMENT_NAME', 'fraud_detection_yandex'),
                '--run-name', f"training_iter{iteration}",
                '--auto-register',
                '--registry-threshold', Variable.get('MLFLOW_REGISTRY_THRESHOLD', '0.8'),
                '--sample-fraction', Variable.get('ML_SAMPLE_FRACTION', '0.1'),
                '--debug-log', f"s3a://{Variable.get('S3_BUCKET_SCRIPTS')}/debug-logs/main-pipeline/iter{iteration}.log",
            ],
            properties={
                'spark.submit.deployMode': 'cluster',
                'spark.sql.adaptive.enabled': 'true',
                'spark.hadoop.fs.s3a.endpoint': _s3_endpoint_host(),
                'spark.hadoop.fs.s3a.path.style.access': 'true',
                'spark.hadoop.fs.s3a.connection.ssl.enabled': 'true',
                'spark.hadoop.fs.s3a.impl': 'org.apache.hadoop.fs.s3a.S3AFileSystem',
                'spark.hadoop.fs.s3a.access.key': Variable.get('S3_ACCESS_KEY'),
                'spark.hadoop.fs.s3a.secret.key': Variable.get('S3_SECRET_KEY'),
                # ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ conda python Ñ Ğ°Ğ²Ñ‚Ğ¾ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¾Ğ¹ MLflow
                'spark.pyspark.python': '/opt/conda/bin/python',
                'spark.yarn.appMasterEnv.PYSPARK_PYTHON': '/opt/conda/bin/python',
                'spark.executorEnv.PYSPARK_PYTHON': '/opt/conda/bin/python',
                # ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ Ğ´Ğ»Ñ MLflow: ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° PYTHONPATH Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸
                'spark.yarn.appMasterEnv.PYTHONPATH': '/opt/conda/lib/python3.9/site-packages',
                'spark.executorEnv.PYTHONPATH': '/opt/conda/lib/python3.9/site-packages',
                # Ğ£Ğ‘Ğ˜Ğ ĞĞ•Ğœ INSTALL_PACKAGES - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ğ² ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğµ
                # ĞŸĞµÑ€ĞµĞ´Ğ°ĞµĞ¼ S3 credentials Ğ² executor Ğ¸ appMaster
                'spark.executorEnv.S3_ACCESS_KEY': Variable.get('S3_ACCESS_KEY'),
                'spark.executorEnv.S3_SECRET_KEY': Variable.get('S3_SECRET_KEY'),
                'spark.yarn.appMasterEnv.S3_ACCESS_KEY': Variable.get('S3_ACCESS_KEY'),
                'spark.yarn.appMasterEnv.S3_SECRET_KEY': Variable.get('S3_SECRET_KEY'),
                # MLflow env Ğ½Ğ° appMaster Ğ¸ executors
                'spark.yarn.appMasterEnv.MLFLOW_TRACKING_URI': Variable.get('MLFLOW_TRACKING_URI'),
                'spark.executorEnv.MLFLOW_TRACKING_URI': Variable.get('MLFLOW_TRACKING_URI'),
                # ĞµÑĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ:
                'spark.yarn.appMasterEnv.MLFLOW_TRACKING_USERNAME': Variable.get('MLFLOW_TRACKING_USERNAME', default_var=''),
                'spark.yarn.appMasterEnv.MLFLOW_TRACKING_PASSWORD': Variable.get('MLFLOW_TRACKING_PASSWORD', default_var=''),
                'spark.executorEnv.MLFLOW_TRACKING_USERNAME': Variable.get('MLFLOW_TRACKING_USERNAME', default_var=''),
                'spark.executorEnv.MLFLOW_TRACKING_PASSWORD': Variable.get('MLFLOW_TRACKING_PASSWORD', default_var=''),
                # ĞµÑĞ»Ğ¸ HTTPS Ñ ÑĞ°Ğ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ¼:
                'spark.yarn.appMasterEnv.MLFLOW_TRACKING_INSECURE_TLS': Variable.get('MLFLOW_TRACKING_INSECURE_TLS', default_var='false'),
                'spark.executorEnv.MLFLOW_TRACKING_INSECURE_TLS': Variable.get('MLFLOW_TRACKING_INSECURE_TLS', default_var='false'),
                # Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾: Ğ¿ÑƒÑ‚ÑŒ Ğº CA
                'spark.yarn.appMasterEnv.REQUESTS_CA_BUNDLE': Variable.get('REQUESTS_CA_BUNDLE', default_var=''),
                'spark.executorEnv.REQUESTS_CA_BUNDLE': Variable.get('REQUESTS_CA_BUNDLE', default_var=''),
            },
            connection_id='yandexcloud_default',
            dag=dag,
        )

        # Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
        delete_cluster = DataprocDeleteClusterOperator(
            task_id=f'delete_cluster_{iteration}',
            cluster_id="{{ ti.xcom_pull(task_ids='ml_training_process.create_cluster_" + str(iteration) + "') }}",
            trigger_rule=TriggerRule.ALL_DONE,
            dag=dag,
        )
        delete_tasks.append(delete_cluster)

        # Ğ¡Ğ²ÑĞ·Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
        validate_files >> create_cluster >> wait_ready >> clean_batch >> train_model_task >> delete_cluster

        # ĞŸĞĞ¡Ğ›Ğ•Ğ”ĞĞ’ĞĞ¢Ğ•Ğ›Ğ¬ĞĞĞ¡Ğ¢Ğ¬ Ğ˜Ğ¢Ğ•Ğ ĞĞ¦Ğ˜Ğ™: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ validate_files Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
        if iteration == 1:
            first_validate = validate_files
        elif iteration == 2:
            second_validate = validate_files
        elif iteration == 3:
            third_validate = validate_files

# Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¸ Ğ¾Ñ‚Ñ‡ĞµÑ‚
cleanup_task = PythonOperator(
    task_id='cleanup_and_report',
    python_callable=cleanup_and_report,
    trigger_rule=TriggerRule.ALL_DONE,
    dag=dag,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”— ĞĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ•ĞĞ˜Ğ• Ğ—ĞĞ’Ğ˜Ğ¡Ğ˜ĞœĞĞ¡Ğ¢Ğ•Ğ™
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ĞŸĞĞ¡Ğ›Ğ•Ğ”ĞĞ’ĞĞ¢Ğ•Ğ›Ğ¬ĞĞĞ¡Ğ¢Ğ¬ Ğ˜Ğ¢Ğ•Ğ ĞĞ¦Ğ˜Ğ™: 2-Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ 1-Ğ¹, 3-Ñ Ğ¿Ğ¾ÑĞ»Ğµ 2-Ğ¹
delete_tasks[0] >> second_validate  # 2-Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° 1-Ğ¹
delete_tasks[1] >> third_validate   # 3-Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° 2-Ğ¹

# ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ flow: Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµĞ¼ â†’ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ„Ğ°Ğ¹Ğ»Ñ‹ â†’ 1-Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ â†’ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚
validate_config_task >> task_select_files >> first_validate
training_group >> cleanup_task
