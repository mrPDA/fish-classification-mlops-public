"""
üêü Fish Species Classification Training DAG
==============================================================

DAG –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–± —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
Yandex DataProc —á–µ—Ä–µ–∑ yc CLI (–æ–ø–µ—Ä–∞—Ç–æ—Ä—ã DataProc –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –≤ Managed Airflow).

Dataset: Red Sea Fish (17 species, 1177 images)
Model: EfficientNet-B4 (fine-tuned)
"""

import os
import uuid
import json
import subprocess
import time
import logging
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.models import Variable
from airflow.utils.dates import days_ago
from airflow.utils.trigger_rule import TriggerRule

# ========================================
# DAG Configuration
# ========================================

DEFAULT_ARGS = {
    'owner': 'ml-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# ========================================
# Hardcoded Configuration (Non-sensitive)
# ========================================

CONFIG = {
    'MLFLOW_TRACKING_URI': 'http://10.11.0.20:5000',
    'MLFLOW_EXPERIMENT_NAME': 'fish-classification',
    'NUM_CLASSES': '17',
    'IMAGE_SIZE': '224',
    'BATCH_SIZE': '32',
    'EPOCHS': '20',
    'LEARNING_RATE': '0.001',
    'ENVIRONMENT': 'production',
}

# ========================================
# Helper Functions
# ========================================

def get_cluster_name():
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∏–º–µ–Ω–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    return f"fish-ml-{uuid.uuid4().hex[:8]}"

def validate_environment(**context):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    logger = logging.getLogger(__name__)
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å yc CLI –≤ —Ä–∞–∑–Ω—ã—Ö –ø—É—Ç—è—Ö
    yc_paths = [
        'yc',  # In PATH
        '/usr/local/bin/yc',
        '/usr/bin/yc',
        '/opt/yandex-cloud/bin/yc',
        '/home/airflow/.local/bin/yc',
    ]
    
    yc_found = False
    yc_cmd = None
    
    for path in yc_paths:
        try:
            logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ yc CLI –ø–æ –ø—É—Ç–∏: {path}")
            result = subprocess.run([path, 'version'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                logger.info(f"‚úÖ yc CLI –Ω–∞–π–¥–µ–Ω: {path}")
                logger.info(f"‚úÖ –í–µ—Ä—Å–∏—è: {result.stdout.strip()}")
                yc_found = True
                yc_cmd = path
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á–∞—Ö
                context['task_instance'].xcom_push(key='yc_path', value=path)
                break
            else:
                logger.warning(f"‚ö†Ô∏è  {path} –≤–µ—Ä–Ω—É–ª –∫–æ–¥ {result.returncode}")
        except FileNotFoundError:
            logger.debug(f"   {path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            continue
        except PermissionError as e:
            logger.warning(f"‚ö†Ô∏è  {path} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–Ω–µ—Ç –ø—Ä–∞–≤): {e}")
            continue
        except subprocess.TimeoutExpired:
            logger.warning(f"‚ö†Ô∏è  {path} –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç (—Ç–∞–π–º–∞—É—Ç)")
            continue
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {path}: {e}")
            continue
    
    if not yc_found:
        logger.error("‚ùå yc CLI –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –≤ –æ–¥–Ω–æ–º –∏–∑ –ø—É—Ç–µ–π:")
        for path in yc_paths:
            logger.error(f"   - {path}")
        raise RuntimeError("yc CLI –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏ Airflow")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ Lockbox
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ Lockbox...")
    required_vars = ['FOLDER_ID', 'SUBNET_ID', 'ZONE', 'DATAPROC_SERVICE_ACCOUNT_ID', 'S3_BUCKET_NAME']
    found_vars = {}
    missing_vars = []
    
    for var in required_vars:
        try:
            value = Variable.get(var)
            found_vars[var] = value
            logger.info(f"‚úÖ {var}: {value[:20]}..." if len(value) > 20 else f"‚úÖ {var}: {value}")
        except Exception as e:
            missing_vars.append(var)
            logger.warning(f"‚ö†Ô∏è  {var} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ Lockbox Variables")
    
    if missing_vars:
        logger.warning(f"‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {', '.join(missing_vars)}")
        logger.warning("   –ë—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ CONFIG")
    
    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(found_vars)}/{len(required_vars)} –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ Lockbox")
    logger.info("‚úÖ –û–∫—Ä—É–∂–µ–Ω–∏–µ –≤–∞–ª–∏–¥–Ω–æ")
    return True

def validate_dataset(**context):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ S3"""
    logger = logging.getLogger(__name__)
    logger.info("üì¶ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ S3...")
    
    bucket_name = Variable.get('S3_BUCKET_NAME', 'fish-classification-data-7wb4zv')
    logger.info(f"üìÇ S3 Bucket: {bucket_name}")
    logger.info(f"üìÇ Dataset path: s3://{bucket_name}/datasets/")
    logger.info("‚úÖ –î–∞—Ç–∞—Å–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–µ–Ω")
    
    return True

def create_dataproc_cluster(**context):
    """–°–æ–∑–¥–∞–Ω–∏–µ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞ —á–µ—Ä–µ–∑ yc CLI"""
    logger = logging.getLogger(__name__)
    cluster_name = get_cluster_name()
    
    logger.info(f"üöÄ –°–æ–∑–¥–∞–Ω–∏–µ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞: {cluster_name}")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ Lockbox Variables
    folder_id = Variable.get('FOLDER_ID', 'b1gjj3po03aa3m4j8ps5')
    subnet_id = Variable.get('SUBNET_ID', 'e9b5umsufggihj02a3o1')
    zone = Variable.get('ZONE', 'ru-central1-a')
    service_account_id = Variable.get('DATAPROC_SERVICE_ACCOUNT_ID', 'ajepv2durkr1nk9rnoui')
    s3_bucket = Variable.get('S3_BUCKET_NAME', 'fish-classification-data-7wb4zv')
    ssh_key = Variable.get('YC_SSH_PUBLIC_KEY', '')
    
    logger.info(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–∞:")
    logger.info(f"  Folder ID: {folder_id}")
    logger.info(f"  Zone: {zone}")
    logger.info(f"  Subnet ID: {subnet_id}")
    logger.info(f"  Service Account: {service_account_id}")
    logger.info(f"  S3 Bucket: {s3_bucket}")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞
    cmd = [
        'yc', 'dataproc', 'cluster', 'create', cluster_name,
        '--folder-id', folder_id,
        '--zone', zone,
        '--service-account-id', service_account_id,
        '--version', '2.1',
        '--services', 'YARN,SPARK,HDFS',
        '--bucket', s3_bucket,
        '--subcluster', 'name=master,role=masternode,resource-preset=s2.small,disk-type=network-ssd,disk-size=128,subnet-id=' + subnet_id + ',hosts-count=1',
        '--subcluster', 'name=data,role=datanode,resource-preset=s2.small,disk-type=network-ssd,disk-size=128,subnet-id=' + subnet_id + ',hosts-count=1',
        '--subcluster', 'name=compute,role=computenode,resource-preset=s2.small,disk-type=network-ssd,disk-size=128,subnet-id=' + subnet_id + ',hosts-count=2',
        '--deletion-protection=false',
    ]
    
    if ssh_key:
        cmd.extend(['--ssh-public-keys-file', '-'])
    
    logger.info(f"üîß –ö–æ–º–∞–Ω–¥–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {' '.join(cmd[:10])}...")
    
    try:
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—É
        if ssh_key:
            result = subprocess.run(
                cmd,
                input=ssh_key,
                capture_output=True,
                text=True,
                timeout=600  # 10 –º–∏–Ω—É—Ç
            )
        else:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=600
            )
        
        if result.returncode != 0:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞: {result.stderr}")
            raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä: {result.stderr}")
        
        # –ü–∞—Ä—Å–∏–º –≤—ã–≤–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è cluster_id
        output = result.stdout
        logger.info(f"üìÑ –í—ã–≤–æ–¥ –∫–æ–º–∞–Ω–¥—ã:\n{output}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º cluster_id –∏–∑ –≤—ã–≤–æ–¥–∞
        for line in output.split('\n'):
            if 'id:' in line.lower():
                cluster_id = line.split(':')[-1].strip()
                break
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ –≤—ã–≤–æ–¥–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º cluster_name
            cluster_id = cluster_name
        
        logger.info(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä —Å–æ–∑–¥–∞–Ω: {cluster_name} (ID: {cluster_id})")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ XCom
        context['task_instance'].xcom_push(key='cluster_name', value=cluster_name)
        context['task_instance'].xcom_push(key='cluster_id', value=cluster_id)
        
        return cluster_id
        
    except subprocess.TimeoutExpired:
        logger.error("‚ùå –¢–∞–π–º–∞—É—Ç —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞ (10 –º–∏–Ω—É—Ç)")
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞: {e}")
        raise

def wait_cluster_ready(**context):
    """–û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    logger = logging.getLogger(__name__)
    
    cluster_id = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_id'
    )
    
    logger.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞: {cluster_id}")
    
    max_attempts = 30  # 15 –º–∏–Ω—É—Ç (30 * 30 —Å–µ–∫)
    attempt = 0
    
    while attempt < max_attempts:
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–ª–∞—Å—Ç–µ—Ä–∞
            result = subprocess.run(
                ['yc', 'dataproc', 'cluster', 'get', cluster_id, '--format', 'json'],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                cluster_info = json.loads(result.stdout)
                status = cluster_info.get('status', 'UNKNOWN')
                health = cluster_info.get('health', 'UNKNOWN')
                
                logger.info(f"üìä –°—Ç–∞—Ç—É—Å –∫–ª–∞—Å—Ç–µ—Ä–∞: {status}, Health: {health}")
                
                if status == 'RUNNING' and health == 'ALIVE':
                    logger.info("‚úÖ –ö–ª–∞—Å—Ç–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
                    return True
                elif status in ['ERROR', 'STOPPED']:
                    raise RuntimeError(f"–ö–ª–∞—Å—Ç–µ—Ä –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ {status}")
            
            attempt += 1
            if attempt < max_attempts:
                logger.info(f"‚è≥ –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{max_attempts}, –∂–¥—ë–º 30 —Å–µ–∫—É–Ω–¥...")
                time.sleep(30)
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞: {e}")
            attempt += 1
            if attempt < max_attempts:
                time.sleep(30)
    
    raise RuntimeError(f"–ö–ª–∞—Å—Ç–µ—Ä –Ω–µ —Å—Ç–∞–ª –≥–æ—Ç–æ–≤ –∑–∞ {max_attempts * 30 / 60} –º–∏–Ω—É—Ç")

def train_model(**context):
    """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    logger = logging.getLogger(__name__)
    
    cluster_id = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_id'
    )
    
    logger.info(f"üéì –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ: {cluster_id}")
    logger.info(f"üìä MLflow: {CONFIG['MLFLOW_TRACKING_URI']}")
    logger.info(f"üî¢ Epochs: {CONFIG['EPOCHS']}, Batch size: {CONFIG['BATCH_SIZE']}")
    logger.info("‚ö†Ô∏è  –†–µ–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ —Å–ª–µ–¥—É—é—â–µ–π –≤–µ—Ä—Å–∏–∏")
    
    # TODO: –ó–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–ø—É—Å–∫ PySpark job —á–µ—Ä–µ–∑ yc dataproc job create-pyspark
    
    return True

def register_model(**context):
    """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ MLflow"""
    logger = logging.getLogger(__name__)
    
    logger.info("üìù –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ MLflow...")
    logger.info(f"üìä MLflow URI: {CONFIG['MLFLOW_TRACKING_URI']}")
    logger.info(f"üè∑Ô∏è  Experiment: {CONFIG['MLFLOW_EXPERIMENT_NAME']}")
    logger.info("‚ö†Ô∏è  –†–µ–∞–ª—å–Ω–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ —Å–ª–µ–¥—É—é—â–µ–π –≤–µ—Ä—Å–∏–∏")
    
    return True

def delete_dataproc_cluster(**context):
    """–£–¥–∞–ª–µ–Ω–∏–µ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞ —á–µ—Ä–µ–∑ yc CLI"""
    logger = logging.getLogger(__name__)
    
    cluster_id = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_id'
    )
    
    if not cluster_id:
        logger.warning("‚ö†Ô∏è  Cluster ID –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —É–¥–∞–ª–µ–Ω–∏–µ")
        return True
    
    logger.info(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞: {cluster_id}")
    
    try:
        result = subprocess.run(
            ['yc', 'dataproc', 'cluster', 'delete', cluster_id, '--async'],
            capture_output=True,
            text=True,
            timeout=60
        )
        
        if result.returncode == 0:
            logger.info(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} —É–¥–∞–ª—è–µ—Ç—Å—è (async)")
            return True
        else:
            logger.error(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞: {result.stderr}")
            # –ù–µ –±—Ä–æ—Å–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã DAG –∑–∞–≤–µ—Ä—à–∏–ª—Å—è
            return False
            
    except Exception as e:
        logger.error(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞: {e}")
        return False

def generate_report(**context):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏"""
    logger = logging.getLogger(__name__)
    
    cluster_id = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_id'
    )
    cluster_name = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_name'
    )
    
    logger.info("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞...")
    logger.info("=" * 60)
    logger.info("üéâ PIPELINE –ó–ê–í–ï–†–®–Å–ù")
    logger.info("=" * 60)
    logger.info(f"üñ•Ô∏è  DataProc Cluster: {cluster_name} (ID: {cluster_id})")
    logger.info(f"üìä MLflow: {CONFIG['MLFLOW_TRACKING_URI']}")
    logger.info(f"üè∑Ô∏è  Experiment: {CONFIG['MLFLOW_EXPERIMENT_NAME']}")
    logger.info(f"üî¢ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: Epochs={CONFIG['EPOCHS']}, Batch={CONFIG['BATCH_SIZE']}")
    logger.info("=" * 60)
    logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω yc CLI –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è DataProc")
    logger.info("=" * 60)
    
    return True

# ========================================
# DAG Definition
# ========================================

with DAG(
    dag_id='fish_classification_training',
    default_args=DEFAULT_ARGS,
    description='Fish Classification Model Training with DataProc via yc CLI',
    schedule_interval=None,  # –ó–∞–ø—É—Å–∫ –≤—Ä—É—á–Ω—É—é
    catchup=False,
    tags=['ml', 'fish-classification', 'dataproc', 'mlflow', 'yc-cli'],
) as dag:
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è
    task_validate_env = PythonOperator(
        task_id='validate_environment',
        python_callable=validate_environment,
        doc_md="""
        ### –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ yc CLI –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ Lockbox.
        """,
    )
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
    task_validate_dataset = PythonOperator(
        task_id='validate_dataset',
        python_callable=validate_dataset,
        doc_md="""
        ### –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ S3 bucket.
        """,
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞
    task_create_cluster = PythonOperator(
        task_id='create_dataproc_cluster',
        python_callable=create_dataproc_cluster,
        execution_timeout=timedelta(minutes=15),
        doc_md="""
        ### –°–æ–∑–¥–∞–Ω–∏–µ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞
        
        –°–æ–∑–¥–∞—ë—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–π Yandex DataProc –∫–ª–∞—Å—Ç–µ—Ä —á–µ—Ä–µ–∑ yc CLI.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ Lockbox.
        
        **–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
        - Master: 1x s2.small (4 vCPU, 16 GB RAM, 128 GB SSD)
        - Data: 1x s2.small (4 vCPU, 16 GB RAM, 128 GB SSD)
        - Compute: 2x s2.small (4 vCPU, 16 GB RAM, 128 GB SSD)
        
        **–°–µ—Ä–≤–∏—Å—ã:** YARN, SPARK, HDFS
        
        **–í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è:** ~5-10 –º–∏–Ω—É—Ç
        """,
    )
    
    # –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
    task_wait_cluster = PythonOperator(
        task_id='wait_cluster_ready',
        python_callable=wait_cluster_ready,
        execution_timeout=timedelta(minutes=20),
        doc_md="""
        ### –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
        
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å –∫–ª–∞—Å—Ç–µ—Ä–∞ –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥.
        –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è: 15 –º–∏–Ω—É—Ç.
        """,
    )
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    task_train = PythonOperator(
        task_id='train_model',
        python_callable=train_model,
        doc_md="""
        ### –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        –ó–∞–ø—É—Å–∫–∞–µ—Ç Spark job –¥–ª—è –æ–±—É—á–µ–Ω–∏—è EfficientNet-B4.
        (–í —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ - –∑–∞–≥–ª—É—à–∫–∞)
        """,
    )
    
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    task_register = PythonOperator(
        task_id='register_model',
        python_callable=register_model,
        doc_md="""
        ### –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ MLflow Model Registry.
        (–í —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ - –∑–∞–≥–ª—É—à–∫–∞)
        """,
    )
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞
    task_cleanup = PythonOperator(
        task_id='delete_dataproc_cluster',
        python_callable=delete_dataproc_cluster,
        trigger_rule=TriggerRule.ALL_DONE,  # –í—ã–ø–æ–ª–Ω–∏—Ç—Å—è –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
        execution_timeout=timedelta(minutes=5),
        doc_md="""
        ### –£–¥–∞–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞
        
        –£–¥–∞–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–π DataProc –∫–ª–∞—Å—Ç–µ—Ä —á–µ—Ä–µ–∑ yc CLI.
        –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤—Å–µ–≥–¥–∞, –¥–∞–∂–µ –µ—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–∞–¥–∞—á–∏ –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å —Å –æ—à–∏–±–∫–æ–π.
        """,
    )
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞
    task_report = PythonOperator(
        task_id='generate_report',
        python_callable=generate_report,
        doc_md="""
        ### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞
        –°–æ–∑–¥–∞—ë—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ pipeline.
        """,
    )
    
    # ========================================
    # DAG Flow
    # ========================================
    
    task_validate_env >> task_validate_dataset >> task_create_cluster >> task_wait_cluster >> task_train >> task_register >> task_cleanup >> task_report