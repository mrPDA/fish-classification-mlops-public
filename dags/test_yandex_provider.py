"""
–¢–µ—Å—Ç–æ–≤—ã–π DAG –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Yandex Provider –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from airflow.providers.yandex.operators.dataproc import (
    DataprocCreateClusterOperator,
    DataprocCreatePysparkJobOperator,
    DataprocDeleteClusterOperator
)

default_args = {
    'owner': 'test',
    'start_date': days_ago(1),
}

dag = DAG(
    'test_yandex_provider_operators',
    default_args=default_args,
    description='Test Yandex Provider availability',
    schedule_interval=None,
    catchup=False,
    tags=['test', 'yandex', 'provider'],
)

def test_yandex_imports(**context):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Yandex Provider –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤"""
    import sys
    import logging
    
    logger = logging.getLogger(__name__)
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Yandex Provider...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é –ø–∞–∫–µ—Ç–∞
    try:
        import airflow.providers.yandex
        logger.info(f"‚úÖ airflow.providers.yandex –¥–æ—Å—Ç—É–ø–µ–Ω")
        logger.info(f"   –ü—É—Ç—å: {airflow.providers.yandex.__file__}")
        if hasattr(airflow.providers.yandex, '__version__'):
            logger.info(f"   –í–µ—Ä—Å–∏—è: {airflow.providers.yandex.__version__}")
    except ImportError as e:
        logger.error(f"‚ùå airflow.providers.yandex –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥—É–ª—è operators.dataproc
    try:
        import airflow.providers.yandex.operators.dataproc as dataproc_module
        logger.info(f"‚úÖ airflow.providers.yandex.operators.dataproc –¥–æ—Å—Ç—É–ø–µ–Ω")
        logger.info(f"   –ü—É—Ç—å: {dataproc_module.__file__}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
        available_items = [item for item in dir(dataproc_module) if not item.startswith('_')]
        logger.info(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã ({len(available_items)}):")
        for item in available_items:
            logger.info(f"     - {item}")
            
    except ImportError as e:
        logger.error(f"‚ùå airflow.providers.yandex.operators.dataproc –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã
    operators_to_check = [
        'DataprocCreateClusterOperator',
        'DataprocDeleteClusterOperator',
        'DataprocCreatePysparkJobOperator',
        'DataprocCreateHiveJobOperator',
    ]
    
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤:")
    for operator_name in operators_to_check:
        try:
            operator_class = getattr(dataproc_module, operator_name, None)
            if operator_class:
                logger.info(f"   ‚úÖ {operator_name} - –î–û–°–¢–£–ü–ï–ù")
                logger.info(f"      –ö–ª–∞—Å—Å: {operator_class}")
            else:
                logger.warning(f"   ‚ö†Ô∏è  {operator_name} - –ù–ï –ù–ê–ô–î–ï–ù –≤ –º–æ–¥—É–ª–µ")
        except Exception as e:
            logger.error(f"   ‚ùå {operator_name} - –û–®–ò–ë–ö–ê: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º hooks
    try:
        import airflow.providers.yandex.hooks.yandex as yandex_hooks
        logger.info(f"‚úÖ airflow.providers.yandex.hooks.yandex –¥–æ—Å—Ç—É–ø–µ–Ω")
        
        available_hooks = [item for item in dir(yandex_hooks) if 'Hook' in item and not item.startswith('_')]
        logger.info(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ hooks ({len(available_hooks)}):")
        for hook in available_hooks:
            logger.info(f"     - {hook}")
            
    except ImportError as e:
        logger.error(f"‚ùå airflow.providers.yandex.hooks –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    
    logger.info("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return True

with dag:
    test_task = PythonOperator(
        task_id='test_yandex_imports',
        python_callable=test_yandex_imports,
    )
