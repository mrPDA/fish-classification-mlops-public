"""
üêü Fish Species Classification Training DAG (REST API Version)
==============================================================

DAG –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–± —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
Yandex DataProc —á–µ—Ä–µ–∑ REST API (–ø—Ä—è–º—ã–µ HTTP –∑–∞–ø—Ä–æ—Å—ã).

Dataset: Red Sea Fish (17 species, 1177 images)
Model: EfficientNet-B4 (fine-tuned)

–†–ï–®–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú–´:
- DataProc –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã –ù–ï –°–£–©–ï–°–¢–í–£–Æ–¢ –≤ apache-airflow-providers-yandex
- yc CLI –ù–ï —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ Managed Airflow
- Yandex Cloud Python SDK –≤—ã–∑—ã–≤–∞–µ—Ç –∑–∞–≤–∏—Å–∞–Ω–∏–µ Terraform
- –†–ï–®–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—ã–µ REST API –∑–∞–ø—Ä–æ—Å—ã —á–µ—Ä–µ–∑ requests + PyJWT
"""

import os
import json
import time
import logging
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.utils.dates import days_ago
from airflow.utils.trigger_rule import TriggerRule

# ========================================
# DAG Configuration
# ========================================

DEFAULT_ARGS = {
    'owner': 'ml-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# ========================================
# Hardcoded Configuration (Non-sensitive)
# ========================================

CONFIG = {
    'MLFLOW_TRACKING_URI': 'http://10.11.0.20:5000',
    'MLFLOW_EXPERIMENT_NAME': 'fish-classification',
    'NUM_CLASSES': '17',
    'IMAGE_SIZE': '224',
    'BATCH_SIZE': '32',
    'EPOCHS': '20',
    'LEARNING_RATE': '0.001',
    'DATAPROC_IMAGE_VERSION': '2.1',  # Hadoop 3.2, Spark 3.0
    'MASTER_RESOURCE_PRESET': 's2.small',  # 4 vCPU, 16 GB RAM
    'WORKER_RESOURCE_PRESET': 's2.small',  # 4 vCPU, 16 GB RAM
    'WORKER_COUNT': '2',
    'DISK_SIZE_GB': '100',
}

# ========================================
# Helper Functions
# ========================================

def create_yandex_connection(**context):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ Yandex Cloud Connection –∏–∑ DP_SA_JSON (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º DAG)
    
    –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –æ–±—ã—á–Ω–æ–º Airflow, –Ω–æ –≤ Managed Airflow
    Connection 'yandexcloud_default' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
    """
    from airflow.models import Connection
    from airflow.settings import Session
    import json
    
    logger = logging.getLogger(__name__)
    logger.info("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è Yandex Cloud...")
    
    try:
        session = Session()
        
        # –ü–æ–ª—É—á–∞–µ–º JSON –∫–ª—é—á —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ –∏–∑ Variables
        sa_json_str = Variable.get('DP_SA_JSON', None)
        if not sa_json_str or sa_json_str == "placeholder":
            logger.warning("‚ö†Ô∏è  DP_SA_JSON –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º yandexcloud_default")
            return False
        
        # –°–æ–∑–¥–∞—ë–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º Connection
        yc_connection = Connection(
            conn_id="yc-fish-dataproc",
            conn_type="yandexcloud",
            extra={
                "extra__yandexcloud__service_account_json": sa_json_str,
            },
        )
        
        existing_conn = session.query(Connection).filter(
            Connection.conn_id == "yc-fish-dataproc"
        ).first()
        
        if existing_conn:
            existing_conn.extra = yc_connection.extra
            logger.info("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ yc-fish-dataproc")
        else:
            session.add(yc_connection)
            logger.info("‚úÖ –°–æ–∑–¥–∞–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ yc-fish-dataproc")
            
        session.commit()
        session.close()
        
        logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ Yandex Cloud –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
        return False


def get_iam_token_from_sa_json(sa_json_str: str) -> str:
    """
    –ü–æ–ª—É—á–∞–µ—Ç IAM —Ç–æ–∫–µ–Ω –∏–∑ JSON –∫–ª—é—á–∞ —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º DAG)
    
    Args:
        sa_json_str: JSON —Å—Ç—Ä–æ–∫–∞ —Å –∫–ª—é—á–æ–º —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞
    
    Returns:
        IAM —Ç–æ–∫–µ–Ω
    """
    import jwt
    import requests
    
    sa_key = json.loads(sa_json_str)
    
    now = int(time.time())
    payload = {
        'aud': 'https://iam.api.cloud.yandex.net/iam/v1/tokens',
        'iss': sa_key['service_account_id'],
        'iat': now,
        'exp': now + 3600
    }
    
    # –°–æ–∑–¥–∞—ë–º JWT
    encoded_token = jwt.encode(
        payload,
        sa_key['private_key'],
        algorithm='PS256',
        headers={'kid': sa_key['id']}
    )
    
    # –û–±–º–µ–Ω–∏–≤–∞–µ–º JWT –Ω–∞ IAM —Ç–æ–∫–µ–Ω
    response = requests.post(
        'https://iam.api.cloud.yandex.net/iam/v1/tokens',
        json={'jwt': encoded_token},
        timeout=10
    )
    
    if response.status_code != 200:
        raise Exception(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è IAM —Ç–æ–∫–µ–Ω–∞: {response.status_code} - {response.text}")
    
    return response.json()['iamToken']


def wait_operation(operation_id: str, iam_token: str, timeout: int = 3600):
    """–û–∂–∏–¥–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç cluster_id"""
    import requests
    
    url = f"https://operation.api.cloud.yandex.net/operations/{operation_id}"
    headers = {"Authorization": f"Bearer {iam_token}"}
    start_time = time.time()
    
    while time.time() - start_time < timeout:
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code != 200:
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏: {response.status_code}")
        
        operation = response.json()
        done = operation.get("done", False)
        
        if done:
            if "error" in operation:
                error = operation["error"]
                raise Exception(f"–û–ø–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π: {error}")
            
            metadata = operation.get("metadata", {})
            cluster_id = metadata.get("clusterId")
            return cluster_id
        
        print(f"  –û–ø–µ—Ä–∞—Ü–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ... ({int(time.time() - start_time)} —Å–µ–∫)")
        time.sleep(10)
    
    raise Exception(f"–¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–∏ ({timeout} —Å–µ–∫)")


# ========================================
# Task Functions
# ========================================

def validate_environment(**context):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –º–æ–¥—É–ª–µ–π"""
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Python –º–æ–¥—É–ª–∏
    required_modules = ['requests', 'jwt']
    for module in required_modules:
        try:
            __import__(module)
            print(f"‚úÖ {module} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        except ImportError:
            raise RuntimeError(f"‚ùå {module} –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–æ–±–∞–≤—å—Ç–µ 'PyJWT' –≤ pip_packages –≤ airflow.tf")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–∞ —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ –∏–∑ Connection
    # –í Yandex Managed Airflow Lockbox —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è Connections!
    try:
        from airflow.hooks.base import BaseHook
        conn = BaseHook.get_connection('yandexcloud_default')
        
        # –ö–ª—é—á —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ extra
        if conn.extra:
            import json
            extra = json.loads(conn.extra)
            if 'service_account_json' in extra or 'key' in extra:
                print("‚úÖ –ö–ª—é—á —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ –Ω–∞–π–¥–µ–Ω –≤ Connection")
            else:
                print("‚ö†Ô∏è  –ö–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ Connection, –∏—Å–ø–æ–ª—å–∑—É–µ–º service account –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
        else:
            print("‚ö†Ô∏è  Connection –ø—É—Å—Ç–æ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º service account –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö VM")
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Connection: {e}")
        print("‚ö†Ô∏è  –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å service account –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö VM")
    
    print("‚úÖ –û–∫—Ä—É–∂–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ")
    return True


def create_dataproc_cluster(**context):
    """–°–æ–∑–¥–∞—ë—Ç DataProc –∫–ª–∞—Å—Ç–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ REST API"""
    import requests
    import os
    
    print("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞ —á–µ—Ä–µ–∑ REST API...")
    
    # –ü–æ–ª—É—á–∞–µ–º DP_SA_JSON –∏–∑ Variables (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º DAG)
    print("üîë –ü–æ–ª—É—á–µ–Ω–∏–µ DP_SA_JSON –∏–∑ Airflow Variables...")
    try:
        sa_json_str = Variable.get('DP_SA_JSON', None)
        if not sa_json_str or sa_json_str == "placeholder":
            print("‚ö†Ô∏è  DP_SA_JSON –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ Variables")
            print("‚ö†Ô∏è  DEMO MODE: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π —Ç–æ–∫–µ–Ω")
            iam_token = "demo_token"
        else:
            print("‚úÖ DP_SA_JSON –Ω–∞–π–¥–µ–Ω, –ø–æ–ª—É—á–∞–µ–º IAM —Ç–æ–∫–µ–Ω...")
            iam_token = get_iam_token_from_sa_json(sa_json_str)
            print("‚úÖ IAM —Ç–æ–∫–µ–Ω —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω!")
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞: {e}")
        print("‚ö†Ô∏è  DEMO MODE: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π —Ç–æ–∫–µ–Ω")
        iam_token = "demo_token"
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ Variables (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º DAG)
    print("üìã –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ Airflow Variables...")
    folder_id = Variable.get('FOLDER_ID', 'b1gjj3po03aa3m4j8ps5')
    zone = Variable.get('ZONE', 'ru-central1-a')
    subnet_id = Variable.get('SUBNET_ID', 'e9b5umsufggihj02a3o1')
    service_account_id = Variable.get('DATAPROC_SERVICE_ACCOUNT_ID', 'ajepv2durkr1nk9rnoui')
    security_group_id = Variable.get('SECURITY_GROUP_ID', 'enpha1v51ug84ddqfob4')
    s3_bucket = Variable.get('S3_BUCKET_NAME', 'fish-classification-data-7wb4zv')
    ssh_public_key = Variable.get('YC_SSH_PUBLIC_KEY', 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC...')
    
    print(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"  Folder ID: {folder_id}")
    print(f"  Zone: {zone}")
    print(f"  Subnet ID: {subnet_id}")
    print(f"  Service Account ID: {service_account_id}")
    print(f"  S3 Bucket: {s3_bucket}")
    
    # –°–æ–∑–¥–∞—ë–º –∫–ª–∞—Å—Ç–µ—Ä
    cluster_name = f"fish-training-{int(time.time())}"
    
    # Sanitize run_id –¥–ª—è labels (—Ç–æ–ª—å–∫–æ [a-z][-_0-9a-z]*)
    run_id_safe = context['run_id'].replace(':', '-').replace('_', '-').replace('+', '-').lower()
    
    cluster_config = {
        "folderId": folder_id,
        "name": cluster_name,
        "description": "DataProc cluster for fish classification training",
        "labels": {
            "created-by": "airflow",
            "dag-id": "fish-classification-training",
            "project": "fish-classification"
        },
        "configSpec": {
            "versionId": CONFIG['DATAPROC_IMAGE_VERSION'],
            "hadoop": {
                "services": ["HDFS", "YARN", "SPARK", "LIVY"],
                "properties": {
                    "yarn:yarn.resourcemanager.am.max-attempts": "5",
                    "spark:spark.sql.warehouse.dir": f"s3a://{s3_bucket}/spark-warehouse/"
                },
                "sshPublicKeys": [ssh_public_key]
            },
            "subclustersSpec": [
                {
                    "name": "masternode",
                    "role": "MASTERNODE",
                    "resources": {
                        "resourcePresetId": CONFIG['MASTER_RESOURCE_PRESET'],
                        "diskTypeId": "network-ssd",
                        "diskSize": str(int(CONFIG['DISK_SIZE_GB']) * 1024 * 1024 * 1024)
                    },
                    "subnetId": subnet_id,
                    "hostsCount": "1"
                },
                {
                    "name": "datanode",
                    "role": "DATANODE",
                    "resources": {
                        "resourcePresetId": CONFIG['WORKER_RESOURCE_PRESET'],
                        "diskTypeId": "network-ssd",
                        "diskSize": str(int(CONFIG['DISK_SIZE_GB']) * 1024 * 1024 * 1024)
                    },
                    "subnetId": subnet_id,
                    "hostsCount": CONFIG['WORKER_COUNT']
                }
            ]
        },
        "zoneId": zone,
        "serviceAccountId": service_account_id,
        "bucket": s3_bucket,
        "uiProxy": True,
        "securityGroupIds": [security_group_id],
        "deletionProtection": False
    }
    
    headers = {
        "Authorization": f"Bearer {iam_token}",
        "Content-Type": "application/json"
    }
    
    url = "https://dataproc.api.cloud.yandex.net/dataproc/v1/clusters"
    print(f"üì§ POST {url}")
    
    response = requests.post(url, headers=headers, json=cluster_config, timeout=30)
    
    if response.status_code not in [200, 201]:
        raise Exception(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞: {response.status_code} - {response.text}")
    
    operation = response.json()
    operation_id = operation.get("id")
    print(f"‚úÖ –û–ø–µ—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–ø—É—â–µ–Ω–∞: {operation_id}")
    
    # –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–∏ (–∏–∑–≤–ª–µ–∫–∞–µ–º cluster_id)
    cluster_id = wait_operation(operation_id, iam_token)
    print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä —Å–æ–∑–¥–∞–Ω: {cluster_id}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º cluster_id –∏ iam_token –≤ XCom
    context['task_instance'].xcom_push(key='cluster_id', value=cluster_id)
    context['task_instance'].xcom_push(key='iam_token', value=iam_token)
    
    return cluster_id


def wait_cluster_ready(**context):
    """–û–∂–∏–¥–∞–µ—Ç –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    import requests
    
    print("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞...")
    
    cluster_id = context['task_instance'].xcom_pull(key='cluster_id', task_ids='create_dataproc_cluster')
    iam_token = context['task_instance'].xcom_pull(key='iam_token', task_ids='create_dataproc_cluster')
    
    if not cluster_id:
        raise ValueError("cluster_id –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ XCom")
    
    print(f"üìã Cluster ID: {cluster_id}")
    
    url = f"https://dataproc.api.cloud.yandex.net/dataproc/v1/clusters/{cluster_id}"
    headers = {"Authorization": f"Bearer {iam_token}"}
    
    timeout = 1800  # 30 –º–∏–Ω—É—Ç
    start_time = time.time()
    
    while time.time() - start_time < timeout:
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code != 200:
            raise Exception(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞: {response.status_code}")
        
        cluster = response.json()
        status = cluster.get("status")
        health = cluster.get("health")
        
        print(f"  Status: {status}, Health: {health}")
        
        if status == "RUNNING" and health == "ALIVE":
            elapsed = int(time.time() - start_time)
            print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä –≥–æ—Ç–æ–≤! (–∑–∞–Ω—è–ª–æ {elapsed} —Å–µ–∫—É–Ω–¥)")
            return True
        
        if status in ["ERROR", "STOPPING", "STOPPED"]:
            raise Exception(f"–ö–ª–∞—Å—Ç–µ—Ä –≤ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏: {status}")
        
        time.sleep(30)
    
    raise Exception(f"–¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ ({timeout} —Å–µ–∫)")


def run_training_job(**context):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ DataProc –∫–ª–∞—Å—Ç–µ—Ä–µ (PLACEHOLDER)"""
    print("üéØ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")
    print("‚ö†Ô∏è  PLACEHOLDER: –ó–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–ø—É—Å–∫ PySpark job —á–µ—Ä–µ–∑ Livy API")
    
    cluster_id = context['task_instance'].xcom_pull(key='cluster_id', task_ids='create_dataproc_cluster')
    print(f"üìã Cluster ID: {cluster_id}")
    
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∑–∞–ø—É—Å–∫ PySpark job —á–µ—Ä–µ–∑ Livy REST API
    # POST https://dataproc.api.cloud.yandex.net/dataproc/v1/clusters/{cluster_id}/jobs
    
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ (PLACEHOLDER)")
    return True


def delete_dataproc_cluster(**context):
    """–£–¥–∞–ª—è–µ—Ç DataProc –∫–ª–∞—Å—Ç–µ—Ä"""
    import requests
    
    print("üóëÔ∏è  –£–¥–∞–ª–µ–Ω–∏–µ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞...")
    
    cluster_id = context['task_instance'].xcom_pull(key='cluster_id', task_ids='create_dataproc_cluster')
    iam_token = context['task_instance'].xcom_pull(key='iam_token', task_ids='create_dataproc_cluster')
    
    if not cluster_id:
        print("‚ö†Ô∏è  cluster_id –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —É–¥–∞–ª–µ–Ω–∏–µ")
        return True
    
    print(f"üìã Cluster ID –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {cluster_id}")
    
    url = f"https://dataproc.api.cloud.yandex.net/dataproc/v1/clusters/{cluster_id}"
    headers = {"Authorization": f"Bearer {iam_token}"}
    
    response = requests.delete(url, headers=headers, timeout=30)
    
    if response.status_code not in [200, 204]:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞: {response.status_code} - {response.text}")
        return False
    
    operation = response.json()
    operation_id = operation.get("id")
    print(f"‚úÖ –û–ø–µ—Ä–∞—Ü–∏—è —É–¥–∞–ª–µ–Ω–∏—è –∑–∞–ø—É—â–µ–Ω–∞: {operation_id}")
    
    # –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–∏
    wait_operation(operation_id, iam_token)
    print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä —É–¥–∞–ª—ë–Ω: {cluster_id}")
    
    return True


# ========================================
# DAG Definition
# ========================================

with DAG(
    dag_id='fish_classification_training_rest_api',
    default_args=DEFAULT_ARGS,
    description='Fish classification training with DataProc (REST API)',
    schedule_interval=None,  # Manual trigger only
    catchup=False,
    tags=['ml', 'training', 'dataproc', 'rest-api'],
) as dag:
    
    # Task 1: Validate environment
    validate_env = PythonOperator(
        task_id='validate_environment',
        python_callable=validate_environment,
        provide_context=True,
    )
    
    # Task 2: Create DataProc cluster
    create_cluster = PythonOperator(
        task_id='create_dataproc_cluster',
        python_callable=create_dataproc_cluster,
        provide_context=True,
    )
    
    # Task 3: Wait for cluster to be ready
    wait_cluster = PythonOperator(
        task_id='wait_cluster_ready',
        python_callable=wait_cluster_ready,
        provide_context=True,
    )
    
    # Task 4: Run training job
    train_model = PythonOperator(
        task_id='run_training_job',
        python_callable=run_training_job,
        provide_context=True,
    )
    
    # Task 5: Delete DataProc cluster (always runs)
    delete_cluster = PythonOperator(
        task_id='delete_dataproc_cluster',
        python_callable=delete_dataproc_cluster,
        provide_context=True,
        trigger_rule=TriggerRule.ALL_DONE,  # –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤—Å–µ–≥–¥–∞
    )
    
    # Define task dependencies
    validate_env >> create_cluster >> wait_cluster >> train_model >> delete_cluster
