"""
Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ DAG Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ DataProc ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Yandex Cloud
"""

import uuid
import datetime
from airflow import DAG
from airflow.utils.trigger_rule import TriggerRule
from airflow.models import Variable
from airflow.operators.python import PythonOperator

# Ğ˜ĞœĞŸĞĞ Ğ¢ Ğ˜Ğ— Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢ĞĞ¦Ğ˜Ğ˜ - Ñ yandexcloud_dataproc!
from airflow.providers.yandex.operators.yandexcloud_dataproc import (
    DataprocCreateClusterOperator,
    DataprocCreatePysparkJobOperator,
    DataprocDeleteClusterOperator,
)

# ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Airflow Variables
def get_cluster_config(**context):
    """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Variables"""
    config = {
        'zone': Variable.get('ZONE', 'ru-central1-a'),
        'ssh_key': Variable.get('YC_SSH_PUBLIC_KEY', ''),
        'subnet_id': Variable.get('SUBNET_ID', ''),
        'sa_id': Variable.get('DATAPROC_SERVICE_ACCOUNT_ID', ''),
        's3_bucket': Variable.get('S3_BUCKET_NAME', ''),
        'security_group_id': Variable.get('SECURITY_GROUP_ID', ''),
    }
    
    print("ğŸ“‹ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°:")
    for key, value in config.items():
        if 'key' in key.lower():
            print(f"  {key}: ***")
        else:
            print(f"  {key}: {value}")
    
    return config

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ DAG
with DAG(
    'test_dataproc_cluster_from_docs',
    schedule_interval=None,
    tags=['test', 'dataproc', 'docs'],
    start_date=datetime.datetime.now(),
    max_active_runs=1,
    catchup=False,
    description='Test DataProc cluster creation using operators from documentation'
) as test_dag:
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸
    check_config = PythonOperator(
        task_id='check_config',
        python_callable=get_cluster_config,
    )
    
    # 1 ÑÑ‚Ğ°Ğ¿: ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Yandex Data Proc
    # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ hardcoded Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ (ĞºĞ°Ğº Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ¼ DAG)
    def create_cluster_task(**context):
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸"""
        import logging
        logger = logging.getLogger(__name__)
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ñ fallback Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (ĞºĞ°Ğº Ğ² fish_classification_training.py)
        folder_id = Variable.get('FOLDER_ID', 'b1gjj3po03aa3m4j8ps5')
        zone = Variable.get('ZONE', 'ru-central1-a')
        subnet_id = Variable.get('SUBNET_ID', 'e9b5umsufggihj02a3o1')
        sa_id = Variable.get('DATAPROC_SERVICE_ACCOUNT_ID', 'ajepv2durkr1nk9rnoui')
        security_group_id = Variable.get('SECURITY_GROUP_ID', 'enpha1v51ug84ddqfob4')
        s3_bucket = Variable.get('S3_BUCKET_NAME', 'fish-classification-data-7wb4zv')
        ssh_key = Variable.get('YC_SSH_PUBLIC_KEY', 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCtV1GrBDN0NKa0VRfqBaGRyFsW6n4pKN7rqKWuStFo6If+6twnYwDYntTaXQBSQcIwF984u5NMP5BVMd8QKSBEvQK/IL9UFjdr3wB73+hKYMpoK9JPY2wPN6ECoxbGzkINZ+vhOgtnHytqGT7Sh79hoB6mKuJg0+yyMbxk0Lq/k7rtKlQYsm28rea4pIlvGmWgUXqzIcnj02K7x+hNGYYbe3kMlUY3MmvKaV1aM0P80gzWgoM0prpbSmdHUTkoP7QdkVlXHNU1xHINBU2lACt5aTL2OOlw/YLcaIDHmvKtghcpfRRskpc4VnJCLqPZgEIA2BTBe85FMcWwBOwTj4nt4YkbDzCgfDVi+K0SevvAKXJla+2H6KEMpUK6pUlpnnm2Q5UKkEiWd+kZ5Tc5ip0gKxkRPG+Q2lT28CbSgqGWmU0EkK2K1je+E/mWRkBybXyObwDHbDYNWmMF0Ztxoo6Rp7WgF2rtNOzeI58pyXFKI+Qn0vyri7s9aLVSBzAey6LVikYs+W5Vpipioh1sI0hETegDyUMQzrmoufoQeGPCBA5tsdAS715mSEhDey326qN5gVNVYcya4nEn2wJ9hNNS1NoSoy2+xvT5NMrLSLRso2Li/P0ud3dd1Xy5AVoMifzrufgZnFJY7nJUnY+0KgElbP35rsZRdsiRMuc5fVpRuQ== denispukinov@MacBook-Pro-Denis.local')
        
        cluster_name = f'test-fish-dp-{uuid.uuid4().hex[:8]}'
        
        logger.info(f"ğŸ—ï¸ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°: {cluster_name}")
        logger.info(f"ğŸ“‹ Folder ID: {folder_id}")
        logger.info(f"ğŸ“‹ Zone: {zone}")
        logger.info(f"ğŸ“‹ Subnet ID: {subnet_id}")
        logger.info(f"ğŸ“‹ Service Account ID: {sa_id}")
        logger.info(f"ğŸ“‹ Security Group ID: {security_group_id}")
        logger.info(f"ğŸ“‹ S3 Bucket: {s3_bucket}")
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ· Lockbox
        create_op = DataprocCreateClusterOperator(
            task_id='dp_cluster_create_op',
            folder_id=folder_id,
            cluster_name=cluster_name,
            cluster_description='Test DataProc cluster for fish classification',
            ssh_public_keys=ssh_key,
            service_account_id=sa_id,
            subnet_id=subnet_id,
            s3_bucket=s3_bucket,
            zone=zone,
            cluster_image_version='2.1',
            
            # Master node (Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ´Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ ĞºĞ²Ğ¾Ñ‚Ñ‹)
            masternode_resource_preset='s2.small',
            masternode_disk_type='network-hdd',  # HDD Ğ²Ğ¼ĞµÑÑ‚Ğ¾ SSD Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ ĞºĞ²Ğ¾Ñ‚Ñ‹
            masternode_disk_size=20,  # ĞœĞ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ Ğ´Ğ»Ñ DataProc (Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 20 GB)
            
            # Compute nodes (Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸)
            computenode_resource_preset='s2.small',
            computenode_disk_type='network-hdd',  # HDD Ğ²Ğ¼ĞµÑÑ‚Ğ¾ SSD Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ ĞºĞ²Ğ¾Ñ‚Ñ‹
            computenode_disk_size=20,  # ĞœĞ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ Ğ´Ğ»Ñ DataProc (Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 20 GB)
            computenode_count=1,
            computenode_max_hosts_count=1,  # Ğ‘ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾ÑĞºĞµĞ¹Ğ»Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸
            
            # Ğ›ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€ Ğ±ĞµĞ· data nodes
            services=['YARN', 'SPARK', 'LIVY'],
            datanode_count=0,
            
            # Security group
            security_group_ids=[security_group_id],
            
            # Ğ¡Ğ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Spark
            properties={
                'spark:spark.sql.warehouse.dir': f's3a://{s3_bucket}/spark-warehouse/',
            },
        )
        
        # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
        result = create_op.execute(context)
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¸Ğ¼Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ
        context['task_instance'].xcom_push(key='cluster_name', value=cluster_name)
        
        logger.info(f"âœ… ĞšĞ»Ğ°ÑÑ‚ĞµÑ€ ÑĞ¾Ğ·Ğ´Ğ°Ğ½: {cluster_name}")
        return result
    
    create_spark_cluster = PythonOperator(
        task_id='dp_cluster_create',
        python_callable=create_cluster_task,
    )
    
    # 2 ÑÑ‚Ğ°Ğ¿: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°-Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºĞ° (Ğ²Ğ¼ĞµÑÑ‚Ğ¾ PySpark job)
    test_placeholder = PythonOperator(
        task_id='test_placeholder',
        python_callable=lambda: print("âœ… ĞšĞ»Ğ°ÑÑ‚ĞµÑ€ ÑĞ¾Ğ·Ğ´Ğ°Ğ½! Ğ—Ğ´ĞµÑÑŒ Ğ±ÑƒĞ´ĞµÑ‚ PySpark job."),
    )
    
    # 3 ÑÑ‚Ğ°Ğ¿: ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
    delete_spark_cluster = DataprocDeleteClusterOperator(
        task_id='dp_cluster_delete',
        trigger_rule=TriggerRule.ALL_DONE,
    )
    
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ DAG
    check_config >> create_spark_cluster >> test_placeholder >> delete_spark_cluster
