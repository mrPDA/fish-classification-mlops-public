"""
üöÄ ML Pipeline Batch Processing DAG - Airflow 2.10 Final Version
================================================================
–°–æ–≤–º–µ—Å—Ç–∏–º —Å Airflow 2.10 –∏ Python 3.12
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ—Ö–æ–∂ –Ω–∞ —Ä–∞–±–æ—á–∏–π opus_dag.py
"""

import uuid
import logging
import random
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.models import Variable, Connection
from airflow.settings import Session
from airflow.utils.dates import days_ago
from airflow.utils.trigger_rule import TriggerRule
from airflow.utils.task_group import TaskGroup
from airflow.providers.yandex.operators.dataproc import (
    DataprocCreateClusterOperator,
    DataprocCreatePysparkJobOperator,
    DataprocDeleteClusterOperator
)

# üéØ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è DAG - –ò–°–ü–û–õ–¨–ó–£–ï–ú days_ago –∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º DAG
default_args = {
    'owner': 'ml-engineer',
    'depends_on_past': False,
    'start_date': days_ago(1),  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º days_ago –∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º DAG
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=10),
}

# üåä –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ DAG - –ò–°–ü–û–õ–¨–ó–£–ï–ú –¢–û–ß–ù–û –¢–ê–ö–û–ô –ñ–ï DAG ID
dag = DAG(
    'ml_pipeline_batch_processing',  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ DAG ID
    default_args=default_args,
    description='üöÄ ML Pipeline - –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ 9 —Ñ–∞–π–ª–æ–≤ –≤ 3 –∏—Ç–µ—Ä–∞—Ü–∏–∏',
    schedule_interval=None,  # –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤—Ä—É—á–Ω—É—é
    catchup=False,
    tags=['ml', 'dataproc', 'batch', 'spark'],
    max_active_runs=1,
)

# üóÇÔ∏è –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Ä–µ–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –∏–∑ –±–∞–∫–µ—Ç–∞)
ALL_FILES = [
    "2019-08-22.txt", "2019-09-21.txt", "2019-10-21.txt",
    "2019-11-20.txt", "2019-12-20.txt", "2020-01-19.txt",
    "2020-02-18.txt", "2020-03-19.txt", "2020-04-18.txt",
    "2020-05-18.txt", "2020-06-17.txt", "2020-07-17.txt",
    "2020-08-16.txt", "2020-09-15.txt", "2020-10-15.txt",
    "2020-11-14.txt", "2020-12-14.txt", "2021-01-13.txt",
    "2021-02-12.txt", "2021-03-14.txt", "2021-04-13.txt",
    "2021-05-13.txt", "2021-06-12.txt", "2021-07-12.txt",
    "2021-08-11.txt", "2021-09-10.txt", "2021-10-10.txt",
    "2021-11-09.txt", "2021-12-09.txt", "2022-01-08.txt",
    "2022-02-07.txt", "2022-03-09.txt", "2022-04-08.txt",
    "2022-05-08.txt", "2022-06-07.txt", "2022-07-07.txt",
    "2022-08-06.txt", "2022-09-05.txt", "2022-10-05.txt",
    "2022-11-04.txt"
]

# üîß –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ - –ò–°–ü–û–õ–¨–ó–£–ï–ú –¢–û–ß–ù–û –¢–ê–ö–ò–ï –ñ–ï
def get_cluster_name(iteration):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∏–º–µ–Ω–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏"""
    return f"ml-batch-cluster-iter{iteration}-{uuid.uuid4().hex[:6]}"

def get_s3_logs_bucket():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ –ª–æ–≥–∞–º S3"""
    bucket_name = Variable.get('S3_BUCKET_SCRIPTS')
    return f"{bucket_name}/ml_batch_logs/"

def select_random_files(**context):
    """–í—ã–±–æ—Ä 9 —Å–ª—É—á–∞–π–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ 3 –≥—Ä—É–ø–ø—ã"""
    logger = logging.getLogger(__name__)
    logger.info("üé≤ –í—ã–±–∏—Ä–∞–µ–º 9 —Å–ª—É—á–∞–π–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
    
    # –í—ã–±–∏—Ä–∞–µ–º 9 —Å–ª—É—á–∞–π–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    selected_files = random.sample(ALL_FILES, 9)
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ 3 –≥—Ä—É–ø–ø—ã –ø–æ 3 —Ñ–∞–π–ª–∞
    file_groups = {
        'iteration_1': selected_files[0:3],
        'iteration_2': selected_files[3:6], 
        'iteration_3': selected_files[6:9]
    }
    
    logger.info("üìã –í—ã–±—Ä–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    for iteration, files in file_groups.items():
        logger.info(f"  {iteration}: {', '.join(files)}")
        context['task_instance'].xcom_push(key=iteration, value=files)
    
    return file_groups

def create_yandex_connection(**context):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è Yandex Cloud"""
    logger = logging.getLogger(__name__)
    logger.info("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è Yandex Cloud...")
    
    try:
        session = Session()
        
        sa_json = Variable.get('DP_SA_JSON', None)
        if not sa_json:
            raise ValueError("DP_SA_JSON variable is required")
        
        yc_connection = Connection(
            conn_id="yc-dataproc-batch",
            conn_type="yandexcloud",
            extra={
                "extra__yandexcloud__service_account_json": sa_json,
            },
        )
        
        existing_conn = session.query(Connection).filter(
            Connection.conn_id == "yc-dataproc-batch"
        ).first()
        
        if existing_conn:
            existing_conn.extra = yc_connection.extra
            logger.info("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ yc-dataproc-batch")
        else:
            session.add(yc_connection)
            logger.info("‚úÖ –°–æ–∑–¥–∞–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ yc-dataproc-batch")
            
        session.commit()
        session.close()
        
        logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ Yandex Cloud –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
        raise

def validate_environment(**context):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
    logger = logging.getLogger(__name__)
    logger.info("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è...")
    
    try:
        required_vars = [
            'ZONE', 'FOLDER_ID', 'SUBNET_ID', 'NETWORK_ID', 'YC_SSH_PUBLIC_KEY',
            'S3_ENDPOINT_URL', 'S3_ACCESS_KEY', 'S3_SECRET_KEY', 'S3_BUCKET_SCRIPTS',
            'DATAPROC_SERVICE_ACCOUNT_ID', 'SECURITY_GROUP_ID', 'DP_SA_JSON', 'S3_DATA_BUCKET'
        ]
        
        missing_vars = []
        
        logger.info("üìã –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:")
        for var in required_vars:
            try:
                value = Variable.get(var)
                if 'secret' in var.lower() or 'key' in var.lower():
                    logger.info(f"   ‚úÖ {var}: ***")
                else:
                    logger.info(f"   ‚úÖ {var}: {value}")
            except Exception:
                logger.error(f"   ‚ùå {var}: –û–¢–°–£–¢–°–¢–í–£–ï–¢")
                missing_vars.append(var)
        
        if missing_vars:
            raise ValueError(f"Missing variables: {missing_vars}")
        
        logger.info("‚úÖ –í—Å–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        raise

def generate_final_report(**context):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    logger = logging.getLogger(__name__)
    
    data_bucket = Variable.get('S3_DATA_BUCKET')
    results_path = f"s3a://{data_bucket}/ml_batch_results/"
    
    logger.info("üéâ –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ML Pipeline –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3 –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ 3 —Ñ–∞–π–ª–∞ = 9 —Ñ–∞–π–ª–æ–≤")
    logger.info(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_path}")
    logger.info("‚úÖ –ì–æ—Ç–æ–≤–æ –¥–ª—è ML –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    return True

# üîÑ –°–æ–∑–¥–∞–Ω–∏–µ TaskGroup –¥–ª—è –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–∏ DAG –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
with dag:
    iteration_groups = []
    
    for iteration in [1, 2, 3]:
        with TaskGroup(f"iteration_{iteration}", tooltip=f"–ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}: —Å–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ ‚Üí –æ–∂–∏–¥–∞–Ω–∏–µ ‚Üí –æ–±—Ä–∞–±–æ—Ç–∫–∞ ‚Üí —É–¥–∞–ª–µ–Ω–∏–µ") as iteration_group:
            
            # üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞ —á–µ—Ä–µ–∑ Python —Ñ—É–Ω–∫—Ü–∏—é - –ö–ê–ö –í –†–ê–ë–û–ß–ï–ú DAG
            cluster_name = get_cluster_name(iteration)
            
            def create_cluster_iteration(**context):
                """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏"""
                from airflow.providers.yandex.operators.dataproc import DataprocCreateClusterOperator
                
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                folder_id = Variable.get('FOLDER_ID')
                subnet_id = Variable.get('SUBNET_ID')
                security_group_id = Variable.get('SECURITY_GROUP_ID')
                zone = Variable.get('ZONE')
                # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –ø–æ–ª—É—á–∞–µ–º SA ID - –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω default
                sa_id = Variable.get('DATAPROC_SERVICE_ACCOUNT_ID', None)
                ssh_key = Variable.get('YC_SSH_PUBLIC_KEY')
                bucket_name = Variable.get('S3_BUCKET_SCRIPTS')
                logs_bucket = f"{bucket_name}/ml_batch_logs/"
                
                logger = logging.getLogger(__name__)
                logger.info(f"üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}: {cluster_name}")
                logger.info(f"üîç Folder ID: {folder_id}")
                logger.info(f"üîç Subnet ID: {subnet_id}")
                logger.info(f"üõ°Ô∏è Security Group ID: {security_group_id}")
                if sa_id:
                    logger.info(f"üë§ Service Account ID: {sa_id}")
                else:
                    logger.info("üë§ Service Account: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è default")
                
                # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞
                cluster_params = {
                    'task_id': f'create_cluster_op_{iteration}',
                    'folder_id': folder_id,
                    'cluster_name': cluster_name,
                    'cluster_description': f'ML Batch Processing Cluster - Iteration {iteration}',
                    'subnet_id': subnet_id,
                    's3_bucket': logs_bucket,
                    'ssh_public_keys': [ssh_key],
                    'zone': zone,
                    'cluster_image_version': "2.0",
                    'security_group_ids': [security_group_id] if security_group_id else [],
                    
                    # üöÄ –ü–û–õ–ù–ê–Ø –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞
                    'masternode_resource_preset': 's3-c4-m16',
                    'masternode_disk_type': 'network-ssd',
                    'masternode_disk_size': 40,
                    
                    'datanode_resource_preset': 's3-c4-m16', 
                    'datanode_disk_type': 'network-ssd',
                    'datanode_disk_size': 100,
                    'datanode_count': 1,
                    
                    'computenode_resource_preset': 's3-c8-m32',
                    'computenode_disk_type': 'network-ssd',
                    'computenode_disk_size': 128,
                    'computenode_count': 2,
                    
                    'services': ['YARN', 'SPARK', 'HDFS', 'MAPREDUCE'],
                    'connection_id': 'yc-dataproc-batch',
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º service_account_id —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –∑–∞–¥–∞–Ω –∏ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω
                if sa_id and sa_id.startswith('aje'):
                    cluster_params['service_account_id'] = sa_id
                
                # –°–æ–∑–¥–∞–µ–º –æ–ø–µ—Ä–∞—Ç–æ—Ä —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
                create_op = DataprocCreateClusterOperator(**cluster_params)
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞
                result = create_op.execute(context)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º—è –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–¥–∞—á
                context['task_instance'].xcom_push(key=f'cluster_name_{iteration}', value=cluster_name)
                
                logger.info(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä —Å–æ–∑–¥–∞–Ω: {cluster_name}")
                return result
            
            create_cluster_task = PythonOperator(
                task_id='create_cluster',
                python_callable=create_cluster_iteration,
            )
            
            # ‚è∞ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ - –ò–°–ü–û–õ–¨–ó–£–ï–ú –¢–û–ß–ù–û –¢–ê–ö–û–ô –ñ–ï –ö–û–î
            wait_cluster_task = BashOperator(
                task_id='wait_cluster_ready',
                bash_command=f'''
                echo "‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}..."
                sleep 120
                echo "‚úÖ –ö–ª–∞—Å—Ç–µ—Ä –≥–æ—Ç–æ–≤ –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}"
                ''',
            )
            
            # üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ PySpark job - –ö–ê–ö –í –†–ê–ë–û–ß–ï–ú DAG
            def run_pyspark_job(**context):
                """–ó–∞–ø—É—Å–∫ PySpark job —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏"""
                from airflow.providers.yandex.operators.dataproc import DataprocCreatePysparkJobOperator
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
                bucket_scripts = Variable.get('S3_BUCKET_SCRIPTS')
                data_bucket = Variable.get('S3_DATA_BUCKET')
                s3_access_key = Variable.get('S3_ACCESS_KEY')
                s3_secret_key = Variable.get('S3_SECRET_KEY')
                
                # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏–∑ XCom
                files = context['task_instance'].xcom_pull(
                    task_ids='select_random_files', 
                    key=f'iteration_{iteration}'
                )
                files_str = ','.join(files) if files else ''
                
                logger = logging.getLogger(__name__)
                logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}")
                logger.info(f"üìã –§–∞–π–ª—ã: {files_str}")
                
                pyspark_op = DataprocCreatePysparkJobOperator(
                    task_id=f'pyspark_job_{iteration}',
                    main_python_file_uri=f"s3a://{bucket_scripts}/scripts/ML_pipeline_batch_fixed.py",
                    connection_id='yc-dataproc-batch',
                    args=[
                        '--bucket', data_bucket,
                        '--mode', 'fast',
                        '--files', files_str,
                        '--iteration', str(iteration),
                        '--output-folder', 'ml_batch_results'
                    ],
                    properties={
                        'spark.hadoop.fs.s3a.access.key': s3_access_key,
                        'spark.hadoop.fs.s3a.secret.key': s3_secret_key,
                        'spark.hadoop.fs.s3a.endpoint': 'storage.yandexcloud.net',
                        'spark.hadoop.fs.s3a.path.style.access': 'true',
                        'spark.hadoop.fs.s3a.connection.ssl.enabled': 'true',
                        'spark.executorEnv.S3_ACCESS_KEY': s3_access_key,
                        'spark.executorEnv.S3_SECRET_KEY': s3_secret_key,
                        'spark.yarn.appMasterEnv.S3_ACCESS_KEY': s3_access_key,
                        'spark.yarn.appMasterEnv.S3_SECRET_KEY': s3_secret_key
                    },
                )
                
                result = pyspark_op.execute(context)
                logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                
                return result
            
            process_task = PythonOperator(
                task_id='process_files',
                python_callable=run_pyspark_job,
            )
            
            # üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ - –ö–ê–ö –í –†–ê–ë–û–ß–ï–ú DAG
            def delete_cluster(**context):
                from airflow.providers.yandex.operators.dataproc import DataprocDeleteClusterOperator
                logger = logging.getLogger(__name__)
                # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –∫–ª–∞—Å—Ç–µ—Ä–∞ –∏–∑ XCom, –∞ –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–Ω–æ–≤–æ
                cluster_name = context['task_instance'].xcom_pull(
                    task_ids='create_cluster',
                    key=f'cluster_name_{iteration}'
                )
                logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}: {cluster_name}")
                delete_op = DataprocDeleteClusterOperator(
                    task_id=f'delete_cluster_op_{iteration}',
                    cluster_id=cluster_name,
                    connection_id='yc-dataproc-batch',
                )
                result = delete_op.execute(context)
                logger.info(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä {cluster_name} —É–¥–∞–ª–µ–Ω")
                return result
                
            cleanup_task = PythonOperator(
                task_id='cleanup_cluster',
                python_callable=delete_cluster,
                trigger_rule=TriggerRule.ALL_DONE,
            )
            
            # –°–¢–†–û–ì–û –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã
            create_cluster_task >> wait_cluster_task >> process_task >> cleanup_task
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä—É–ø–ø—É –¥–ª—è —Å–≤—è–∑—ã–≤–∞–Ω–∏—è
        iteration_groups.append(iteration_group)
    
    # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –≥—Ä—É–ø–ø—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    iteration_1_group, iteration_2_group, iteration_3_group = iteration_groups

    # üìã –°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á
    task_select_files = PythonOperator(
        task_id='select_random_files',
        python_callable=select_random_files,
    )

    task_create_connection = PythonOperator(
        task_id='create_yandex_connection',
        python_callable=create_yandex_connection,
    )

    task_validate = PythonOperator(
        task_id='validate_environment',
        python_callable=validate_environment,
    )

    task_final_report = PythonOperator(
        task_id='generate_final_report',
        python_callable=generate_final_report,
        trigger_rule=TriggerRule.ALL_DONE,
    )

    # üîó –°–≤—è–∑—ã–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á - –ü–†–ê–í–ò–õ–¨–ù–´–ï –ó–ê–í–ò–°–ò–ú–û–°–¢–ò
    # –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ, –∑–∞—Ç–µ–º –≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∏ –≤—ã–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª—ã
    task_create_connection >> task_validate >> task_select_files

    # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏—Ç–µ—Ä–∞—Ü–∏–π - –°–¢–†–û–ì–û –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û  
    # –ö–†–ò–¢–ò–ß–ù–û: –ö–∞–∂–¥–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –∂–¥–∞—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è!
    task_select_files >> iteration_1_group
    iteration_1_group >> iteration_2_group  
    iteration_2_group >> iteration_3_group
    iteration_3_group >> task_final_report
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å: —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ –ü–ï–†–ï–î –∏—Ç–µ—Ä–∞—Ü–∏—è–º–∏
    task_create_connection >> iteration_1_group 