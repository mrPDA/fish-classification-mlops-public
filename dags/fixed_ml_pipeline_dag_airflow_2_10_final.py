"""
ðŸš€ ML Pipeline Batch Processing DAG - Airflow 2.10 Final Version
================================================================
Ð¡Ð¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼ Ñ Airflow 2.10 Ð¸ Python 3.12
ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ Ð¿Ð¾Ñ…Ð¾Ð¶ Ð½Ð° Ñ€Ð°Ð±Ð¾Ñ‡Ð¸Ð¹ opus_dag.py
"""

import uuid
import logging
import random
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.models import Variable, Connection
from airflow.settings import Session
from airflow.utils.dates import days_ago
from airflow.utils.trigger_rule import TriggerRule
from airflow.utils.task_group import TaskGroup
from airflow.providers.yandex.operators.dataproc import (
    DataprocCreateClusterOperator,
    DataprocCreatePysparkJobOperator,
    DataprocDeleteClusterOperator
)

# ðŸŽ¯ ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ DAG - Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—Ð£Ð•Ðœ days_ago ÐºÐ°Ðº Ð² Ñ€Ð°Ð±Ð¾Ñ‡ÐµÐ¼ DAG
default_args = {
    'owner': 'ml-engineer',
    'depends_on_past': False,
    'start_date': days_ago(1),  # Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ days_ago ÐºÐ°Ðº Ð² Ñ€Ð°Ð±Ð¾Ñ‡ÐµÐ¼ DAG
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=10),
}

# ðŸŒŠ ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ DAG - Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—Ð£Ð•Ðœ Ð¢ÐžÐ§ÐÐž Ð¢ÐÐšÐžÐ™ Ð–Ð• DAG ID
dag = DAG(
    'ml_pipeline_batch_processing',  # Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ‚Ð¾Ñ‚ Ð¶Ðµ DAG ID
    default_args=default_args,
    description='ðŸš€ ML Pipeline - ÐŸÐ°ÐºÐµÑ‚Ð½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° 9 Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð² 3 Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸',
    schedule_interval=None,  # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ÑÑ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ
    catchup=False,
    tags=['ml', 'dataproc', 'batch', 'spark'],
    max_active_runs=1,
)

# ðŸ—‚ï¸ Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ (Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ Ð¸Ð· Ð±Ð°ÐºÐµÑ‚Ð°)
ALL_FILES = [
    "2019-08-22.txt", "2019-09-21.txt", "2019-10-21.txt",
    "2019-11-20.txt", "2019-12-20.txt", "2020-01-19.txt",
    "2020-02-18.txt", "2020-03-19.txt", "2020-04-18.txt",
    "2020-05-18.txt", "2020-06-17.txt", "2020-07-17.txt",
    "2020-08-16.txt", "2020-09-15.txt", "2020-10-15.txt",
    "2020-11-14.txt", "2020-12-14.txt", "2021-01-13.txt",
    "2021-02-12.txt", "2021-03-14.txt", "2021-04-13.txt",
    "2021-05-13.txt", "2021-06-12.txt", "2021-07-12.txt",
    "2021-08-11.txt", "2021-09-10.txt", "2021-10-10.txt",
    "2021-11-09.txt", "2021-12-09.txt", "2022-01-08.txt",
    "2022-02-07.txt", "2022-03-09.txt", "2022-04-08.txt",
    "2022-05-08.txt", "2022-06-07.txt", "2022-07-07.txt",
    "2022-08-06.txt", "2022-09-05.txt", "2022-10-05.txt",
    "2022-11-04.txt"
]

# ðŸ”§ Ð’ÑÐ¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ - Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—Ð£Ð•Ðœ Ð¢ÐžÐ§ÐÐž Ð¢ÐÐšÐ˜Ð• Ð–Ð•
def get_cluster_name(iteration):
    """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¸Ð¼ÐµÐ½Ð¸ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° Ð´Ð»Ñ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸"""
    return f"ml-batch-cluster-iter{iteration}-{uuid.uuid4().hex[:6]}"

def get_s3_logs_bucket():
    """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿ÑƒÑ‚Ð¸ Ðº Ð»Ð¾Ð³Ð°Ð¼ S3"""
    bucket_name = Variable.get('S3_BUCKET_SCRIPTS')
    return f"{bucket_name}/ml_batch_logs/"

def select_random_files(**context):
    """Ð’Ñ‹Ð±Ð¾Ñ€ 9 ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð¸ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð½Ð° 3 Ð³Ñ€ÑƒÐ¿Ð¿Ñ‹"""
    logger = logging.getLogger(__name__)
    logger.info("ðŸŽ² Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ 9 ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð²...")
    
    # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ 9 ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð²
    random.seed(42)  # Ð”Ð»Ñ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸
    selected_files = random.sample(ALL_FILES, 9)
    
    # Ð Ð°Ð·Ð´ÐµÐ»ÑÐµÐ¼ Ð½Ð° 3 Ð³Ñ€ÑƒÐ¿Ð¿Ñ‹ Ð¿Ð¾ 3 Ñ„Ð°Ð¹Ð»Ð°
    file_groups = {
        'iteration_1': selected_files[0:3],
        'iteration_2': selected_files[3:6], 
        'iteration_3': selected_files[6:9]
    }
    
    logger.info("ðŸ“‹ Ð’Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹:")
    for iteration, files in file_groups.items():
        logger.info(f"  {iteration}: {', '.join(files)}")
        context['task_instance'].xcom_push(key=iteration, value=files)
    
    return file_groups

def create_yandex_connection(**context):
    """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Yandex Cloud"""
    logger = logging.getLogger(__name__)
    logger.info("ðŸ”§ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Yandex Cloud...")
    
    try:
        session = Session()
        
        sa_json = Variable.get('DP_SA_JSON', None)
        if not sa_json:
            raise ValueError("DP_SA_JSON variable is required")
        
        yc_connection = Connection(
            conn_id="yc-dataproc-batch",
            conn_type="yandexcloud",
            extra={
                "extra__yandexcloud__service_account_json": sa_json,
            },
        )
        
        existing_conn = session.query(Connection).filter(
            Connection.conn_id == "yc-dataproc-batch"
        ).first()
        
        if existing_conn:
            existing_conn.extra = yc_connection.extra
            logger.info("ðŸ”„ ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ yc-dataproc-batch")
        else:
            session.add(yc_connection)
            logger.info("âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ yc-dataproc-batch")
            
        session.commit()
        session.close()
        
        logger.info("âœ… ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Yandex Cloud Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¾")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ: {e}")
        raise

def validate_environment(**context):
    """Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð¸ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ…"""
    logger = logging.getLogger(__name__)
    logger.info("ðŸ” Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ...")
    
    try:
        required_vars = [
            'ZONE', 'FOLDER_ID', 'SUBNET_ID', 'NETWORK_ID', 'YC_SSH_PUBLIC_KEY',
            'S3_ENDPOINT_URL', 'S3_ACCESS_KEY', 'S3_SECRET_KEY', 'S3_BUCKET_SCRIPTS',
            'DATAPROC_SERVICE_ACCOUNT_ID', 'SECURITY_GROUP_ID', 'DP_SA_JSON', 'S3_DATA_BUCKET'
        ]
        
        missing_vars = []
        
        logger.info("ðŸ“‹ ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ…:")
        for var in required_vars:
            try:
                value = Variable.get(var)
                if 'secret' in var.lower() or 'key' in var.lower():
                    logger.info(f"   âœ… {var}: ***")
                else:
                    logger.info(f"   âœ… {var}: {value}")
            except Exception:
                logger.error(f"   âŒ {var}: ÐžÐ¢Ð¡Ð£Ð¢Ð¡Ð¢Ð’Ð£Ð•Ð¢")
                missing_vars.append(var)
        
        if missing_vars:
            raise ValueError(f"Missing variables: {missing_vars}")
        
        logger.info("âœ… Ð’ÑÐµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸: {e}")
        raise

def generate_final_report(**context):
    """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° Ð¿Ð°ÐºÐµÑ‚Ð½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸"""
    logger = logging.getLogger(__name__)
    
    data_bucket = Variable.get('S3_DATA_BUCKET')
    results_path = f"s3a://{data_bucket}/ml_batch_results/"
    
    logger.info("ðŸŽ‰ ÐŸÐ°ÐºÐµÑ‚Ð½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ML Pipeline Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°!")
    logger.info(f"ðŸ“Š ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ 3 Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ 3 Ñ„Ð°Ð¹Ð»Ð° = 9 Ñ„Ð°Ð¹Ð»Ð¾Ð²")
    logger.info(f"ðŸ“ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð²: {results_path}")
    logger.info("âœ… Ð“Ð¾Ñ‚Ð¾Ð²Ð¾ Ð´Ð»Ñ ML Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ")
    
    return True

# ðŸ”„ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ TaskGroup Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ DAG ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
with dag:
    iteration_groups = []
    
    for iteration in [1, 2, 3]:
        with TaskGroup(f"iteration_{iteration}", tooltip=f"Ð˜Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ñ {iteration}: ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° â†’ Ð¾Ð¶Ð¸Ð´Ð°Ð½Ð¸Ðµ â†’ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° â†’ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ") as iteration_group:
            
            # ðŸ—ï¸ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ DataProc ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° Ñ‡ÐµÑ€ÐµÐ· Python Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ - ÐšÐÐš Ð’ Ð ÐÐ‘ÐžÐ§Ð•Ðœ DAG
            cluster_name = get_cluster_name(iteration)
            
            def create_cluster_iteration(**context):
                """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° Ñ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼Ð¸ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¼Ð¸"""
                from airflow.providers.yandex.operators.dataproc import DataprocCreateClusterOperator
                
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð²ÑÐµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ
                folder_id = Variable.get('FOLDER_ID')
                subnet_id = Variable.get('SUBNET_ID')
                security_group_id = Variable.get('SECURITY_GROUP_ID')
                zone = Variable.get('ZONE')
                # ÐžÐ¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ SA ID - ÐµÑÐ»Ð¸ Ð½Ðµ Ð·Ð°Ð´Ð°Ð½, Ð±ÑƒÐ´ÐµÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½ default
                sa_id = Variable.get('DATAPROC_SERVICE_ACCOUNT_ID', None)
                ssh_key = Variable.get('YC_SSH_PUBLIC_KEY')
                bucket_name = Variable.get('S3_BUCKET_SCRIPTS')
                logs_bucket = f"{bucket_name}/ml_batch_logs/"
                
                logger = logging.getLogger(__name__)
                logger.info(f"ðŸ—ï¸ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° Ð´Ð»Ñ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ {iteration}: {cluster_name}")
                logger.info(f"ðŸ” Folder ID: {folder_id}")
                logger.info(f"ðŸ” Subnet ID: {subnet_id}")
                logger.info(f"ðŸ›¡ï¸ Security Group ID: {security_group_id}")
                if sa_id:
                    logger.info(f"ðŸ‘¤ Service Account ID: {sa_id}")
                else:
                    logger.info("ðŸ‘¤ Service Account: Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ default")
                
                # Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°
                cluster_params = {
                    'task_id': f'create_cluster_op_{iteration}',
                    'folder_id': folder_id,
                    'cluster_name': cluster_name,
                    'cluster_description': f'ML Batch Processing Cluster - Iteration {iteration}',
                    'subnet_id': subnet_id,
                    's3_bucket': logs_bucket,
                    'ssh_public_keys': [ssh_key],
                    'zone': zone,
                    'cluster_image_version': "2.0",
                    'security_group_ids': [security_group_id] if security_group_id else [],
                    
                    # ðŸš€ ÐŸÐžÐ›ÐÐÐ¯ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°
                    'masternode_resource_preset': 's3-c4-m16',
                    'masternode_disk_type': 'network-ssd',
                    'masternode_disk_size': 40,
                    
                    'datanode_resource_preset': 's3-c4-m16', 
                    'datanode_disk_type': 'network-ssd',
                    'datanode_disk_size': 100,
                    'datanode_count': 1,
                    
                    'computenode_resource_preset': 's3-c8-m32',
                    'computenode_disk_type': 'network-ssd',
                    'computenode_disk_size': 128,
                    'computenode_count': 2,
                    
                    'services': ['YARN', 'SPARK', 'HDFS', 'MAPREDUCE'],
                    'connection_id': 'yc-dataproc-batch',
                }
                
                # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ service_account_id Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ Ð¾Ð½ Ð·Ð°Ð´Ð°Ð½ Ð¸ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚ÐµÐ½
                if sa_id and sa_id.startswith('aje'):
                    cluster_params['service_account_id'] = sa_id
                
                # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€ Ñ Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¼Ð¸
                create_op = DataprocCreateClusterOperator(**cluster_params)
                
                # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°
                result = create_op.execute(context)
                
                # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¸Ð¼Ñ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° Ð´Ð»Ñ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ñ… Ð·Ð°Ð´Ð°Ñ‡
                context['task_instance'].xcom_push(key=f'cluster_name_{iteration}', value=cluster_name)
                
                logger.info(f"âœ… ÐšÐ»Ð°ÑÑ‚ÐµÑ€ ÑÐ¾Ð·Ð´Ð°Ð½: {cluster_name}")
                return result
            
            create_cluster_task = PythonOperator(
                task_id='create_cluster',
                python_callable=create_cluster_iteration,
            )
            
            # â° ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° - Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—Ð£Ð•Ðœ Ð¢ÐžÐ§ÐÐž Ð¢ÐÐšÐžÐ™ Ð–Ð• ÐšÐžÐ”
            wait_cluster_task = BashOperator(
                task_id='wait_cluster_ready',
                bash_command=f'''
                echo "â³ ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° Ð´Ð»Ñ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ {iteration}..."
                sleep 120
                echo "âœ… ÐšÐ»Ð°ÑÑ‚ÐµÑ€ Ð³Ð¾Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ {iteration}"
                ''',
            )
            
            # ðŸš€ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ„Ð°Ð¹Ð»Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· PySpark job - ÐšÐÐš Ð’ Ð ÐÐ‘ÐžÐ§Ð•Ðœ DAG
            def run_pyspark_job(**context):
                """Ð—Ð°Ð¿ÑƒÑÐº PySpark job Ñ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼Ð¸ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¼Ð¸"""
                from airflow.providers.yandex.operators.dataproc import DataprocCreatePysparkJobOperator
                
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ
                bucket_scripts = Variable.get('S3_BUCKET_SCRIPTS')
                data_bucket = Variable.get('S3_DATA_BUCKET')
                s3_access_key = Variable.get('S3_ACCESS_KEY')
                s3_secret_key = Variable.get('S3_SECRET_KEY')
                
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð¸Ð· XCom
                files = context['task_instance'].xcom_pull(
                    task_ids='select_random_files', 
                    key=f'iteration_{iteration}'
                )
                files_str = ','.join(files) if files else ''
                
                logger = logging.getLogger(__name__)
                logger.info(f"ðŸš€ Ð—Ð°Ð¿ÑƒÑÐº Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð´Ð»Ñ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ {iteration}")
                logger.info(f"ðŸ“‹ Ð¤Ð°Ð¹Ð»Ñ‹: {files_str}")
                
                pyspark_op = DataprocCreatePysparkJobOperator(
                    task_id=f'pyspark_job_{iteration}',
                    main_python_file_uri=f"s3a://{bucket_scripts}/scripts/ML_pipeline_batch_fixed.py",
                    connection_id='yc-dataproc-batch',
                    args=[
                        '--bucket', data_bucket,
                        '--mode', 'fast',
                        '--files', files_str,
                        '--iteration', str(iteration),
                        '--output-folder', 'ml_batch_results'
                    ],
                    properties={
                        'spark.hadoop.fs.s3a.access.key': s3_access_key,
                        'spark.hadoop.fs.s3a.secret.key': s3_secret_key,
                        'spark.hadoop.fs.s3a.endpoint': 'storage.yandexcloud.net',
                        'spark.hadoop.fs.s3a.path.style.access': 'true',
                        'spark.hadoop.fs.s3a.connection.ssl.enabled': 'true',
                        'spark.executorEnv.S3_ACCESS_KEY': s3_access_key,
                        'spark.executorEnv.S3_SECRET_KEY': s3_secret_key,
                        'spark.yarn.appMasterEnv.S3_ACCESS_KEY': s3_access_key,
                        'spark.yarn.appMasterEnv.S3_SECRET_KEY': s3_secret_key
                    },
                )
                
                result = pyspark_op.execute(context)
                logger.info(f"âœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ {iteration} Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°")
                
                return result
            
            process_task = PythonOperator(
                task_id='process_files',
                python_callable=run_pyspark_job,
            )
            
            # ðŸ—‘ï¸ Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° - ÐšÐÐš Ð’ Ð ÐÐ‘ÐžÐ§Ð•Ðœ DAG
            def delete_cluster(**context):
                from airflow.providers.yandex.operators.dataproc import DataprocDeleteClusterOperator
                logger = logging.getLogger(__name__)
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¸Ð¼Ñ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° Ð¸Ð· XCom, Ð° Ð½Ðµ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð·Ð°Ð½Ð¾Ð²Ð¾
                cluster_name = context['task_instance'].xcom_pull(
                    task_ids='create_cluster',
                    key=f'cluster_name_{iteration}'
                )
                logger.info(f"ðŸ—‘ï¸ Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° Ð´Ð»Ñ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ {iteration}: {cluster_name}")
                delete_op = DataprocDeleteClusterOperator(
                    task_id=f'delete_cluster_op_{iteration}',
                    cluster_id=cluster_name,
                    connection_id='yc-dataproc-batch',
                )
                result = delete_op.execute(context)
                logger.info(f"âœ… ÐšÐ»Ð°ÑÑ‚ÐµÑ€ {cluster_name} ÑƒÐ´Ð°Ð»ÐµÐ½")
                return result
                
            cleanup_task = PythonOperator(
                task_id='cleanup_cluster',
                python_callable=delete_cluster,
                trigger_rule=TriggerRule.ALL_DONE,
            )
            
            # Ð¡Ð¢Ð ÐžÐ“Ðž ÐŸÐžÐ¡Ð›Ð•Ð”ÐžÐ’ÐÐ¢Ð•Ð›Ð¬ÐÐž Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ð³Ñ€ÑƒÐ¿Ð¿Ñ‹
            create_cluster_task >> wait_cluster_task >> process_task >> cleanup_task
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð³Ñ€ÑƒÐ¿Ð¿Ñƒ Ð´Ð»Ñ ÑÐ²ÑÐ·Ñ‹Ð²Ð°Ð½Ð¸Ñ
        iteration_groups.append(iteration_group)
    
    # ÐŸÑ€Ð¸ÑÐ²Ð°Ð¸Ð²Ð°ÐµÐ¼ Ð³Ñ€ÑƒÐ¿Ð¿Ñ‹ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¼ Ð´Ð»Ñ ÑƒÐ´Ð¾Ð±ÑÑ‚Ð²Ð°
    iteration_1_group, iteration_2_group, iteration_3_group = iteration_groups

    # ðŸ“‹ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡
    task_select_files = PythonOperator(
        task_id='select_random_files',
        python_callable=select_random_files,
    )

    task_create_connection = PythonOperator(
        task_id='create_yandex_connection',
        python_callable=create_yandex_connection,
    )

    task_validate = PythonOperator(
        task_id='validate_environment',
        python_callable=validate_environment,
    )

    task_final_report = PythonOperator(
        task_id='generate_final_report',
        python_callable=generate_final_report,
        trigger_rule=TriggerRule.ALL_DONE,
    )

    # ðŸ”— Ð¡Ð²ÑÐ·Ñ‹Ð²Ð°Ð½Ð¸Ðµ Ð·Ð°Ð´Ð°Ñ‡ - ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐ«Ð• Ð—ÐÐ’Ð˜Ð¡Ð˜ÐœÐžÐ¡Ð¢Ð˜
    # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ, Ð·Ð°Ñ‚ÐµÐ¼ Ð²Ð°Ð»Ð¸Ð´Ð¸Ñ€ÑƒÐµÐ¼ Ð¸ Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ñ„Ð°Ð¹Ð»Ñ‹
    task_create_connection >> task_validate >> task_select_files

    # ÐŸÐ¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¹ - Ð¡Ð¢Ð ÐžÐ“Ðž ÐŸÐžÐ¡Ð›Ð•Ð”ÐžÐ’ÐÐ¢Ð•Ð›Ð¬ÐÐž  
    # ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž: ÐšÐ°Ð¶Ð´Ð°Ñ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ñ Ð´Ð¾Ð»Ð¶Ð½Ð° Ð¶Ð´Ð°Ñ‚ÑŒ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ!
    task_select_files >> iteration_1_group
    iteration_1_group >> iteration_2_group  
    iteration_2_group >> iteration_3_group
    iteration_3_group >> task_final_report
    
    # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÑŒ: ÑƒÐ±ÐµÐ¶Ð´Ð°ÐµÐ¼ÑÑ Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ ÑÐ¾Ð·Ð´Ð°Ð½Ð¾ ÐŸÐ•Ð Ð•Ð” Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸ÑÐ¼Ð¸
    task_create_connection >> iteration_1_group 