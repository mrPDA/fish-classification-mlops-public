"""
üêü Fish Species Classification Training DAG (with Yandex Cloud SDK)
==============================================================

DAG –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–± —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
Yandex DataProc —á–µ—Ä–µ–∑ Yandex Cloud Python SDK.

Dataset: Red Sea Fish (17 species, 1177 images)
Model: EfficientNet-B4 (fine-tuned)
"""

import os
import uuid
import json
import time
import logging
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.utils.dates import days_ago
from airflow.utils.trigger_rule import TriggerRule

# ========================================
# DAG Configuration
# ========================================

DEFAULT_ARGS = {
    'owner': 'ml-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# ========================================
# Helper Functions
# ========================================

def get_cluster_name():
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∏–º–µ–Ω–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    return f"fish-ml-{uuid.uuid4().hex[:8]}"

def validate_environment(**context):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ Yandex Cloud SDK"""
    logger = logging.getLogger(__name__)
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Yandex Cloud SDK
    try:
        import yandexcloud
        logger.info(f"‚úÖ Yandex Cloud SDK —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {yandexcloud.__version__}")
    except ImportError as e:
        logger.error(f"‚ùå Yandex Cloud SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
        raise RuntimeError("Yandex Cloud SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏ Airflow")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ Lockbox
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ Lockbox...")
    required_vars = ['FOLDER_ID', 'SUBNET_ID', 'ZONE', 'DATAPROC_SERVICE_ACCOUNT_ID', 'S3_BUCKET_NAME']
    found_vars = {}
    missing_vars = []
    
    for var in required_vars:
        try:
            value = Variable.get(var)
            found_vars[var] = value
            logger.info(f"‚úÖ {var}: {value[:20]}..." if len(value) > 20 else f"‚úÖ {var}: {value}")
        except Exception as e:
            missing_vars.append(var)
            logger.warning(f"‚ö†Ô∏è  {var} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ Lockbox Variables")
    
    if missing_vars:
        logger.warning(f"‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {', '.join(missing_vars)}")
        logger.warning("   –ë—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ CONFIG")
    
    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(found_vars)}/{len(required_vars)} –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ Lockbox")
    logger.info("‚úÖ –û–∫—Ä—É–∂–µ–Ω–∏–µ –≤–∞–ª–∏–¥–Ω–æ")
    return True

def validate_dataset(**context):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ S3"""
    logger = logging.getLogger(__name__)
    logger.info("üì¶ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ S3...")
    
    s3_bucket_name = Variable.get('S3_BUCKET_NAME', 'fish-classification-data-7wb4zv')
    logger.info(f"üìÇ S3 Bucket: {s3_bucket_name}")
    logger.info(f"üìÇ Dataset path: s3://{s3_bucket_name}/datasets/")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ boto3
    try:
        import boto3
        s3_access_key = Variable.get('S3_ACCESS_KEY')
        s3_secret_key = Variable.get('S3_SECRET_KEY')
        s3_endpoint = Variable.get('S3_ENDPOINT_URL', 'https://storage.yandexcloud.net')
        
        s3_client = boto3.client(
            's3',
            endpoint_url=s3_endpoint,
            aws_access_key_id=s3_access_key,
            aws_secret_access_key=s3_secret_key
        )
        
        response = s3_client.list_objects_v2(Bucket=s3_bucket_name, Prefix='datasets/', MaxKeys=1)
        if 'Contents' in response:
            logger.info("‚úÖ –î–∞—Ç–∞—Å–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –≤ S3")
        else:
            logger.warning("‚ö†Ô∏è  –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ S3, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
        logger.warning("   –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ")
    
    return True

def create_dataproc_cluster_sdk(**context):
    """–°–æ–∑–¥–∞–Ω–∏–µ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞ —á–µ—Ä–µ–∑ Yandex Cloud SDK"""
    logger = logging.getLogger(__name__)
    cluster_name = get_cluster_name()
    
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–æ–∑–¥–∞–Ω–∏—è DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞: {cluster_name}")
    logger.info("‚ö†Ô∏è  –í —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ - –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ SDK")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ Lockbox
    folder_id = Variable.get('FOLDER_ID')
    subnet_id = Variable.get('SUBNET_ID')
    zone = Variable.get('ZONE')
    dataproc_sa_id = Variable.get('DATAPROC_SERVICE_ACCOUNT_ID')
    s3_bucket_name = Variable.get('S3_BUCKET_NAME')
    
    logger.info(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–∞:")
    logger.info(f"   Folder ID: {folder_id}")
    logger.info(f"   Subnet ID: {subnet_id}")
    logger.info(f"   Zone: {zone}")
    logger.info(f"   Service Account: {dataproc_sa_id}")
    logger.info(f"   S3 Bucket: {s3_bucket_name}")
    
    # TODO: –†–µ–∞–ª—å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —á–µ—Ä–µ–∑ SDK
    # from yandexcloud import SDK
    # sdk = SDK(service_account_key=...)
    # cluster = sdk.client(dataproc_v1.ClusterServiceStub).Create(...)
    
    # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º fake cluster_id
    fake_cluster_id = f"c9q{uuid.uuid4().hex[:16]}"
    logger.info(f"‚úÖ (DEMO) –ö–ª–∞—Å—Ç–µ—Ä —Å–æ–∑–¥–∞–Ω: {fake_cluster_id}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ XCom
    context['task_instance'].xcom_push(key='cluster_id', value=fake_cluster_id)
    context['task_instance'].xcom_push(key='cluster_name', value=cluster_name)
    
    return fake_cluster_id

def wait_cluster_ready_sdk(**context):
    """–û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    logger = logging.getLogger(__name__)
    
    cluster_id = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_id'
    )
    
    logger.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞: {cluster_id}")
    logger.info("‚ö†Ô∏è  –í —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ - –∑–∞–≥–ª—É—à–∫–∞ (sleep 10s)")
    
    time.sleep(10)
    
    logger.info(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} –≥–æ—Ç–æ–≤ (DEMO)")
    return True

def train_model_sdk(**context):
    """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    logger = logging.getLogger(__name__)
    
    cluster_id = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_id'
    )
    
    mlflow_tracking_uri = Variable.get('MLFLOW_TRACKING_URI', 'http://10.11.0.20:5000')
    mlflow_experiment_name = Variable.get('MLFLOW_EXPERIMENT_NAME', 'fish-classification')
    
    logger.info(f"üéì –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ: {cluster_id}")
    logger.info(f"üìä MLflow Tracking URI: {mlflow_tracking_uri}")
    logger.info(f"üè∑Ô∏è  MLflow Experiment Name: {mlflow_experiment_name}")
    logger.info("‚ö†Ô∏è  –†–µ–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ —Å–ª–µ–¥—É—é—â–µ–π –≤–µ—Ä—Å–∏–∏")
    
    return True

def register_model_sdk(**context):
    """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ MLflow"""
    logger = logging.getLogger(__name__)
    
    mlflow_tracking_uri = Variable.get('MLFLOW_TRACKING_URI', 'http://10.11.0.20:5000')
    mlflow_experiment_name = Variable.get('MLFLOW_EXPERIMENT_NAME', 'fish-classification')
    
    logger.info("üìù –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ MLflow...")
    logger.info(f"üìä MLflow URI: {mlflow_tracking_uri}")
    logger.info(f"üè∑Ô∏è  Experiment: {mlflow_experiment_name}")
    logger.info("‚ö†Ô∏è  –†–µ–∞–ª—å–Ω–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ —Å–ª–µ–¥—É—é—â–µ–π –≤–µ—Ä—Å–∏–∏")
    
    return True

def delete_dataproc_cluster_sdk(**context):
    """–£–¥–∞–ª–µ–Ω–∏–µ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    logger = logging.getLogger(__name__)
    
    cluster_id = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_id'
    )
    
    if not cluster_id:
        logger.warning("Cluster ID not found in XCom. Skipping cluster deletion.")
        return False
    
    logger.info(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞: {cluster_id}")
    logger.info("‚ö†Ô∏è  –í —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ - –∑–∞–≥–ª—É—à–∫–∞")
    
    # TODO: –†–µ–∞–ª—å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ SDK
    # sdk.client(dataproc_v1.ClusterServiceStub).Delete(...)
    
    logger.info(f"‚úÖ (DEMO) –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} —É–¥–∞–ª—ë–Ω")
    return True

def generate_report_sdk(**context):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏"""
    logger = logging.getLogger(__name__)
    
    cluster_id = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_id'
    )
    cluster_name = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_name'
    )
    
    logger.info("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞...")
    logger.info("=" * 60)
    logger.info("üéâ PIPELINE –ó–ê–í–ï–†–®–Å–ù (DEMO MODE)")
    logger.info("=" * 60)
    logger.info(f"üñ•Ô∏è  DataProc Cluster: {cluster_name} ({cluster_id})")
    logger.info("‚úÖ Yandex Cloud SDK —Ä–∞–±–æ—Ç–∞–µ—Ç!")
    logger.info("‚ö†Ô∏è  –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞")
    logger.info("=" * 60)
    
    return True

# ========================================
# DAG Definition
# ========================================

with DAG(
    dag_id='fish_classification_training_sdk',
    default_args=DEFAULT_ARGS,
    description='Fish Classification with Yandex Cloud SDK (DEMO)',
    schedule_interval=None,
    catchup=False,
    tags=['ml', 'fish-classification', 'dataproc', 'yandex-sdk', 'demo'],
) as dag:

    task_validate_env = PythonOperator(
        task_id='validate_environment',
        python_callable=validate_environment,
    )

    task_validate_dataset = PythonOperator(
        task_id='validate_dataset',
        python_callable=validate_dataset,
    )

    task_create_cluster = PythonOperator(
        task_id='create_dataproc_cluster',
        python_callable=create_dataproc_cluster_sdk,
    )

    task_wait_cluster = PythonOperator(
        task_id='wait_cluster_ready',
        python_callable=wait_cluster_ready_sdk,
    )

    task_train = PythonOperator(
        task_id='train_model',
        python_callable=train_model_sdk,
    )

    task_register = PythonOperator(
        task_id='register_model',
        python_callable=register_model_sdk,
    )

    task_cleanup = PythonOperator(
        task_id='delete_dataproc_cluster',
        python_callable=delete_dataproc_cluster_sdk,
        trigger_rule=TriggerRule.ALL_DONE,
    )

    task_report = PythonOperator(
        task_id='generate_report',
        python_callable=generate_report_sdk,
    )

    # DAG Flow
    task_validate_env >> task_validate_dataset >> task_create_cluster >> task_wait_cluster >> task_train >> task_register >> task_cleanup >> task_report
