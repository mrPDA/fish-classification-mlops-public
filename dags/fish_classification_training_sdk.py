"""
ğŸŸ Fish Species Classification Training DAG (with Yandex Cloud SDK)
==============================================================

DAG Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ€Ñ‹Ğ± Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼
Yandex DataProc Ñ‡ĞµÑ€ĞµĞ· Yandex Cloud Python SDK.

Dataset: Red Sea Fish (17 species, 1177 images)
Model: EfficientNet-B4 (fine-tuned)
"""

import os
import uuid
import json
import time
import logging
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.utils.dates import days_ago
from airflow.utils.trigger_rule import TriggerRule

# ========================================
# DAG Configuration
# ========================================

DEFAULT_ARGS = {
    'owner': 'ml-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# ========================================
# Helper Functions
# ========================================

def get_cluster_name():
    """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ¼ĞµĞ½Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°"""
    return f"fish-ml-{uuid.uuid4().hex[:8]}"

def validate_environment(**context):
    """Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Yandex Cloud SDK"""
    logger = logging.getLogger(__name__)
    logger.info("ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ...")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Yandex Cloud SDK
    try:
        import yandexcloud
        logger.info(f"âœ… Yandex Cloud SDK ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½: {yandexcloud.__version__}")
    except ImportError as e:
        logger.error(f"âŒ Yandex Cloud SDK Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½: {e}")
        raise RuntimeError("Yandex Cloud SDK Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ² Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Airflow")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Lockbox
    logger.info("ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Lockbox...")
    required_vars = ['FOLDER_ID', 'SUBNET_ID', 'ZONE', 'DATAPROC_SERVICE_ACCOUNT_ID', 'S3_BUCKET_NAME']
    found_vars = {}
    missing_vars = []
    
    for var in required_vars:
        try:
            value = Variable.get(var)
            found_vars[var] = value
            logger.info(f"âœ… {var}: {value[:20]}..." if len(value) > 20 else f"âœ… {var}: {value}")
        except Exception as e:
            missing_vars.append(var)
            logger.warning(f"âš ï¸  {var} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² Lockbox Variables")
    
    if missing_vars:
        logger.warning(f"âš ï¸  ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ: {', '.join(missing_vars)}")
        logger.warning("   Ğ‘ÑƒĞ´ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· CONFIG")
    
    logger.info(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(found_vars)}/{len(required_vars)} Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Lockbox")
    logger.info("âœ… ĞĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾")
    return True

def validate_dataset(**context):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ² S3"""
    logger = logging.getLogger(__name__)
    logger.info("ğŸ“¦ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ² S3...")
    
    s3_bucket_name = Variable.get('S3_BUCKET_NAME', 'fish-classification-data-7wb4zv')
    logger.info(f"ğŸ“‚ S3 Bucket: {s3_bucket_name}")
    logger.info(f"ğŸ“‚ Dataset path: s3://{s3_bucket_name}/datasets/")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· boto3
    try:
        import boto3
        s3_access_key = Variable.get('S3_ACCESS_KEY')
        s3_secret_key = Variable.get('S3_SECRET_KEY')
        s3_endpoint = Variable.get('S3_ENDPOINT_URL', 'https://storage.yandexcloud.net')
        
        s3_client = boto3.client(
            's3',
            endpoint_url=s3_endpoint,
            aws_access_key_id=s3_access_key,
            aws_secret_access_key=s3_secret_key
        )
        
        response = s3_client.list_objects_v2(Bucket=s3_bucket_name, Prefix='datasets/', MaxKeys=1)
        if 'Contents' in response:
            logger.info("âœ… Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ² S3")
        else:
            logger.warning("âš ï¸  Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² S3, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼")
    except Exception as e:
        logger.warning(f"âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°: {e}")
        logger.warning("   ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ")
    
    return True

def create_dataproc_cluster_sdk(**context):
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ DataProc ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ñ‡ĞµÑ€ĞµĞ· Yandex Cloud SDK"""
    logger = logging.getLogger(__name__)
    cluster_name = get_cluster_name()
    
    logger.info(f"ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ DataProc ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°: {cluster_name}")
    logger.info("âš ï¸  Ğ’ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸ - Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºĞ° Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ SDK")
    
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Lockbox
    folder_id = Variable.get('FOLDER_ID')
    subnet_id = Variable.get('SUBNET_ID')
    zone = Variable.get('ZONE')
    dataproc_sa_id = Variable.get('DATAPROC_SERVICE_ACCOUNT_ID')
    s3_bucket_name = Variable.get('S3_BUCKET_NAME')
    
    logger.info(f"ğŸ“‹ ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°:")
    logger.info(f"   Folder ID: {folder_id}")
    logger.info(f"   Subnet ID: {subnet_id}")
    logger.info(f"   Zone: {zone}")
    logger.info(f"   Service Account: {dataproc_sa_id}")
    logger.info(f"   S3 Bucket: {s3_bucket_name}")
    
    # TODO: Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· SDK
    # from yandexcloud import SDK
    # sdk = SDK(service_account_key=...)
    # cluster = sdk.client(dataproc_v1.ClusterServiceStub).Create(...)
    
    # Ğ”Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ - Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ fake cluster_id
    fake_cluster_id = f"c9q{uuid.uuid4().hex[:16]}"
    logger.info(f"âœ… (DEMO) ĞšĞ»Ğ°ÑÑ‚ĞµÑ€ ÑĞ¾Ğ·Ğ´Ğ°Ğ½: {fake_cluster_id}")
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² XCom
    context['task_instance'].xcom_push(key='cluster_id', value=fake_cluster_id)
    context['task_instance'].xcom_push(key='cluster_name', value=cluster_name)
    
    return fake_cluster_id

def wait_cluster_ready_sdk(**context):
    """ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°"""
    logger = logging.getLogger(__name__)
    
    cluster_id = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_id'
    )
    
    logger.info(f"â³ ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°: {cluster_id}")
    logger.info("âš ï¸  Ğ’ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸ - Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºĞ° (sleep 10s)")
    
    time.sleep(10)
    
    logger.info(f"âœ… ĞšĞ»Ğ°ÑÑ‚ĞµÑ€ {cluster_id} Ğ³Ğ¾Ñ‚Ğ¾Ğ² (DEMO)")
    return True

def train_model_sdk(**context):
    """Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    logger = logging.getLogger(__name__)
    
    cluster_id = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_id'
    )
    
    mlflow_tracking_uri = Variable.get('MLFLOW_TRACKING_URI', 'http://10.11.0.20:5000')
    mlflow_experiment_name = Variable.get('MLFLOW_EXPERIMENT_NAME', 'fish-classification')
    
    logger.info(f"ğŸ“ ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğµ: {cluster_id}")
    logger.info(f"ğŸ“Š MLflow Tracking URI: {mlflow_tracking_uri}")
    logger.info(f"ğŸ·ï¸  MLflow Experiment Name: {mlflow_experiment_name}")
    logger.info("âš ï¸  Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ÑƒĞ´ĞµÑ‚ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ² ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸")
    
    return True

def register_model_sdk(**context):
    """Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² MLflow"""
    logger = logging.getLogger(__name__)
    
    mlflow_tracking_uri = Variable.get('MLFLOW_TRACKING_URI', 'http://10.11.0.20:5000')
    mlflow_experiment_name = Variable.get('MLFLOW_EXPERIMENT_NAME', 'fish-classification')
    
    logger.info("ğŸ“ Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² MLflow...")
    logger.info(f"ğŸ“Š MLflow URI: {mlflow_tracking_uri}")
    logger.info(f"ğŸ·ï¸  Experiment: {mlflow_experiment_name}")
    logger.info("âš ï¸  Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ±ÑƒĞ´ĞµÑ‚ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ² ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸")
    
    return True

def delete_dataproc_cluster_sdk(**context):
    """Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ DataProc ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°"""
    logger = logging.getLogger(__name__)
    
    cluster_id = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_id'
    )
    
    if not cluster_id:
        logger.warning("Cluster ID not found in XCom. Skipping cluster deletion.")
        return False
    
    logger.info(f"ğŸ—‘ï¸  Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°: {cluster_id}")
    logger.info("âš ï¸  Ğ’ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸ - Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºĞ°")
    
    # TODO: Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· SDK
    # sdk.client(dataproc_v1.ClusterServiceStub).Delete(...)
    
    logger.info(f"âœ… (DEMO) ĞšĞ»Ğ°ÑÑ‚ĞµÑ€ {cluster_id} ÑƒĞ´Ğ°Ğ»Ñ‘Ğ½")
    return True

def generate_report_sdk(**context):
    """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ° Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸"""
    logger = logging.getLogger(__name__)
    
    cluster_id = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_id'
    )
    cluster_name = context['task_instance'].xcom_pull(
        task_ids='create_dataproc_cluster',
        key='cluster_name'
    )
    
    logger.info("ğŸ“Š Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°...")
    logger.info("=" * 60)
    logger.info("ğŸ‰ PIPELINE Ğ—ĞĞ’Ğ•Ğ Ğ¨ĞĞ (DEMO MODE)")
    logger.info("=" * 60)
    logger.info(f"ğŸ–¥ï¸  DataProc Cluster: {cluster_name} ({cluster_id})")
    logger.info("âœ… Yandex Cloud SDK Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚!")
    logger.info("âš ï¸  Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ÑˆĞ°Ğ³: Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°")
    logger.info("=" * 60)
    
    return True

# ========================================
# DAG Definition
# ========================================

with DAG(
    dag_id='fish_classification_training_sdk',
    default_args=DEFAULT_ARGS,
    description='Fish Classification with Yandex Cloud SDK (DEMO)',
    schedule_interval=None,
    catchup=False,
    tags=['ml', 'fish-classification', 'dataproc', 'yandex-sdk', 'demo'],
) as dag:

    task_validate_env = PythonOperator(
        task_id='validate_environment',
        python_callable=validate_environment,
    )

    task_validate_dataset = PythonOperator(
        task_id='validate_dataset',
        python_callable=validate_dataset,
    )

    task_create_cluster = PythonOperator(
        task_id='create_dataproc_cluster',
        python_callable=create_dataproc_cluster_sdk,
    )

    task_wait_cluster = PythonOperator(
        task_id='wait_cluster_ready',
        python_callable=wait_cluster_ready_sdk,
    )

    task_train = PythonOperator(
        task_id='train_model',
        python_callable=train_model_sdk,
    )

    task_register = PythonOperator(
        task_id='register_model',
        python_callable=register_model_sdk,
    )

    task_cleanup = PythonOperator(
        task_id='delete_dataproc_cluster',
        python_callable=delete_dataproc_cluster_sdk,
        trigger_rule=TriggerRule.ALL_DONE,
    )

    task_report = PythonOperator(
        task_id='generate_report',
        python_callable=generate_report_sdk,
    )

    # DAG Flow
    task_validate_env >> task_validate_dataset >> task_create_cluster >> task_wait_cluster >> task_train >> task_register >> task_cleanup >> task_report
